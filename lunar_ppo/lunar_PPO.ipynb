{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0s8gFzJ3HU_"
   },
   "source": [
    "# Welcome!\n",
    "\n",
    "We will use convolutional neural nets, multiprocessing, and pytorch to implement and train our policy. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oAvQQPDF3HVA",
    "outputId": "0d2b37c2-5ee9-4bc7-a52e-80ae4b163ce2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ipython\n",
      "Using device cuda:0 imported from PPO_utils.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    print(\"Ipython\")\n",
    "    #!pip install IPython\n",
    "    from IPython import display\n",
    "else:\n",
    "    print(\"pyvirtualdisplay\")\n",
    "    #!pip install pyvirtualdisplay\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(1400, 900))\n",
    "    display.start()\n",
    "ion_context = plt.ion()\n",
    "\n",
    "#import lunar_PPO_utils\n",
    "#!pip install JSAnimation\n",
    "from lunar_PPO_utils import *\n",
    "from parallelEnv import parallelEnv #X as parallelEnv\n",
    "from surrogate_fxns import clipped_surrogate\n",
    "print(\"Using device\", device, \"imported from PPO_utils.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRgtfw4Q3HVB",
    "outputId": "183cb620-e146-4b9c-fdc5-91c3b75586f7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install JSAnimation\n",
    "#from lunar_PPO_utils import *\n",
    "#print(\"Using device\", device, \"imported from PPO_utils.\")\n",
    "#from parallelEnv import parallelEnv #X as parallelEnv\n",
    "#from surrogate_fxns import clipped_surrogate\n",
    "#from lunar_PPO_utils import collect_trajectories\n",
    "# widget bar to display progress\n",
    "#!pip install progressbar\n",
    "#import progressbar as pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t5iHqFYS3HVB",
    "outputId": "2a8336af-9fb7-413a-ed5d-e4cb3027c813",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  8\n",
      "Number of actions:  4 \n",
      "\n",
      "1 False\n",
      "1.544\n",
      "[ 0.086  1.498  0.593  0.186 -0.018  0.052  0.     0.   ]\n"
     ]
    }
   ],
   "source": [
    "#import gymnasium as gym\n",
    "SEED=1234\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\") #,  #\"human\",\n",
    "                                 #continuous= True,\n",
    "                                 #gravity= -9.81,\n",
    "                                 #enable_wind= True,\n",
    "                                 #wind_power= 3.14,\n",
    "                                 #turbulence_power= 0.1)\n",
    "\n",
    "state, info = env.reset(seed = SEED)\n",
    "state_shape = env.observation_space.shape\n",
    "state_size = state_shape[0]\n",
    "action_size = env.action_space.n\n",
    "print('State shape: ', state_size)\n",
    "print('Number of actions: ', action_size, \"\\n\")\n",
    "\n",
    "for i in range(13):\n",
    "    action = env.action_space.sample()  \n",
    "    state, reward, done, trun, info = env.step(action)\n",
    "\n",
    "print(action, done or trun)\n",
    "print(np.round(reward, 3))\n",
    "print(np.round(env.step(2)[0], 3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pNDWMPLk3HVB"
   },
   "outputs": [],
   "source": [
    "# run your own policy!\n",
    "policy=VectorPolicy().to(device)\n",
    "\n",
    "# Solution policy:\n",
    "#policy=lunar_PPO_utils.Policy().to(device)\n",
    "\n",
    "# Start with the adam optimizer with learning rate 2e-4\n",
    "# TO DO consider optim.SGD with learning rate 1e-4\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(policy.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dw_I9TpV3HVB",
    "outputId": "0f55eb08-5bc8-4767-8a20-2312f4976a15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.092  1.501  0.593  0.16  -0.015  0.052  0.     0.   ] 1234\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "print(np.round(env.step(0)[0], 3), SEED)   #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YE-COHHv3HVB"
   },
   "source": [
    "# Preprocessing\n",
    "To speed up training, we can simplify the input by cropping the images and use every other pixel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "g7zgPV2g3HVB",
    "outputId": "f38de3ca-bb0c-4842-b98c-3d0fcf18f756",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show what a preprocessed image looks like\n",
    "env.reset()\n",
    "_, _, _, _, _ = env.step(2)\n",
    "# get a frame after 20 steps\n",
    "for _ in range(17):\n",
    "    _, _, _, _, _ = env.step(0)\n",
    "    frame = env.render()\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis('off')\n",
    "plt.imshow(frame)\n",
    "plt.title('original image')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis('off')\n",
    "plt.title('preprocessed image')\n",
    "# 80 x 80 black and white image\n",
    "plt.imshow(prep_frame(frame))#, cmap='Greys')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prpdI-JB3HVC"
   },
   "source": [
    "# Policy\n",
    "\n",
    "## Exercise 1: Implement your policy\n",
    " \n",
    "**Policy input**:\n",
    "* $P_{\\rm up}$, the probability of moving out. Note that $P_{\\rm left}= 1-P_{\\rm down}$\n",
    "\n",
    "**Policy output**:\n",
    "* A tuple of floats representing thrust to main and lateral engines, respectively.\n",
    "* In (-1,1)\n",
    "* **TO DO** clip these per game settings: (main>=0, lateral<-0.5 or lateral>0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1vmfff23HVC",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Game Configuration\n",
    "\n",
    "\n",
    "If continuous=True is passed, continuous actions (corresponding to the throttle of the engines) will be used and the action space will be Box(-1, +1, (2,), dtype=np.float32). The first coordinate of an action determines the throttle of the main engine, while the second coordinate specifies the throttle of the lateral boosters. Given an action np.array([main, lateral]), the main engine will be turned off completely if main < 0 and the throttle scales affinely from 50% to 100% for 0 <= main <= 1 (in particular, the main engine doesnâ€™t work with less than 50% power). Similarly, if -0.5 < lateral < 0.5, the lateral boosters will not fire at all. If lateral < -0.5, the left booster will fire, and if lateral > 0.5, the right booster will fire. Again, the throttle scales affinely from 50% to 100% between -1 and -0.5 (and 0.5 and 1, respectively).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmK2WCmr3HVH",
    "tags": []
   },
   "source": [
    "## Specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsGHhqlR3HVH"
   },
   "source": [
    "*Observation Space*\n",
    "\n",
    "The **state** is an 8-dimensional vector: the coordinates of the lander in x & y, its linear velocities in x & y, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n",
    "\n",
    "*Rewards*\n",
    "\n",
    "After every step a reward is calculated based on state and action values at that step: this is the ***reward function*** the network will try to approximate with these step values and its learned expection of the reward values for future states and actions following from that step (indirectly through weight adjustment, but **TO DO** *maybe also eg. an RMM-like feed-forward link too?*).\n",
    "\n",
    "For each step, the **step reward**:\n",
    "\n",
    "* is increased/decreased the closer/further the lander is to the landing pad.\n",
    "\n",
    "* is increased/decreased the slower/faster the lander is moving.\n",
    "\n",
    "* is decreased the more the lander is tilted (angle not horizontal).\n",
    "\n",
    "* is increased by 10 points for each leg that is in contact with the ground.\n",
    "\n",
    "* is decreased by 0.03 points each frame a side engine is firing.\n",
    "\n",
    "* is decreased by 0.3 points each frame the main engine is firing.\n",
    "\n",
    "The **total reward** of an episode is the sum of the all the step-rewards accumulated for that episode. The episode receives an final additional reward of -100 or +100 points for crashing or landing safely, respectively. An episode is considered **a solution** if it scores at least 200 points on average over 100 episodes.\n",
    "\n",
    "**Starting State**\n",
    "> The lander starts at the top center of the viewport with a random initial force applied to its center of mass.\n",
    "\n",
    "**Episode Termination**\n",
    "> The episode finishes if:\n",
    "  * the lander crashes (the lander body is in contact with the moon)\n",
    "  * the lander remains outside of the viewport (x coordinate is greater than 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cVu4rl_3HVH",
    "tags": []
   },
   "source": [
    "# Game Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9d-kxTKf3HVH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#play(env, policy, time=200) \n",
    "\n",
    "# try to add the option \"preprocess=pong_utils.preprocess_single\"\n",
    "# to see what the agent sees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fo7uRWWp3HVH",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [
     "fxn",
     "defs"
    ]
   },
   "source": [
    "# Function Definitions\n",
    "Here you will define key functions for training. \n",
    "\n",
    "## Exercise 2: write your own function for training\n",
    "(what I call scalar function is the same as policy_loss up to a negative sign)\n",
    "\n",
    "### PPO\n",
    "Later on, you'll implement the PPO algorithm as well, and the scalar function is given by\n",
    "$\\frac{1}{T}\\sum^T_t \\min\\left\\{R_{t}^{\\rm future}\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)},R_{t}^{\\rm future}{\\rm clip}_{\\epsilon}\\!\\left(\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}\\right)\\right\\}$\n",
    "\n",
    "the ${\\rm clip}_\\epsilon$ function is implemented in pytorch as ```torch.clamp(ratio, 1-epsilon, 1+epsilon)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pL_eqhE73HVI",
    "outputId": "7f872701-2a9d-4aec-9399-e9cc6037dca9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.   , 0.88 , 0.774, 0.681, 0.6  , 0.528, 0.464, 0.409, 0.36 ,\n",
       "        0.316, 0.279, 0.245, 0.216, 0.19 , 0.167, 0.147, 0.129, 0.114,\n",
       "        0.1  , 0.088, 0.078, 0.068, 0.06 , 0.053, 0.047, 0.041, 0.036,\n",
       "        0.032, 0.028, 0.025, 0.022, 0.019, 0.017, 0.015, 0.013, 0.011,\n",
       "        0.01 , 0.009, 0.008, 0.007, 0.006, 0.005, 0.005, 0.004, 0.004,\n",
       "        0.003, 0.003, 0.002, 0.002, 0.002, 0.002, 0.001, 0.001, 0.001,\n",
       "        0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.   , 0.   , 0.   ,\n",
       "        0.   ]),\n",
       " (64,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount=0.88 #1-1/4\n",
    "rewards=np.random.rand(64)\n",
    "discount = discount**np.arange(64)\n",
    "#rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "#rewards_future.shape\n",
    "np.round(discount, 3), rewards[::-1].cumsum(axis=0).shape#[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zD03LCIe3HVI"
   },
   "source": [
    "# Training\n",
    "We are now ready to train our policy!\n",
    "WARNING: make sure to turn on GPU, which also enables multicore processing. It may take up to 45 minutes even with GPU enabled, otherwise it will take much longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "tvhrjYz53HVI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(policy, envs, episodes, tmax=1000, SGD_epoch=3, gamma=0.999, epsilon=0.995, beta=0.999):\n",
    "\n",
    "    # keep track of progress\n",
    "    mean_rewards = []\n",
    "\n",
    "    # keep track of how long training takes\n",
    "    #widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar(), ' ', pb.ETA() ]\n",
    "    #timer = pb.ProgressBar(widgets=widget, maxval=episodes).start()\n",
    "\n",
    "    for e in range(episodes):\n",
    "\n",
    "        # collect trajectories returns **lists** of arrays:\n",
    "        # latest version: prob_list, state_list, action_list, reward_list\n",
    "        old_probs, states, actions, rewards = collect_trajectories(envs, policy, tmax=tmax, skip=11)\n",
    "\n",
    "        total_returns = np.sum(rewards, axis=0)\n",
    "        # get the average reward of the parallel environments\n",
    "        mean_returns = np.mean(rewards, axis=0)\n",
    "        mean_rewards.append(mean_returns)\n",
    "\n",
    "        # stochastic gradient Ascent step\n",
    "        for _ in range(SGD_epoch):\n",
    "\n",
    "            # Loss is surrogate function ratio, R\n",
    "            # made negative because ascent delta is in the _opposite direction_ of loss\n",
    "            # clipped to given limits to avoid \"reward cliff\" run-away estimation\n",
    "            L = -clipped_surrogate(policy, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "            optimizer.zero_grad()\n",
    "            L.backward()\n",
    "            optimizer.step()\n",
    "            del L\n",
    "\n",
    "        # the clipping parameter reduces over time\n",
    "        epsilon = max(epsilon*epsilon, 0.005)  #.999\n",
    "\n",
    "        # the regulation term also reduces to decrease exploration after many runs\n",
    "        beta = max(beta*beta, 0.005) #.995\n",
    "          \n",
    "        #scores_window.append(score)       # save most recent score\n",
    "        #scores.append(score)              # save most recent score\n",
    "        #episode_lengths.append(episteps)\n",
    "        #eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        \n",
    "        #cycle_steps = agent.steps%BUFFER_SIZE\n",
    "        #buffer_cycle = agent.steps//BUFFER_SIZE\n",
    "\n",
    "        print(\"\\rEpisode {:4d} | Average: {:7.2f} | Total: {:7.2f} | Epsilon: {:1.3f} | Beta: {:1.3f} |\".format(e, \n",
    "                                                                                                                mean_returns,\n",
    "                                                                                                                total_returns,\n",
    "                                                                                                                epsilon,\n",
    "                                                                                                                beta), end=\"\")\n",
    "                                                                                                                                        \n",
    "        if e % 100 == 0:\n",
    "            #chkpntname = \"data/chkpnt{}.pth\".format(e)\n",
    "            #torch.save(agent.qnetwork_local.state_dict(), chkpntname)   \n",
    "            print(\"\\rEpisode {:4d} | Average: {:7.2f} | Total: {:7.2f} | Epsilon: {:1.3f} | Beta: {:1.3f} |\".format(e, \n",
    "                                                                                                                    mean_returns,\n",
    "                                                                                                                    total_returns,\n",
    "                                                                                                                    epsilon,\n",
    "                                                                                                                    beta))\n",
    "        \n",
    "        if mean_returns>=200. and FIRST:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:5.2f}'.format(e-100, mean_rewards))\n",
    "            #torch.save(agent.qnetwork_local.state_dict(), 'data/slvdpnt.pth')\n",
    "            FIRST = False\n",
    "        elif mean_returns>=250. :\n",
    "            print(\"\\n***** High Score! *****\")\n",
    "            print(\"\\tGame over.\")\n",
    "            #torch.save(agent.qnetwork_local.state_dict(), 'data/hipnt.pth')\n",
    "            break\n",
    "            \n",
    "        # update progress widget bar\n",
    "        #timer.update(e+1)\n",
    "\n",
    "    #timer.finish()\n",
    "    return mean_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5018, device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_probs, states, actions, rewards = collect_trajectories(envs, policy, tmax=tmax, skip=11)\n",
    "clipped_surrogate(policy, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "id": "hqDtiQd33HVJ",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m  \u001b[38;5;66;03m#800\u001b[39;00m\n\u001b[0;32m     11\u001b[0m SEED\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1234\u001b[39m\n\u001b[1;32m---> 12\u001b[0m mean_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSGD_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscount_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(policy, envs, episodes, tmax, SGD_epoch, gamma, epsilon, beta)\u001b[0m\n\u001b[0;32m     27\u001b[0m L \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mclipped_surrogate(policy, old_probs, states, actions, rewards, epsilon\u001b[38;5;241m=\u001b[39mepsilon, beta\u001b[38;5;241m=\u001b[39mbeta)\n\u001b[0;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 29\u001b[0m \u001b[43mL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m L\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\general\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\general\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Control \n",
    "n_parallel_agents=5\n",
    "envs = parallelEnv(\"LunarLander-v2\", n=n_parallel_agents, seed=1234)\n",
    "discount_rate = 0.999#1 - 1/(n_parallel_agents+1)\n",
    "epsilon = 0.995  # 0.05 no decay...?\n",
    "beta = 0.999     # .01 \n",
    "tmax = 1000    # training loop max = 1000\n",
    "SGD_epoch = 2  #\n",
    "# WARNING: running through all 800 episodes will take 30-45 minutes\n",
    "episodes = 200  #800\n",
    "SEED=1234\n",
    "mean_rewards = train(policy, envs, episodes, tmax, SGD_epoch, gamma=discount_rate, epsilon=epsilon, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "R69TUzEQ3HVI",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "62d08d1f-12d5-4ee6-b94d-469a0fb97459",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.1823007 ,  2.438727  , -1.270335  , -0.5617797 , -0.15886863,\n",
       "          0.07746112, -0.1714521 , -0.1714521 ],\n",
       "        [-0.4596616 ,  2.4700966 , -0.79966253,  0.5325886 , -0.45232955,\n",
       "         -0.37848827, -0.45627156, -0.45627156],\n",
       "        [-0.29874447,  2.568453  , -0.05204507, -0.95356935, -0.3040415 ,\n",
       "         -0.35764307, -0.30120468, -0.30120468],\n",
       "        [-0.5566876 ,  2.456928  ,  0.16145879,  0.36627325, -0.57213247,\n",
       "         -0.72814196, -0.563849  , -0.563849  ],\n",
       "        [-0.17464083,  2.4991915 , -0.77636415, -1.0182447 , -0.16167633,\n",
       "         -0.03098305, -0.16864128, -0.16864128]], dtype=float32),\n",
       " array([[0.49805182, 0.9687297 , 0.44079956, 0.47897178, 0.5010789 ,\n",
       "         0.5134098 , 0.        , 0.        ],\n",
       "        [0.499451  , 0.97390985, 0.4833169 , 0.5480422 , 0.5003048 ,\n",
       "         0.503779  , 0.        , 0.        ],\n",
       "        [0.50040114, 0.9679134 , 0.5121881 , 0.4680885 , 0.49977913,\n",
       "         0.49723923, 0.        , 0.        ],\n",
       "        [0.5011227 , 0.9735877 , 0.5341134 , 0.5437465 , 0.49937993,\n",
       "         0.4922728 , 0.        , 0.        ],\n",
       "        [0.4989499 , 0.9669607 , 0.4680884 , 0.45538723, 0.5005821 ,\n",
       "         0.50722843, 0.        , 0.        ]], dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_parallel_agents=5\n",
    "envs = parallelEnv(\"LunarLander-v2\", n=n_parallel_agents, seed=1234)\n",
    "discount = 0.999 #1 - 1/(n_parallel_agents+1)\n",
    "epsilon = 0.995  # 0.05 no decay...?\n",
    "beta = 0.999     # .01 \n",
    "tmax = 1000      # training loop max = 1000\n",
    "SGD_epoch = 4    #\n",
    "# WARNING: running through all 800 episodes will take 30-45 minutes\n",
    "episodes = 200  #800\n",
    "SEED=1234\n",
    "\n",
    "states = envs.reset()\n",
    "states = np.asarray(states)\n",
    "mean = np.mean(states, axis=1)\n",
    "std = np.std(states, axis=1) + 1.0e-8\n",
    "norm_states = (states - mean[:, np.newaxis])/std[:, np.newaxis]\n",
    "norm_states, scale_input_batch(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 8, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = envs.reset()\n",
    "states = np.asarray(states)\n",
    "shog = states.shape\n",
    "np.reshape(states, (*shog,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4766, 0.4665, 0.5042, 0.4897],\n",
       "         [0.4772, 0.4672, 0.5039, 0.4900],\n",
       "         [0.4773, 0.4668, 0.5037, 0.4902],\n",
       "         [0.4777, 0.4675, 0.5036, 0.4901],\n",
       "         [0.4768, 0.4665, 0.5040, 0.4899]], device='cuda:0'),\n",
       " tensor([[0.2421, 0.2325, 0.2703, 0.2551],\n",
       "         [0.2424, 0.2328, 0.2697, 0.2551],\n",
       "         [0.2425, 0.2325, 0.2695, 0.2554],\n",
       "         [0.2427, 0.2330, 0.2693, 0.2550],\n",
       "         [0.2423, 0.2324, 0.2700, 0.2553]], device='cuda:0'),\n",
       " tensor([[-0.0936, -0.1342,  0.0167, -0.0410],\n",
       "         [-0.0912, -0.1316,  0.0157, -0.0400],\n",
       "         [-0.0908, -0.1331,  0.0147, -0.0391],\n",
       "         [-0.0894, -0.1302,  0.0145, -0.0397],\n",
       "         [-0.0927, -0.1343,  0.0159, -0.0403]], device='cuda:0'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_to_sigs(policy, scale_input_batch(states)), states_to_probs(policy, scale_input_batch(states)), states_to_pols(policy, scale_input_batch(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mbCSL_Wl3HVI",
    "outputId": "3fe15c9f-2e85-4981-85bf-fe9ae7c35ed6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards _________________\n",
      "\t (80, 5)\n",
      "gamma ___________________\n",
      "\t (80,)\n",
      "gamma new axis __________\n",
      "\t (80, 1)\n",
      "rewards gamma ________\n",
      "\t (80, 5)\n",
      "\n",
      "rewards_future ___________\n",
      "\t (80, 5)\n",
      "\n",
      "rews mean ____________\n",
      "\t (80,)\n",
      "rews mean newaxis ____\n",
      "\t (80, 1)\n",
      "rews std _____________\n",
      "\t (80,)\n",
      "rews std newaxis _____\n",
      "\t (80, 1)\n",
      "rewards normed _________\n",
      " (80, 5)\n",
      "would_be rewards _______\n",
      " (80, 5, 1)\n",
      "rewards tensor _________\n",
      " torch.Size([80, 5])\n",
      "states ___________________\n",
      " (80, 5, 8)\n",
      "states scaled ___________\n",
      " (400, 8)\n",
      "states reshaped ___________\n",
      " (80, 5, 8)\n",
      "states tensor ___________\n",
      " torch.Size([80, 5, 8])\n",
      "new_sigs _________________\n",
      " torch.Size([80, 5, 4])\n",
      "old_sigs __________________\n",
      " torch.Size([80, 5, 4])\n",
      "ratio ___________________\n",
      " torch.Size([80, 5, 4])\n",
      "clip ___________________\n",
      " torch.Size([80, 5, 4])\n",
      ">>> The problem is here.\n",
      "ratio ___________________\n",
      " torch.Size([80, 5, 4])\n",
      "rewards ___________________\n",
      " torch.Size([80, 5])\n",
      "clip ___________________\n",
      " torch.Size([80, 5, 4])\n",
      ">>> Solution:\n",
      "rewards new view ________\n",
      " torch.Size([80, 5, 1])\n",
      "R x R, C X R ___________________\n",
      " torch.Size([80, 5, 4]) torch.Size([80, 5, 4])\n",
      "clipped surrogate__________\n",
      ">>>  torch.Size([80, 5, 4])\n",
      "entropy ___________________\n",
      " torch.Size([80, 5, 4])\n",
      "\n",
      "return:torch.Size([80, 5])\n"
     ]
    }
   ],
   "source": [
    "########## CLIPPED from surrogate_fxns\n",
    "#del clipped_surrogate from surrogate_fxns import clipped_surrogate\n",
    "\n",
    "discount = 0.999\n",
    "epsilon = 0.995\n",
    "beta = 0.99\n",
    "n_parallel_agents=5\n",
    "envs = parallelEnv(\"LunarLander-v2\", n=n_parallel_agents, seed=1234)\n",
    "\n",
    "### CT returns **lists**\n",
    "old_sigs, states, actions, rewards = collect_trajectories(envs, policy, tmax=1000, skip=11)\n",
    "\n",
    "# convert everything into pytorch tensors and move to gpu if available\n",
    "#actions = np.asarray(actions, dtype=np.int8)\n",
    "#actions = torch.tensor(actions, dtype=torch.int8, device=lpu.device)\n",
    "\n",
    "#### convert rewards to normed future rewards\n",
    "rewards = np.asarray(rewards)\n",
    "print(\"rewards _________________\\n\\t\", rewards.shape)\n",
    "gamma = discount**np.arange(len(rewards))\n",
    "print(\"gamma ___________________\\n\\t\", gamma.shape)\n",
    "gamma = gamma[:,np.newaxis]\n",
    "print(\"gamma new axis __________\\n\\t\", gamma.shape)\n",
    "rewards = rewards*gamma\n",
    "print(\"rewards gamma ________\\n\\t\", rewards.shape)\n",
    "rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "print(\"\\nrewards_future ___________\\n\\t\", rewards_future.shape)\n",
    "\n",
    "mean = np.mean(rewards_future, axis=1)\n",
    "print(\"\\nrews mean ____________\\n\\t\", mean.shape)\n",
    "mean = mean[:, np.newaxis]\n",
    "print(\"rews mean newaxis ____\\n\\t\", mean.shape)\n",
    "std = np.std(rewards_future, axis=1) + 1.0e-8\n",
    "print(\"rews std _____________\\n\\t\", std.shape)\n",
    "std = std[:, np.newaxis]\n",
    "print(\"rews std newaxis _____\\n\\t\", std.shape)\n",
    "rewards_normalized = (rewards_future - mean)/std\n",
    "print(\"rewards normed _________\\n\", rewards.shape)\n",
    "print(\"would_be rewards _______\\n\", rewards[:,:, np.newaxis].shape)\n",
    "rewards = torch.tensor(rewards_normalized, dtype=torch.float32, device=lpu.device)\n",
    "print(\"rewards tensor _________\\n\", rewards.shape)\n",
    "\n",
    "##### Pre-Norm States\n",
    "states = np.asarray(states, dtype=np.float32)\n",
    "shog = states.shape\n",
    "print(\"states ___________________\\n\", states.shape)\n",
    "states = lpu.scale_input_batch( states.reshape(-1, 8) )\n",
    "print(\"states scaled ___________\\n\", states.shape)\n",
    "states = states.reshape(*shog)\n",
    "print(\"states reshaped ___________\\n\", states.shape)\n",
    "states = torch.tensor(states, dtype=torch.float32, device=lpu.device) \n",
    "print(\"states tensor ___________\\n\", states.shape)\n",
    "# convert states to policy (or probability)\n",
    "new_sigs = states_to_sigs(policy, states)\n",
    "print(\"new_sigs _________________\\n\", new_sigs.shape)\n",
    "old_sigs = np.asarray(old_sigs, dtype=np.float32)\n",
    "old_sigs = torch.tensor(old_sigs, dtype=torch.float32, device=lpu.device)\n",
    "print(\"old_sigs __________________\\n\", old_sigs.shape)\n",
    "# ratio for clipping\n",
    "ratio = new_sigs/old_sigs\n",
    "print(\"ratio ___________________\\n\", ratio.shape)\n",
    "# clipped function\n",
    "clip = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "print(\"clip ___________________\\n\", clip.shape)\n",
    "try:\n",
    "    rxr = ratio*rewards\n",
    "    cxr = clip*rewards\n",
    "except:\n",
    "    print(\">>> The problem is here.\")\n",
    "    print(\"ratio ___________________\\n\", ratio.shape)\n",
    "    print(\"rewards ___________________\\n\", rewards.shape)\n",
    "    print(\"clip ___________________\\n\", clip.shape)\n",
    "    rewards = rewards.view(-1, rewards.shape[1], 1)\n",
    "    print(\">>> Solution:\\nrewards new view ________\\n\", rewards.shape)\n",
    "\n",
    "rxr = ratio*rewards\n",
    "cxr = clip*rewards\n",
    "print(\"R x R, C X R ___________________\\n\", rxr.shape, cxr.shape)\n",
    "\n",
    "try:\n",
    "    clipped_surge = torch.min(rxr, cxr)\n",
    "    print(\"clipped surrogate__________\\n>>> \", clipped_surge.shape)\n",
    "except:\n",
    "    print(\"clipped surrogate__________\\n>>> Well that didn't work! <<<\")\n",
    "\n",
    "# include a regularization term to steer new_policy towards 0.5\n",
    "try:\n",
    "    entropy = -1*( new_sigs*torch.log(old_sigs+1.e-10) + (1.0-new_sigs)*torch.log(1.0-old_sigs+1.e-10) )\n",
    "    print(\"entropy ___________________\\n\", entropy.shape)\n",
    "\n",
    "except:\n",
    "    print(\"entropy ___________________\\n>>> Problem. <<<\")\n",
    "\n",
    "# returns an average of all the entries of the tensor\n",
    "# effective computing L_sur^clip / T averaged over time-step and number of trajectories\n",
    "try:\n",
    "    rtrnval = torch.mean(clipped_surge + beta*entropy, axis=-1)\n",
    "    print(\"\\nreturn:{}\".format(rtrnval.shape))\n",
    "except:\n",
    "    print(\"\\nAnd that's it.\")\n",
    "\n",
    "#############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0327, device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_sigs, states, actions, rewards = collect_trajectories(envs, policy, tmax=1000, skip=11)\n",
    "\n",
    "clipped_surrogate(policy, old_sigs, states, actions, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states, rewards, dones, infos, shapes:\n",
      "[[-0.194  1.882 -0.805  0.951  0.414  0.108  0.     0.   ]\n",
      " [ 0.607  1.598  1.258  0.9   -0.099  0.032  0.     0.   ]\n",
      " [ 0.014  1.394  0.728 -0.4   -0.019 -0.209  0.     0.   ]\n",
      " [-0.213  2.565 -0.509  1.25   0.167  0.004  0.     0.   ]\n",
      " [-0.295  1.948 -0.457  1.151  0.015 -0.063  0.     0.   ]]\n",
      "[-0.411 -1.521 -2.369 -0.146  0.416]\n",
      "[False False False False False]\n",
      "({}, {}, {}, {}, {})\n",
      "[(5, 8), (5,), (5,), 5]\n"
     ]
    }
   ],
   "source": [
    "states, rewards, dones, infos = envs.step([envs.action_space.sample()]*5)\n",
    "print(\"states, rewards, dones, infos, shapes:\\n{}\\n{}\\n{}\\n{}\\n{}\".format(np.round(states,3), np.round(rewards, 3), dones, infos,\n",
    "                                                                         [x.shape for x in [states, rewards, dones]]+[len(infos)])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "aoApeUL63HVJ",
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mean_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSGD_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscount_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(policy, envs, episodes, tmax, SGD_epoch, gamma, epsilon, beta)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#L = -pong_utils.clipped_surrogate(policy, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 31\u001b[0m \u001b[43mL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m L\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\general\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\general\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "mean_rewards = train(policy, envs, episodes, tmax, SGD_epoch, gamma=discount_rate, epsilon=epsilon, beta=beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPkXgXkA3HVJ"
   },
   "outputs": [],
   "source": [
    "plt.plot(mean_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwebDzdf3HVJ"
   },
   "outputs": [],
   "source": [
    "# save your policy!\n",
    "torch.save(policy, 'lunar_PPO.policy')\n",
    "\n",
    "# load policy if needed\n",
    "#policy = torch.load('PPO.policy')\n",
    "#pong_utils.play(env, policy, time=1000) \n",
    "\n",
    "# try and test out the solution \n",
    "# make sure GPU is enabled, otherwise loading will fail\n",
    "# (the PPO version can win more often than not)!\n",
    "#\n",
    "#policy_solution = torch.load('PPO_solution.policy')\n",
    "#pong_utils.play(env, policy_solution, time=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLslSZrT3HVJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
