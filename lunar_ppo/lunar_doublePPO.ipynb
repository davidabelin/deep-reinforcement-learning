{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKH0-yBwyZA4",
    "tags": []
   },
   "source": [
    "# Welcome!\n",
    "Below, we will learn to implement and train a policy to play atari-pong, using only the pixels as input. We will use convolutional neural nets, multiprocessing, and pytorch to implement and train our policy. Let's get started!\n",
    "\n",
    "(I strongly recommend you to try this notebook on the Udacity workspace first before running it locally on your desktop/laptop, as performance might suffer in different environments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lk3IydYxyZA5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# render ai gym environment\n",
    "#!pip install gymnasium[box2d]\n",
    "import gymnasium as gym\n",
    "\n",
    "#!pip install progressbar\n",
    "import progressbar as pb\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "#%matplotlib inline\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "else:  \n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=True, size=(1400, 900))\n",
    "    display.start()\n",
    "    \n",
    "# install package for displaying animation    \n",
    "from JSAnimation.IPython_display import display_animation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using device: \",device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### NEW GYM = GYMNASIUM\n",
    "#!pip install gymnasium[box2d]\n",
    "#import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\",\n",
    "                                 continuous= False,\n",
    "                                 gravity= -9.81,\n",
    "                                 enable_wind= True,\n",
    "                                 wind_power= 1.11,\n",
    "                                 turbulence_power= 0.11)\n",
    "            #new_step_api=False,  )\n",
    "\n",
    "state, info = env.reset(seed = 1234)\n",
    "obs = env.render()\n",
    "    \n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    state, reward, done, trun, info = env.step(action)\n",
    "    #print(action, reward)\n",
    "    obs = env.render()\n",
    "    plt.imshow(obs)\n",
    "\n",
    "state_shape = env.observation_space.shape\n",
    "state_size = state_shape[0]\n",
    "action_size = env.action_space.n\n",
    "print('State shape: ', state_size)\n",
    "print('Number of actions: ', action_size)\n",
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Space\n",
    "\n",
    "The observation space is an 8-dimensional vector: \n",
    "* the coordinates of the lander (x & y), \n",
    "* its linear velocities (x & y), \n",
    "* its angle (radians), \n",
    "* its angular velocity, \n",
    "* and two booleans for whether/not each leg has ground contact.\n",
    "\n",
    "Observation Highs:\n",
    "* [1.5  1.5  5.0  5.0  3.14  5.0  True  True ]\n",
    "\n",
    "Observation Lows:\n",
    "* [-1.5  -1.5  -5.0  -5.0  -3.14  -5.0  False  False ]\n",
    "\n",
    "Wind function:\n",
    "\n",
    "tanh(sin(2 k (t+C)) + sin(pi k (t+C))). k is set to 0.01. C is sampled randomly between -9999 and 9999\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Action Space\n",
    "\n",
    "There are four discrete actions available:\n",
    "\n",
    "* 0: do nothing\n",
    "* 1: fire left orientation engine\n",
    "* 2: fire main engine\n",
    "* 3: fire right orientation engine\n",
    "\n",
    "\n",
    "### Rewards\n",
    "\n",
    "After every step a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
    "\n",
    "For each step, the reward:\n",
    "\n",
    "* is increased/decreased the closer/further the lander is to the landing pad.\n",
    "* is increased/decreased the slower/faster the lander is moving.\n",
    "* is decreased the more the lander is tilted (angle not horizontal).\n",
    "* is increased by 10 points for each leg that is in contact with the ground.\n",
    "* is decreased by 0.03 points each frame a side engine is firing.\n",
    "* is decreased by 0.3 points each frame the main engine is firing.\n",
    "\n",
    "The episode receives an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
    "\n",
    "An episode is considered a solution if it scores at least 200 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ueCaKFD2rLw"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lunar_PPO_utils as plutils\n",
    "\n",
    "OFF=0\n",
    "MAIN=2\n",
    "RIGHT=3\n",
    "LEFT=1\n",
    "ACTIONS=[OFF,MAIN,RIGHT,LEFT]\n",
    "LOWS=np.array([-1.5, -1.5, -5., -5., -3.1415927, -5., False, False])\n",
    "HIGHS=np.array([1.5, 1.5, 5., 5., 3.1415927, 5., True, True])\n",
    "\n",
    "norm = lambda x: (x - x.mean())/x.std() if x.std()!=0. else 0.\n",
    "scale = lambda x: (x - x.min())/(x.max() - x.min()) if x.max()!=x.min() else x\n",
    "pix_norm = lambda x: x/255.\n",
    "vals2probs = nn.Softmax(dim=1)\n",
    "\n",
    "def scale_input(state):\n",
    "    scaled_state = np.zeros_like(state)\n",
    "    scaled_state[:-2] = (state[:-2] - LOWS[:-2]) / (HIGHS[:-2] - LOWS[:-2])\n",
    "    scaled_state[-2:] = state[-2:] \n",
    "    return scaled_state\n",
    "\n",
    "def scale_input_batch(states):\n",
    "    scaled_batch =  np.array([scale_input(state) for state in states])     \n",
    "    return scaled_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def states_to_probs(policy, states):  \n",
    "    vals = policy(torch.from_numpy(np.asarray(states)).to(device))\n",
    "    probs = vals2probs(vals)\n",
    "    return probs.detach().cpu().numpy()\n",
    "    \n",
    "def states_to_vals(policy, states):\n",
    "    '''\n",
    "       Params\n",
    "       ======\n",
    "       policy (nn)\n",
    "       states (list)\n",
    "       Returns: probs (as array) raw output of NN, log-probs of actions\n",
    "    '''\n",
    "    vals = policy(torch.from_numpy(np.asarray(states)).to(device))\n",
    "    return vals.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because the full states of many of the Atari games are not completely observable from the image frames, \n",
    "#Mnih et al. “stacked” the four most recent frames so that the inputs to the network had dimension\n",
    "#84⇥84⇥4. This did not eliminate partial observability for all of the games, but it was\n",
    "#helpful in making many of them more Markovian.\n",
    "# Text pg. 438\n",
    "\n",
    "def prep_input(state0, state1):\n",
    "    '''Takes two sequential states and returns input to the network   \n",
    "        Params\n",
    "        states (np.array) shape (8,)\n",
    "        Returns (np.array) shape (2,8) prepped for input \n",
    "    '''\n",
    "    state0 = scale_input(state0)\n",
    "    state1 = scale_input(state1)\n",
    "    state = np.asarray([state0, state1])\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpACk9hYyZA6"
   },
   "source": [
    "## Preprocessing\n",
    "To speed up training, we can simplify the input by cropping the images and use every other pixel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-GMFDmryZA7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show what a preprocessed image looks like\n",
    "state, info = env.reset()\n",
    "_, _, _, _, _ = env.step(0)\n",
    "# get a frame after 20 steps\n",
    "for _ in range(20):\n",
    "    _, _, _, _, _ = env.step(np.random.choice([0,1,2,3]))\n",
    "frame = env.render()\n",
    "\n",
    "#fig = plt.Figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('original, shape='+str(frame.shape))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "#pframe = prep_img(frame)\n",
    "pframe = plutils.preprocess_single(frame)\n",
    "plt.title('processed, shape='+str(pframe.shape))\n",
    "# 80 x 80 black and white image\n",
    "plt.imshow(pframe, cmap='Greys')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from dqn_agent import Agent\n",
    "\n",
    "    env.seed = 1234\n",
    "    agent = Agent(state_size=state_space[0], action_size=action_size, seed=env.seed)\n",
    "    #agent.qnetwork_local.load_state_dict(torch.load('checkpoint_2k.pth'))\n",
    "\n",
    "    # watch a well-trained agent\n",
    "    state, _ = env.reset()\n",
    "    img = plt.imshow(env.render())\n",
    "    for j in range(200):\n",
    "        action = agent.act(state)\n",
    "        state, reward, done,trun, _ = env.step(action)\n",
    "        img.set_data(env.render()) \n",
    "        plt.axis('off')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        if done:\n",
    "            break \n",
    "\n",
    "    #env.close()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yutVg1-yZA7"
   },
   "source": [
    "# Policy\n",
    "\n",
    "## Exercise 1: Implement your policy\n",
    " \n",
    "Here, we define our policy. The input is the stack of two different frames (which captures the movement), and the output is a number $P_{\\rm right}$, the probability of moving left. Note that $P_{\\rm left}= 1-P_{\\rm right}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    ''' Estimate action probabilities from states\n",
    "        state_size: number of vector observations\n",
    "        action_size: number of discrete actions available\n",
    "    '''\n",
    "    def __init__(self, state_size=8, action_size=4, seed=1234):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # fully connected layers\n",
    "        self.state_in = nn.Linear(state_size, 64)\n",
    "        #self.action_in = nn.Linear(action_size, 32) \n",
    "        self.hidden = nn.Linear(64, 16)\n",
    "        self.action_out = nn.Linear(16, action_size)\n",
    "        # sigmoid to logprobs\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "        #self.tanh = nn.Tanh()\n",
    "        #self.leaky = nn.LeakyReLU()\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        ##### expecting pre-scaled values to 0,1    #around mean +/- std\n",
    "        x = F.relu(self.state_in(state))\n",
    "        x = F.relu(self.hidden(x))\n",
    "        x = self.action_out(x)\n",
    "        #return self.soft(x)\n",
    "        ######## returning raw values!\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhkD58VYz_jh"
   },
   "outputs": [],
   "source": [
    "#seed=12345\n",
    "#import lunar_PPO_utils as plutils\n",
    "\n",
    "# use your own policy!\n",
    "#policy=Policy().to(device)\n",
    "policy=Policy(state_size=state_size, action_size=action_size).to(device)\n",
    "\n",
    "# we use the adam optimizer with learning rate 2e-4\n",
    "# optim.SGD is also possible\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(policy.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HY_lXi4iiqRt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    x, w = env.reset()\n",
    "    x1, y1, z1, u1, w1 = env.step(1)\n",
    "    print(\"env.step(1)\", scale_input(x1), y1, z1, u1, w1)\n",
    "    x2, y2, z2, u2, w2 = env.step(2)\n",
    "    print(\"env.step(2)\", scale_input(x2), y2, z2, u2, w2)\n",
    "    x = np.asarray([scale_input(x1), scale_input(x2)])\n",
    "    p = policy(torch.from_numpy(x).to(device))\n",
    "    print(\"policy(x)\", p)\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    sp = sm(p.detach().to('cpu'))\n",
    "    print(\"softmax(p),\", sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMuJlNZbyZA7"
   },
   "source": [
    "# Game visualization\n",
    "pong_utils contain a play function given the environment and a policy. An optional preprocess function can be supplied. Here we define a function that plays a game and shows learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Za18MpsyZA7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plutils.play(env, policy, time=100) \n",
    "# try to add the option \"preprocess=pong_utils.preprocess_single\"\n",
    "# to see what the agent sees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOAWmC1lV-HD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Observe untrained agent\n",
    "\n",
    "seed=1234\n",
    "steps = 100\n",
    "smax = nn.Softmax(dim=1)\n",
    "\n",
    "state, _ = env.reset(seed=seed)\n",
    "frame = env.render()\n",
    "frame = pix_norm(frame)\n",
    "img = plt.imshow(frame)\n",
    "action =  np.random.choice(ACTIONS)   \n",
    "for i in range(steps):\n",
    "    state1, reward1, is_done, is_trunc, info = env.step(action)\n",
    "    frame1 =  env.render()\n",
    "    state2, reward2, is_done, is_trunc, info = env.step(0)\n",
    "    frame2 =  env.render()\n",
    "    states = np.asarray([scale_input(state1),scale_input(state2)])\n",
    "    vals = policy(torch.from_numpy(states).to(device))\n",
    "    probs = smax(vals).detach().cpu().numpy()\n",
    "    action = np.argmax(probs[-1])\n",
    "    #print(action)\n",
    "    reward = round((2*reward1+reward2)/3, 2)\n",
    "    \n",
    "    #frame = pix_norm(frame2)#      (frame1 + frame2)/2)\n",
    "    frame = scale(frame2-frame1)\n",
    "    img.set_data(frame)\n",
    "    plt.title(str(i)+\"   \"+str(action)+\"   \"+str(reward))\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    if is_done or is_trunc:\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UfGiK-IyZA8",
    "tags": []
   },
   "source": [
    "# Rollout\n",
    "Before we start the training, we need to collect samples. To make things efficient we use parallelized environments to collect multiple examples at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2x_DGGGyZA8",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import lunar_PPO_utils as plutils\n",
    "envs = plutils.parallelEnv('LunarLander-v2', n=4, seed=1234)\n",
    "vals, probs, states, actions, rewards =  plutils.collect_trajectories(envs, policy, tmax=100, skip=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCVhvhANyZA8",
    "tags": []
   },
   "source": [
    "# Function Definitions\n",
    "Here you will define key functions for training. \n",
    "\n",
    "## Exercise 2: write your own function for training\n",
    "(this is the same as policy_loss except the negative sign)\n",
    "\n",
    "### REINFORCE\n",
    "you have two choices (usually it's useful to divide by the time since we've normalized our rewards and the time of each trajectory is fixed)\n",
    "\n",
    "1. $\\frac{1}{T}\\sum^T_t R_{t}^{\\rm future}\\log(\\pi_{\\theta'}(a_t|s_t))$\n",
    "2. $\\frac{1}{T}\\sum^T_t R_{t}^{\\rm future}\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}$ where $\\theta'=\\theta$ and make sure that the no_grad is enabled when performing the division\n",
    "\n",
    "### PPO\n",
    "Later on, you'll implement the PPO algorithm as well, and the scalar function is given by\n",
    "$\\frac{1}{T}\\sum^T_t \\min\\left\\{R_{t}^{\\rm future}\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)},R_{t}^{\\rm future}{\\rm clip}_{\\epsilon}\\!\\left(\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}\\right)\\right\\}$\n",
    "\n",
    "the ${\\rm clip}_\\epsilon$ function is implemented in pytorch as ```torch.clamp(ratio, 1-epsilon, 1+epsilon)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### If policy returns raw values:\n",
    "\n",
    "vals2probs = nn.Softmax(dim=1)\n",
    "\n",
    "n=len(envs.ps)\n",
    "envs.reset()\n",
    "\n",
    "for _ in range(10):\n",
    "    _, _, _, _ = envs.step(np.random.choice(ACTIONS,n))\n",
    "    states, _, _, _ = envs.step([0]*n) #fire main engine once each\n",
    "\n",
    "batch_input = scale_input_batch(states.copy())\n",
    "vals = policy(torch.from_numpy(batch_input).to(device))\n",
    "probs = vals2probs(vals).squeeze().cpu().detach().numpy()\n",
    "actions = np.argmax(probs, axis=1)\n",
    "\n",
    "states, scale_input_batch(states), vals, probs, actions\n",
    "#sts = torch.stack(torch.tensor(states, dtype=torch.float32))\n",
    "#policy_input = sts.view(-1,*sts.shape[-2:])\n",
    "#policy(policy_input).view(states.shape[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, probs, states, actions, rewards =  plutils.collect_trajectories(envs, policy, tmax=100, skip=4)\n",
    "sts = tuple(torch.from_numpy(s) for s in states)\n",
    "print(len(sts), sts[0].shape)\n",
    "sts = torch.stack(sts)\n",
    "print(len(states), states[0].shape, sts.shape)\n",
    "policy_input = sts.view(-1, sts.shape[-1])\n",
    "print(sts.shape, policy_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8gfuHvAyZA8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clipped surrogate function\n",
    "# return clipped sum of log-prob divided by T\n",
    "# same thing as -policy_loss but clipped\n",
    "def clipped_surrogate(policy, old_vals, \n",
    "                      states, actions, rewards,\n",
    "                      discount=0.995, epsilon=0.1, beta=0.01):\n",
    "    '''Clipped Surrogate Function\n",
    "       \n",
    "       Args:\n",
    "       policy (torch.nn):\n",
    "       old_vals (list of np.arrays):\n",
    "       states: list of np.arrays\n",
    "       actions: list of np.arrays\n",
    "       rewards: list of np.arrays\n",
    "       discount (np.float32):\n",
    "       epsilon (np.float32):\n",
    "       beta (np.float32):\n",
    "       \n",
    "       Returns (tensor): clipped sum of log-probs divided by T\n",
    "    '''\n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "    # convert rewards to future rewards\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "    \n",
    "    # convert states to raw policy output\n",
    "    # as log-probabilities of actions\n",
    "    new_vals = states_to_vals(policy, states)\n",
    "    # ratio for clipping\n",
    "    ratio = new_vals/old_vals\n",
    "        \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    #states = torch.tensor(states, dtype=torch.float, device=device)\n",
    "    old_vals = torch.tensor(old_vals, dtype=torch.float, device=device)\n",
    "    new_vals = torch.tensor(new_vals, dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "\n",
    "\n",
    "    # clip before applying ratio to rewards\n",
    "    clip = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "    clipped_surrogate = torch.min(ratio*rewards, clip*rewards)\n",
    "\n",
    "    # include a regularization term to steer new_policy towards 0.5\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -1*(new_vals*torch.log(old_vals+1.e-10)+(1.0-new_vals)*torch.log(1.0-old_vals+1.e-10))\n",
    "\n",
    "    \n",
    "    # returns an average of all the entries of the tensor\n",
    "    # effective computing L_sur^clip / T averaged over time-step and number of trajectories\n",
    "    # after all rewards have been normalized\n",
    "    # This is the value to maximize.\n",
    "    return torch.mean(clipped_surrogate + beta*entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "old_vals, probs, states, actions, rewards =  plutils.collect_trajectories(envs, policy, tmax=100, skip=10)\n",
    "discount=0.995\n",
    "epsilon=0.1\n",
    "beta=0.01\n",
    "print(\"1:\",[np.asarray(x).shape for x in [old_vals, states, actions, rewards]])\n",
    "\n",
    "\n",
    "discount = discount**np.arange(len(rewards))\n",
    "rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "print(\"2:\",discount.shape, discount[:,np.newaxis].shape, rewards.shape)\n",
    "\n",
    "rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "mean = np.mean(rewards_future, axis=1)\n",
    "std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "print(\"3:\",rewards_future.shape, rewards_normalized.shape, mean.shape)\n",
    "\n",
    "new_vals = states_to_vals(policy, states)\n",
    "old_vals = torch.tensor(old_vals, dtype=torch.float, device=device)\n",
    "states = torch.tensor(states, dtype=torch.float, device=device)\n",
    "rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "print(\"4:\",[x.shape for x in [states, old_vals, new_vals]])\n",
    "\n",
    "ratio = new_vals/old_vals\n",
    "clip = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "print(\"5:\",[x.shape for x in [rewards, torch.view(rewards), ratio, clip]])\n",
    "\n",
    "clipped_surrogate = torch.min(ratio*rewards, clip*rewards)\n",
    "entropy = -1*(new_vals*torch.log(old_vals+1.e-10)+(1.0-new_vals)*torch.log(1.0-old_vals+1.e-10))\n",
    "result=torch.mean(clipped_surrogate + beta*entropy)\n",
    "print(\"6:\",[x.shape for x in [clipped_surrogate, entropy, result]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "old_vals, probs, states, actions, rewards =  plutils.collect_trajectories(envs, policy, tmax=100, skip=10)\n",
    "discount=0.995\n",
    "epsilon=0.1\n",
    "beta=0.01\n",
    "print(\"1:\",[np.asarray(x).shape for x in [states, rewards]])\n",
    "print([r for r in rewards])\n",
    "\n",
    "\n",
    "discount = discount**np.arange(len(rewards))\n",
    "rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "print(\"2:\",discount.shape, discount[:,np.newaxis].shape, rewards.shape)\n",
    "\n",
    "print(discount)\n",
    "print(discount[:,np.newaxis])\n",
    "print(rewards)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_vals, probs, states, actions, rewards =  plutils.collect_trajectories(envs, policy, tmax=100, skip=10)\n",
    "new_vals = states_to_vals(policy, states)\n",
    "ratio = new_vals/np.asarray(old_vals)\n",
    "\n",
    "print(\"1:\",[x.shape for x in [ratio, np.asarray(rewards)]])\n",
    "print(ratio)\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(rewards[0].shape, rewards[0])\n",
    "print(ratio[0].shape, ratio[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print( rewards[:,None,:] == rewards[:,None,:]*ratio)\n",
    "[x.shape for x in [rewards, rewards[:,None,:], ratio, rewards[:,None,:]*ratio]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "old_vals, probs, states, actions, rewards =  plutils.collect_trajectories(envs, policy, tmax=100, skip=4)\n",
    "\n",
    "clipped_surrogate(policy, old_vals, states, actions, rewards, epsilon=0.1, beta=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHN9oeSnyZA8",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Training\n",
    "We are now ready to train our policy!\n",
    "WARNING: make sure to turn on GPU, which also enables multicore processing. It may take up to 45 minutes even with GPU enabled, otherwise it will take much longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5QGfS02p-XB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################### PPO ###############################\n",
    "#### WARNING: running through all 800 episodes will take 30-45 minutes\n",
    "\n",
    "# training loop max iterations\n",
    "episodes = 100\n",
    "# episode = 800\n",
    "\n",
    "# widget bar to display progress\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episodes).start()\n",
    "\n",
    "envs = parallelEnv('LunarLander-v2', n=8, seed=1234)\n",
    "\n",
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "tmax = 320\n",
    "SGD_epoch = 4\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episodes):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_vals, old_probs, states, actions, rewards = plutils.collect_trajectories(envs, policy, tmax=tmax, skip=6)\n",
    "        \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "    # gradient ascent step\n",
    "    for _ in range(SGD_epoch):\n",
    "        \n",
    "        # utilize your own clipped function!\n",
    "        L = -clipped_surrogate(policy, old_vals, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "    \n",
    "    # the clipping parameter reduces as time goes on\n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_aALEueyZA9"
   },
   "outputs": [],
   "source": [
    "# save your policy!\n",
    "torch.save(policy, 'PPO.policy')\n",
    "\n",
    "# load your policy if needed\n",
    "# policy = torch.load('REINFORCE_OG.policy')\n",
    "\n",
    "# try and test out the solution!\n",
    "# policy = torch.load('PPO_OG.policy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gm3n6N4byZA9"
   },
   "outputs": [],
   "source": [
    "plt.plot(mean_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Observe Gameplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcifaOgcyZA9"
   },
   "outputs": [],
   "source": [
    "# play game after training!\n",
    "play(env, policy, time=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Observe agent gameplay\n",
    "\n",
    "t = 300\n",
    "smax = nn.Softmax(dim=1)\n",
    "\n",
    "state, _ = env.reset(seed=SEED)\n",
    "frame = env.render()\n",
    "img = plt.imshow(pix_norm(frame))\n",
    "action =  np.random.choice(ACTIONS)   \n",
    "for _ in range(t):\n",
    "    state1, reward1, is_done, is_trunc, info = env.step(action)\n",
    "    frame1 =  pix_norm(env.render())\n",
    "    state2, reward2, is_done, is_trunc, info = env.step(OFF)\n",
    "    frame2 =  pix_norm(env.render())\n",
    "    states = np.asarray([scale_input(state1),\n",
    "                         scale_input(state2)])\n",
    "    probs = np.asarray([agent.act(s) for s in states])\n",
    "    #vals = smax(sigs).detach().cpu().numpy()\n",
    "    #probs = probs.detach().cpu().numpy()\n",
    "    action = np.argmax(probs[-1])\n",
    "    \n",
    "    reward = reward2 + reward1/2\n",
    "    \n",
    "    frame = scale(frame2-frame1/2)\n",
    "    img.set_data(frame)\n",
    "    plt.title(str(action)+\"    \"+str(np.round(reward, 3)))\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    if is_done or is_trunc:\n",
    "        break "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "BKH0-yBwyZA4",
    "wpACk9hYyZA6",
    "7yutVg1-yZA7",
    "yCVhvhANyZA8"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
