{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fqr9l3fYACgB"
   },
   "source": [
    "# Udacity Deep Reinforcement Learning Project 1: Navigation \n",
    "## *Learning to Navigate through a Banana-Rich Environment*\n",
    "\n",
    "### **Project Description**\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: https://user-images.githubusercontent.com/10624937/42135619-d90f2f28-7d12-11e8-8823-82b970a54d7e.gif \"Trained Agent\"\n",
    "\n",
    "A reinforcement learning agent will be trained to navigate on a 2D surface by choosing in which direction to travel given 37 dimensions of information about its environment at each time step, such as its location, momentum, and the distance from it to the bananas and walls in its \"view\". The agent must respond to each state with one of these four actions:\n",
    "\n",
    "- **`0`** - move forward.\n",
    "- **`1`** - move backward.\n",
    "- **`2`** - turn left.\n",
    "- **`3`** - turn right.\n",
    "\n",
    "Visual representation of the state space from the agent's point of view:\n",
    "\n",
    "![Trained Agent][image1]\n",
    "\n",
    "The goal of training is for the agent to develop a policy of actions maximizing the agent's collisions with yellow bananas while minimizing its collisions with blue bananas. Therefore at each step the agent recieves one of three awards according to the outcome of the agent's previous choice of action:\n",
    "-  **` 0`**  - no collision, or collision with wall\n",
    "-  **`+1`** - successfully running over a yellow banana\n",
    "-  **`-1`** - any contact with a blue banana\n",
    "\n",
    "Episodes end after a pre-specified number of steps (with the default apparently set to a maximum of 300 steps by the environment). An episode's final score is the unweighted sum of its rewards after each step.\n",
    "\n",
    "The agent will use Deep Q-Learning techniques to asymptotically approximate the optimal policy for collecting as many yellow bananas as possible while avoiding blue bananas over the preset number of steps for each episode.  \n",
    "\n",
    "The training benchmark specified in the [project rubric](https://review.udacity.com/#!/rubrics/3641/view) is to achieve an average score of at least 13 (over 100 consecutive episodes) over only 1800 training episodes or less.\n",
    "\n",
    "\n",
    "#### **References and Resources**\n",
    "\n",
    "- Background and design of Deep Q-Learning models and training methodologies covered in the [Course Textbook](http://go.udacity.com/rl-textbook) Chapter 6 Section 5.\n",
    "- DQN algorithms used in the project developed from [this paper in *Nature*](https://www.nature.com/articles/nature14236) \n",
    " - Mnih, V., Kavukcuoglu, K., Silver, D. *et al.* Human-level control through deep reinforcement learning. *Nature* **518**, 529â€“533 (2015). https://doi.org/10.1038/nature14236\n",
    "\n",
    "- [Unity Machine Learning Agents](https://blogs.unity3d.com/2017/09/19/introducing-unity-machine-learning-agents/) Environment Resource\n",
    "\n",
    "**Course code:**\n",
    "- Copied to project folder directly from [DQN solution folder](https://github.com/udacity/deep-reinforcement-learning/blob/master/dqn/solution/) and used here as-is:\n",
    " - dqn_agent.py\n",
    " - model.py\n",
    "- Some modifications (discussed below) were made to the training function in the [DQN solution notebook](dqn_solution.ipynb), some cells from which have been copied into this Report notebook.\n",
    "\n",
    "**Saved weights** of agents trained to benchmark or better performance: \n",
    "- [benchmark_weights](benchmark_weights.pth): Stored weights for a QNetwork right after achieving the benchmark average score of 13+\n",
    "- [trained_weights](trained_weights.pth'):  Stored weights for a QNetwork after training it on a full set of 2000 episodes, achieving a final average score of about 16.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sn8D7gHOymG"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwRIbUDoACf-"
   },
   "source": [
    "## Environment Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7ei34N2ACgC"
   },
   "source": [
    "### Initialize a UnityAgents Banana-World Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dO-1r7UDTMZL",
    "outputId": "3e256f04-b7ea-412e-f2fc-471a5d8566d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#!pip -q install ./python\n",
    "from unityagents import UnityEnvironment\n",
    "# change file_name if necessary:\n",
    "env = UnityEnvironment(file_name='.\\Banana_Windows_x86_64\\Banana.exe', no_graphics=True, worker_id=88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eP0uvSXILZJ8"
   },
   "source": [
    "### Load and Display UnityAgent Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vWNXs3JTACgC",
    "outputId": "4264e77f-6983-4f56-8b30-73c8fca98d3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BananaBrain\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n",
      "Agents:  1\n",
      "Actions:  4\n",
      "State size:  37\n",
      "\n",
      "Action: 0\n",
      "Next State:\n",
      "[1.         0.         0.         0.         0.84173167 0.\n",
      " 0.         1.         0.         0.06433759 0.         1.\n",
      " 0.         0.         0.2517986  1.         0.         0.\n",
      " 0.         0.72478515 0.         1.         0.         0.\n",
      " 0.2527748  0.         0.         1.         0.         0.08042004\n",
      " 0.         1.         0.         0.         0.31255436 0.\n",
      " 7.81049442]\n",
      "Reward: 0.0\n",
      "Done: False\n"
     ]
    }
   ],
   "source": [
    "# Load the \"banana brain\" brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "print(brain_name)\n",
    "print(brain)\n",
    "\n",
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# Agent Information\n",
    "print(\"Agents: \", len(env_info.agents))\n",
    "\n",
    "# Action Space\n",
    "action_size = brain.vector_action_space_size\n",
    "print(\"Actions: \", action_size)\n",
    "\n",
    "# State Space \n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "state_size = len(state)\n",
    "print(\"State size: \", state_size)\n",
    "\n",
    "# State observation and Unity environment syntax\n",
    "score = 0                                          # initialize the score\n",
    "action = np.random.randint(action_size)            # select a (random) action\n",
    "env_info = env.step(action)[brain_name]            # send the action to the environment\n",
    "next_state = env_info.vector_observations[0]       # get the next state\n",
    "reward = env_info.rewards[0]                       # get the reward\n",
    "done = env_info.local_done[0]                      # see if episode has finished\n",
    "\n",
    "print(\"\\nAction: {}\\nNext State:\\n{}\\nReward: {}\\nDone: {}\".format(action, next_state, reward, done))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0t_R0dvACgD"
   },
   "source": [
    "## Project Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lAHrnGhpACgG"
   },
   "outputs": [],
   "source": [
    "from dqn_agent import Agent\n",
    "\n",
    "### An Agent's Network Hyperparameters\n",
    "### (defaults are set in [dqn_agent](dqn_agent.py))\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor used by Mnih et al. (2015)\n",
    "TAU = 1e-3              # for (very!) 'soft update' of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network, originally 4 (why?)\n",
    "\n",
    "MAX_T = 301             # Max steps per episode; environment default may be 300\n",
    "fc_units = 64           # 32 in the model described in Mnih et al. (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FA2MwrULACgE"
   },
   "source": [
    "### The (Slightly) Modified Deep Q-Network training function\n",
    "#### **dqn**(*agent, n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.1, eps_step=0.001*)\n",
    "\n",
    "Some of the *slight* modifications to \n",
    "The code provided in the [Deep Q-Network Solution notebook](https://github.com/udacity/deep-reinforcement-learning/tree/master/dqn/solution) required a few very slight changes to account for the difference in environment syntax.\n",
    "- small changes in syntax for interaction with the UnityAgent environment\n",
    "- epsilon now decreases *linearly* at default rate of 0.001/episode; epsilon decayed exponentially in original (inspired by Mnih *et al.* (2015))\n",
    "- the Agent model to be trained must be provided to dqn(); this change just made it easier to work with different models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GpM6TIOCACgG"
   },
   "outputs": [],
   "source": [
    "#@title Function **dqn**(*agent, n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.1, eps_step=0.001*)\n",
    "\n",
    "### This dqn algorithm is slightly modified from the one provided\n",
    "### in the solution folder for the DQN coding exercise.\n",
    "### The difference is mostly the change in syntax for interacting \n",
    "### with the UnityAgents environment, as provided by the course\n",
    "### in the [Deep Q Network solution](https://github.com/udacity/deep-reinforcement-learning/tree/master/dqn/solution) directory\n",
    "\n",
    "from collections import deque\n",
    "import torch\n",
    "\n",
    "def dqn(agent, n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_step=1e-3):\n",
    "    \"\"\"The Deep Q-Learning algorithm from the DQN solution folder, with some minor changes:\n",
    "           - small changes in syntax for interaction with UA environment\n",
    "           - epsilon decays **linearly**\n",
    "           - the Agent model being trained must be explicitly specified  \n",
    "    Params\n",
    "    ======\n",
    "        agent (object): name of agent to train\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode (env default max: 300)\n",
    "        eps_start (float): starting value of epsilon for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_step (float): *linear* factor decreasing epsilon after each episode\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    window_size = 100                  # number of scores to rolling-remember\n",
    "    scores_window = deque(maxlen=window_size)\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    dones = 0                          # count dones to see if there are any\n",
    "    SOLVED = False                     # training will continue past benchmark\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "\n",
    "        #### vvv Changes made here vvv\n",
    "        #### UnityAgents Environment syntax:\n",
    "        env_info = env.reset(train_mode=True)[brain_name]  # reset the environment\n",
    "        state = env_info.vector_observations[0]            # get the current state\n",
    "        score = 0                                          # reset score\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)                 # epsilon-greedy action choice\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "        #### ^^^ End of syntax changes^^^\n",
    "\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                ### Question: are there ever any dones?\n",
    "                dones += 1\n",
    "                break \n",
    "        \n",
    "        scores_window.append(score)       # save episode score for averaging\n",
    "        scores.append(score)              # save episode score\n",
    "        eps = max(eps_end, eps-eps_step)  # decrease epsilon LINEARLY to minimum of eps_end\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tEpsilon: {:.3f}'.format(i_episode, np.mean(scores_window), eps))\n",
    "        \n",
    "        if np.mean(scores_window)>=13.0 and not SOLVED:\n",
    "            print('\\nEnvironment solved in {:d} episodes!'.format(i_episode))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'benchmark_weights.pth')\n",
    "            #break\n",
    "            ### continue training...\n",
    "            SOLVED = True\n",
    "            \n",
    "    ### Final small cosmetic change made here:\n",
    "    print('\\r\\nTotal Episodes {}\\tFinal Average Score: {:.2f}'.format(n_episodes, np.mean(scores_window)))\n",
    "    torch.save(agent.qnetwork_local.state_dict(), 'trained_weights.pth')    \n",
    "    print(\"Dones: \", dones)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.int64' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m state \u001b[38;5;241m=\u001b[39m env_info\u001b[38;5;241m.\u001b[39mvector_observations[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[1;32m----> 4\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m[brain_name]\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\general\\lib\\site-packages\\unityagents\\environment.py:322\u001b[0m, in \u001b[0;36mUnityEnvironment.step\u001b[1;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnityActionException(\n\u001b[0;32m    319\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are no external brains in the environment, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep cannot take a value input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 322\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m brain_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[43mvector_action\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(memory\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(text_action\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m brain_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_external_brain_names:\n\u001b[0;32m    324\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnityActionException(\n\u001b[0;32m    325\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe name \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not correspond to an external brain \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    326\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the environment\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(brain_name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.int64' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "state = env_info.vector_observations[0]\n",
    "action = agent.act(state)\n",
    "env.step(action)[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMtYLzOQACgG"
   },
   "source": [
    "## Agent Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "C1-LJvb3ACgH",
    "outputId": "ca95a60a-6735-47f1-dfb2-5786940f7c1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.00"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.int64' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(state_size, action_size, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Run DQN Training\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mdqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m             \u001b[49m\u001b[43meps_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 43\u001b[0m, in \u001b[0;36mdqn\u001b[1;34m(agent, n_episodes, max_t, eps_start, eps_end, eps_step)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_t):\n\u001b[0;32m     42\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state, eps)                 \u001b[38;5;66;03m# epsilon-greedy action choice\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     env_info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m[brain_name]        \u001b[38;5;66;03m# send the action to the environment\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m env_info\u001b[38;5;241m.\u001b[39mvector_observations[\u001b[38;5;241m0\u001b[39m]   \u001b[38;5;66;03m# get the next state\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     reward \u001b[38;5;241m=\u001b[39m env_info\u001b[38;5;241m.\u001b[39mrewards[\u001b[38;5;241m0\u001b[39m]                   \u001b[38;5;66;03m# get the reward\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\general\\lib\\site-packages\\unityagents\\environment.py:322\u001b[0m, in \u001b[0;36mUnityEnvironment.step\u001b[1;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnityActionException(\n\u001b[0;32m    319\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are no external brains in the environment, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep cannot take a value input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 322\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m brain_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[43mvector_action\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(memory\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(text_action\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m brain_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_external_brain_names:\n\u001b[0;32m    324\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnityActionException(\n\u001b[0;32m    325\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe name \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not correspond to an external brain \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    326\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the environment\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(brain_name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.int64' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "# Initialize a new agent\n",
    "agent = Agent(state_size, action_size, seed=10)\n",
    "\n",
    "# Run DQN Training\n",
    "scores = dqn(agent, n_episodes=2000, max_t=500, \n",
    "             eps_start=1.0, eps_end=0.001, eps_step=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bF1-Gry0ACgH"
   },
   "source": [
    "## Results\n",
    "\n",
    "**Benchmark average score of 13+ was reached by this agent after 924 episodes.**\n",
    "\n",
    "**Final average score after 2000 training episodes was 15+.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-Q1oz4cACgH"
   },
   "outputs": [],
   "source": [
    "### Calculate average scores over past 100 episodes\n",
    "mean_scores = []\n",
    "for i in np.arange(1,len(scores)):\n",
    "    if i < 100:\n",
    "        mean_score = np.mean(scores[:i])\n",
    "    else:\n",
    "        mean_score = np.mean(scores[i-100:i])       \n",
    "    mean_scores.append(mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MixSPxY4NQiB"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSH9c7gmACgH",
    "outputId": "8e89a6da-2626-49e3-a96c-dcb7fffd8261"
   },
   "outputs": [],
   "source": [
    "### Display results\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "### plot episode scores in blue:\n",
    "plt.plot(np.arange(len(scores)), scores, 'bo--', linewidth=0.25, markersize=0.5,)\n",
    "\n",
    "### plot running average in red:\n",
    "plt.plot(np.arange(10, len(mean_scores)), mean_scores[10:], 'ro-', linewidth=0.25, markersize=0.1,)\n",
    "\n",
    "### plot benchmark line\n",
    "plt.plot(np.arange(len(mean_scores)), np.ones((len(mean_scores)))*13, 'ko-', linewidth=0.1, markersize=0.1,)\n",
    "\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbPd61NDLEo5"
   },
   "source": [
    "## Future Directions and Possible Project Improvements\n",
    "\n",
    "A few **unanswered questions to pursue** after this project include:\n",
    "\n",
    "- The UPDATE_EVERY constant operates how?\n",
    "- Why set to 4? Isn't that low?\n",
    "\n",
    "\n",
    "From Agent.py about line 105:\n",
    "```\n",
    "# Compute Q targets for current states \n",
    "# Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "Is the (1-dones) term there because predicted rewards should always be 0 for terminal state...?\n",
    "Are there ever any dones...? Or does every timed-out episode get a done on final step?\n",
    "```\n",
    "\n",
    "Other questions and likely directions for future work concerning:\n",
    "- QNetwork and Agent Structure\n",
    "- Temporal-Difference as a Loss function\n",
    "- Replay memory --> **priority** replay\n",
    "- DQN Design and Mods\n",
    "- **from-pixels** challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cj0aFsvOLZJ-"
   },
   "source": [
    "## Close environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REiRzTCUACgH"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOz765AWMWhL"
   },
   "source": [
    "## Appendix I: Unnecessary Data for Future Consideration"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "lKZPbIiwLZJ_"
   },
   "source": [
    "Episode 100\tAverage Score: 0.08\tEpsilon: 0.900\n",
    "Episode 200\tAverage Score: 0.75\tEpsilon: 0.800\n",
    "Episode 300\tAverage Score: 1.90\tEpsilon: 0.700\n",
    "Episode 400\tAverage Score: 3.07\tEpsilon: 0.600\n",
    "Episode 500\tAverage Score: 5.05\tEpsilon: 0.500\n",
    "Episode 600\tAverage Score: 6.62\tEpsilon: 0.400\n",
    "Episode 700\tAverage Score: 8.48\tEpsilon: 0.300\n",
    "Episode 800\tAverage Score: 10.44\tEpsilon: 0.200\n",
    "Episode 900\tAverage Score: 12.06\tEpsilon: 0.100\n",
    "Episode 990\tAverage Score: 13.01\n",
    "Environment solved in 990 episodes!\tAverage Score: 13.01\n",
    "Episode 1000\tAverage Score: 13.01\tEpsilon: 0.001\n",
    "Episode 1100\tAverage Score: 14.45\tEpsilon: 0.001\n",
    "Episode 1200\tAverage Score: 14.75\tEpsilon: 0.001\n",
    "Episode 1300\tAverage Score: 15.43\tEpsilon: 0.001\n",
    "Episode 1400\tAverage Score: 15.50\tEpsilon: 0.001\n",
    "Episode 1500\tAverage Score: 16.34\tEpsilon: 0.001\n",
    "Episode 1600\tAverage Score: 16.98\tEpsilon: 0.001\n",
    "Episode 1700\tAverage Score: 16.47\tEpsilon: 0.001\n",
    "Episode 1800\tAverage Score: 16.48\tEpsilon: 0.001\n",
    "Episode 1900\tAverage Score: 16.13\tEpsilon: 0.001\n",
    "Episode 2000\tAverage Score: 16.08\tEpsilon: 0.001\n",
    "\n",
    "Total Episodes 2000\tFinal Average Score: 16.08\n",
    "Dones:  2000"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "2MvATFmWACgH"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "UPDATE_EVERY = 12\n",
    "TAU = 0.001\n",
    "MAX_T = 200\n",
    "agent = Agent(state_size, action_size, seed=10)\n",
    "scores, steps = dqn(agent, n_episodes=1800, max_t=MAX_T, \n",
    "                    eps_start=1.0, eps_end=0.01, eps_step=0.001)\n",
    "                    \n",
    "Episode 100\tAverage Score: -0.08\tAverage Steps: 199.00\tEpsilon: 0.90\n",
    "Episode 200\tAverage Score: 0.05\tAverage Steps: 199.00\tEpsilon: 0.80\n",
    "Episode 300\tAverage Score: 0.26\tAverage Steps: 199.00\tEpsilon: 0.70\n",
    "Episode 400\tAverage Score: 0.69\tAverage Steps: 199.00\tEpsilon: 0.60\n",
    "Episode 500\tAverage Score: 1.01\tAverage Steps: 199.00\tEpsilon: 0.50\n",
    "Episode 600\tAverage Score: 1.97\tAverage Steps: 199.00\tEpsilon: 0.40\n",
    "Episode 700\tAverage Score: 3.28\tAverage Steps: 199.00\tEpsilon: 0.30\n",
    "Episode 800\tAverage Score: 5.13\tAverage Steps: 199.00\tEpsilon: 0.20\n",
    "Episode 900\tAverage Score: 5.74\tAverage Steps: 199.00\tEpsilon: 0.10\n",
    "Episode 1000\tAverage Score: 6.68\tAverage Steps: 199.00\tEpsilon: 0.01\n",
    "Episode 1100\tAverage Score: 7.15\tAverage Steps: 199.00\tEpsilon: 0.01\n",
    "Episode 1200\tAverage Score: 8.09\tAverage Steps: 199.00\tEpsilon: 0.01\n",
    "Episode 1300\tAverage Score: 9.34\tAverage Steps: 199.00\tEpsilon: 0.01\n",
    "Episode 1400\tAverage Score: 9.80\tAverage Steps: 199.00\tEpsilon: 0.01\n",
    "Episode 1500\tAverage Score: 10.59\tAverage Steps: 199.00\tEpsilon: 0.01\n",
    "Episode 1600\tAverage Score: 10.55\tAverage Steps: 199.00\tEpsilon: 0.01\n",
    "Episode 1700\tAverage Score: 11.08\tAverage Steps: 199.00\tEpsilon: 0.01\n",
    "Episode 1800\tAverage Score: 10.27\tAverage Steps: 199.00\tEpsilon: 0.01\n",
    "\n",
    "\n",
    "UPDATE_EVERY = 8\n",
    "#TAU = 0.001\n",
    "MAX_T = 500\n",
    "\n",
    "agent = Agent(state_size, action_size, seed=10)\n",
    "#agent = agent.qnetwork_local.load_state_dict(torch.load('solved_point.pth'))\n",
    "\n",
    "scores, steps = dqn(agent, n_episodes=1200, max_t=MAX_T, \n",
    "                    eps_start=1.0, eps_end=0.01, eps_step=0.001)\n",
    "Episode 100\tAverage Score: -0.09\tAverage Steps: 299.00\tEpsilon: 0.90\n",
    "Episode 200\tAverage Score: 0.41\tAverage Steps: 299.00\tEpsilon: 0.80\n",
    "Episode 300\tAverage Score: 1.52\tAverage Steps: 299.00\tEpsilon: 0.70\n",
    "Episode 400\tAverage Score: 2.77\tAverage Steps: 299.00\tEpsilon: 0.60\n",
    "Episode 500\tAverage Score: 4.63\tAverage Steps: 299.00\tEpsilon: 0.50\n",
    "Episode 600\tAverage Score: 5.90\tAverage Steps: 299.00\tEpsilon: 0.40\n",
    "Episode 700\tAverage Score: 7.76\t    Average Steps: 299.00\tEpsilon: 0.30\n",
    "Episode 800\tAverage Score: 9.37\t    Average Steps: 299.00\tEpsilon: 0.20\n",
    "Episode 900\tAverage Score: 11.76\tAverage Steps: 299.00\tEpsilon: 0.10\n",
    "\n",
    "Episode 954\tAverage Score: 13.01\tEpsilon: 0.05\n",
    "Environment solved in 954 episodes!\tAverage Score: 13.01\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "**UPDATE_EVERY = 10**\n",
    "Episode 100\t  Average Score: -0.03\t    Epsilon: 0.90\n",
    "Episode 200\t  Average Score: 0.27\t    Epsilon: 0.80\n",
    "...\n",
    "Episode 800\t  Average Score: 9.87\t    Epsilon: 0.20\n",
    "Episode 900\t  Average Score: 12.39\t    Epsilon: 0.10\n",
    "Episode 944\t  Average Score: 13.06      Epsilon: 0.10\n",
    "**Environment solved in 944 episodes!\tAverage Score: 13.06**\n",
    "\n",
    "\n",
    "**UPDATE_EVERY = 4**\n",
    "Episode 100\t    Average Score: 0.24\t    Epsilon: 0.90\n",
    "Episode 200\t    Average Score: 0.68\t    Epsilon: 0.80\n",
    "...\n",
    "Episode 1000\tAverage Score: 12.27\tEpsilon: 0.10\n",
    "Episode 1062\tAverage Score: 13.05    Epsilon: 0.10\n",
    "**Environment solved in 1062 episodes!\tAverage Score: 13.05**\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NOz765AWMWhL"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
