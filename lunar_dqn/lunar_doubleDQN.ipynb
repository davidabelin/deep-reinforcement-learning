{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArKFzS5ksz5C",
    "tags": []
   },
   "source": [
    "# Deep Q-Network (DQN): Discrete Lunar Lander problem\n",
    "---\n",
    "In this notebook, you will implement a DQN agent with OpenAI Gym's LunarLander-v2 environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x2c4faea2d00>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# render ai gym environment\n",
    "#!pip install gymnasium[box2d]\n",
    "import gymnasium as gym\n",
    "\n",
    "#!pip install progressbar\n",
    "import progressbar as pb\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "#%matplotlib inline\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "else:  \n",
    "    #!python -m pip install pyvirtualdisplay\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=True, size=(1400, 900))\n",
    "    display.start()\n",
    "\n",
    "# install package for displaying animation\n",
    "#!pip install JSAnimation\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import DQN_agent as dqn_utils\n",
    "from DQN_agent import Agent\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using device: \",device) \n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(5e5)  # replay buffer size int(1e5)\n",
    "BATCH_SIZE = 128         # minibatch size 64\n",
    "GAMMA = 0.99            # discount factor 0.99\n",
    "TAU = 1e-3              # for soft update of target parameters 1e-3\n",
    "LR = 2e-4               # learning rate  5e-4\n",
    "UPDATE_EVERY = 4        # how often to update the network 4\n",
    "OFF=0\n",
    "MAIN=2\n",
    "RIGHT=3\n",
    "LEFT=1\n",
    "ACTIONS=[0,2,3,1]\n",
    "num2act = {num:act for num, act in zip(ACTIONS, [\"OFF\", \"MAIN\", \"RIGHT\", \"LEFT\"])}\n",
    "LOWS=np.array([-1.5, -1.5, -5., -5., -3.1415927, -5., False, False])\n",
    "HIGHS=np.array([1.5, 1.5, 5., 5., 3.1415927, 5., True, True])\n",
    "norm = lambda x: (x - x.mean())/x.std() if x.std()!=0. else 0.\n",
    "pix_norm = lambda x: x/255.\n",
    "scale = lambda x: (x - x.min())/(x.max() - x.min()) if x.max()!=x.min() else x\n",
    "SEED = 1234\n",
    "\n",
    "def scale_input(state):\n",
    "    st = np.zeros_like(state)\n",
    "    st[:-2] = (state[:-2] - LOWS[:-2]) / (HIGHS[:-2] - LOWS[:-2])\n",
    "    st[-2:] = state[-2:] \n",
    "    return st\n",
    "\n",
    "def scale_input_batch(states):\n",
    "    sts =  np.array([scale_input(state) for state in states])     \n",
    "    return sts\n",
    "\n",
    "def prep_input(state0, state1):\n",
    "    '''Takes two sequential states and returns input to the network   \n",
    "        Params\n",
    "        states (np.array) shape (8,)\n",
    "        Returns (np.array) shape (2,8) prepped for input \n",
    "    '''\n",
    "    state0 = scale_input(state0)\n",
    "    state1 = scale_input(state1)\n",
    "    state = np.asarray([state0, state1])\n",
    "    return state  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print([k for k in gym.envs.registry.keys() if \"Lunar\" in k])  #.all().keys()  #.make\n",
    "#gym.envs.registry['LunarLanderContinuous-v2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4Sl_Wc_sz5E"
   },
   "source": [
    "### 2. Instantiate the Environment and Agent\n",
    "\n",
    "Initialize the environment in the code cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space is an 8-dimensional vector: \n",
    "* the coordinates of the lander (x & y), \n",
    "* its linear velocities (x & y), \n",
    "* its angle (radians), \n",
    "* its angular velocity, \n",
    "* and two booleans for whether/not each leg has ground contact.\n",
    "\n",
    "Observation Highs:\n",
    "* [1.5  1.5  5.0  5.0  3.14  5.0  True  True ]\n",
    "\n",
    "Observation Lows:\n",
    "* [-1.5  -1.5  -5.0  -5.0  -3.14  -5.0  False  False ]\n",
    "\n",
    "Wind function:\n",
    "\n",
    "tanh(sin(2 k (t+C)) + sin(pi k (t+C))). k is set to 0.01. C is sampled randomly between -9999 and 9999\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4ukSEf9sz5E"
   },
   "source": [
    "### Observation Space\n",
    "\n",
    "The observation space is an 8-dimensional vector: \n",
    "* the coordinates of the lander (x & y), \n",
    "* its linear velocities (x & y), \n",
    "* its angle (radians), \n",
    "* its angular velocity, \n",
    "* and two booleans for whether/not each leg has ground contact.\n",
    "\n",
    "Observation Highs:\n",
    "* [1.5  1.5  5.0  5.0  3.14  5.0  True  True ]\n",
    "\n",
    "Observation Lows:\n",
    "* [-1.5  -1.5  -5.0  -5.0  -3.14  -5.0  False  False ]\n",
    "\n",
    "Wind function:\n",
    "\n",
    "`tanh`(sin(2 k (t+C)) + sin(pi k (t+C))). k is set to 0.01. C is sampled randomly between -9999 and 9999\n",
    "\n",
    "\n",
    "### Discrete Action Space\n",
    "\n",
    "There are four discrete actions available:\n",
    "\n",
    "* 0: do nothing\n",
    "* 1: fire left orientation engine\n",
    "* 2: fire main engine\n",
    "* 3: fire right orientation engine\n",
    "\n",
    "\n",
    "### Rewards\n",
    "\n",
    "After every step a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
    "\n",
    "For each step, the reward:\n",
    "\n",
    "* is increased/decreased the closer/further the lander is to the landing pad.\n",
    "* is increased/decreased the slower/faster the lander is moving.\n",
    "* is decreased the more the lander is tilted (angle not horizontal).\n",
    "* is increased by 10 points for each leg that is in contact with the ground.\n",
    "* is decreased by 0.03 points each frame a side engine is firing.\n",
    "* is decreased by 0.3 points each frame the main engine is firing.\n",
    "\n",
    "The episode receives an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
    "\n",
    "An episode is considered a solution if it scores at least 200 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### \n",
    "OLD_GYM=False\n",
    "if OLD_GYM:\n",
    "    #env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "    env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\", options={'continuous': False,\n",
    "                                                                       'gravity': -9.81,\n",
    "                                                                       'enable_wind': True,\n",
    "                                                                       'wind_power': 1.5,\n",
    "                                                                       'turbulence_power': 0.15}\n",
    "                  )\n",
    "    #,\n",
    "    #continuous = False, #False\n",
    "    #gravity = -9.81, #0 and -12\n",
    "    #enable_wind = True, #False\n",
    "    #wind_power = 1.5, #0.0 and 20.0\n",
    "    #turbulence_power = 0.15, #0.0 and 2.0\n",
    "    #)\n",
    "\n",
    "    state, info = env.reset(seed = 1234)\n",
    "    obs = env.render()\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "        state, reward, done, trun, info = env.step(action)\n",
    "        print(action, reward)\n",
    "        obs = env.render()\n",
    "        plt.imshow(obs)\n",
    "\n",
    "    state_shape = env.observation_space.shape\n",
    "    state_size = state_shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    print('State shape: ', state_size)\n",
    "    print('Number of actions: ', action_size)\n",
    "    plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "id": "SY0GEqHHsz5E",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  8\n",
      "Number of actions:  4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2c49057e1f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### NEW GYM = GYMNASIUM\n",
    "#!pip install gymnasium[box2d]\n",
    "#import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\",\n",
    "                                 continuous= False,\n",
    "                                 gravity= -9.81,\n",
    "                                 enable_wind= True,\n",
    "                                 wind_power= 0.88,\n",
    "                                 turbulence_power= 0.08)\n",
    "            #new_step_api=False,  )\n",
    "\n",
    "state, info = env.reset(seed = 1234)\n",
    "obs = env.render()\n",
    "    \n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    state, reward, done, trun, info = env.step(action)\n",
    "    #print(action, reward)\n",
    "    obs = env.render()\n",
    "    plt.imshow(obs)\n",
    "\n",
    "state_shape = env.observation_space.shape\n",
    "state_size = state_shape[0]\n",
    "action_size = env.action_space.n\n",
    "print('State shape: ', state_size)\n",
    "print('Number of actions: ', action_size)\n",
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYCfgBlUsz5F"
   },
   "source": [
    "### 3. Train the Agent with DQN\n",
    "\n",
    "Run the code cell below to train the agent from scratch.  You are welcome to amend the supplied values of the parameters in the function, to try to see if you can get better performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Evaluator(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed=SEED):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(Evaluator, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.state0 = nn.Linear(state_size, 2*state_size)\n",
    "        self.state1 = nn.Linear(state_size, 2*state_size)\n",
    "        self.hidden = nn.Linear(4*state_size, 4*action_size)\n",
    "        self.out = nn.Linear(4*action_size, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\n",
    "           Shape of input state should be: (batch_size, 2, state_size)\n",
    "        \"\"\"\n",
    "        #print(\"input shape: \", state.shape)\n",
    "        s0 = state[:,0,:]#.squeeze()\n",
    "        s0 = F.relu(self.state0(s0))\n",
    "        #print(\"s0 shape: \", s0.shape)\n",
    "        s1 = state[:,1,:]#.squeeze()\n",
    "        s1 = F.relu(self.state1(s1)) \n",
    "        #print(\"s1 shape: \", s1.shape)\n",
    "        x = torch.cat((s0,s1), -1)\n",
    "        #print(\"concat shape: \", x.shape)\n",
    "        x = F.relu(self.hidden(x))\n",
    "        return self.out(x)\n",
    "\n",
    "evaluator = Evaluator(state_size, action_size)\n",
    "evaluator = evaluator.to(device)\n",
    "def evaluate(state, evaluator=evaluator):    \n",
    "    \"\"\"No_Grad a network to map state -> action values.\n",
    "       Shape of input state should be: (batch_size, 2, state_size)\n",
    "       Output shape should be: (batch_size, action_size)\n",
    "    \"\"\"\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "    evaluator.eval()\n",
    "    with torch.no_grad():\n",
    "        action_values = evaluator(state)\n",
    "    evaluator.train()\n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent = Agent(state_size, action_size, SEED)\n",
    "#agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Observe untrained agent\n",
    "\n",
    "seed=SEED\n",
    "t = 300\n",
    "total_reward = 0.\n",
    "smax = nn.Softmax(dim=-1).to(device)\n",
    "pix_norm = lambda x: x/255.\n",
    "\n",
    "state, _ = env.reset(seed=seed)\n",
    "frame = env.render()\n",
    "frame = pix_norm(frame)\n",
    "img = plt.imshow(frame)\n",
    "action =  np.random.choice(ACTIONS)   \n",
    "for _ in range(t):\n",
    "    state1, reward1, is_done, is_trunc, info = env.step(action)\n",
    "    frame1 =  pix_norm(env.render())\n",
    "    state2, reward2, is_done, is_trunc, info = env.step(0)\n",
    "    frame2 =  pix_norm(env.render())\n",
    "    state = np.asarray([scale_input(state1), \n",
    "                        scale_input(state2)])\n",
    "    ### Evaluator\n",
    "    pols = evaluate(state)\n",
    "    probs = smax(pols).detach().cpu().numpy()\n",
    "    #print(\"probs.shape\", probs.shape)\n",
    "    action = np.argmax(probs[-1]) #greedy choice of probs\n",
    "    polaction = np.argmax(pols[-1].detach().cpu().numpy()) #greedy choice of pols\n",
    "    #if action != polaction:\n",
    "        #print(\"Action: {}  Polaction: {}\".format(action, polaction))\n",
    "        \n",
    "    reward = round(reward1+reward2/2, 2)\n",
    "    total_reward = round(reward+total_reward, 2)\n",
    "    \n",
    "    #frame = pix_norm(frame2)#      (frame1 + frame2)/2)\n",
    "    frame = scale(frame2-frame1/2)\n",
    "    img.set_data(frame)\n",
    "    #pprobs = [p for p in np.round(probs[-1], 3)]\n",
    "    #plt.title(str(pprobs)+\"   \"+ dqn_utils.num2act(action))\n",
    "    plt.title(str(total_reward)+\"   \"+str(reward)+\"   \"+ dqn_utils.num2act[action])\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    if is_done or is_trunc:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_step(state, eps):\n",
    "    nul_state, reward1, done, trun, info = env.step(0)\n",
    "    if done or trun:\n",
    "        return nul_state, reward1, done or trun\n",
    "    two_stateA = np.asarray([scale_input(nul_state), scale_input(state)])\n",
    "    \n",
    "    action = agent.act(two_stateA, eps)\n",
    "    next_state, reward2, done, trun, info = env.step(action)\n",
    "    \n",
    "    two_stateB = np.asarray([scale_input(next_state), scale_input(nul_state)])\n",
    "    agent.step(two_stateA, action, reward2, two_stateB, done or trun)\n",
    "    return next_state, reward1+reward2, done or trun\n",
    "\n",
    "\n",
    "def train_dqn(n_episodes=2000, max_steps=1000, \n",
    "              eps_start=1.0, eps_end=0.005, eps_decay=0.99):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    window_size = 100                  # scores to rolling-remember\n",
    "    scores_window = deque(maxlen=window_size)\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        ### Begin new episode\n",
    "        score = 0  \n",
    "        state, _ = env.reset()\n",
    "        for t in range(max_steps): \n",
    "            state, reward, done = two_step(state, eps)\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)       # save most recent episode score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")  #\\tEpsilon: {:.2f}\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tEpsilon: {:.2f}'.format(i_episode, np.mean(scores_window), eps))\n",
    "        if np.mean(scores_window)>=121.0:\n",
    "            print('\\nEnvironment over-solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'highpoint.pth')\n",
    "            break\n",
    "        elif np.mean(scores_window)>=100.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')           \n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3\tAverage Score: -173.86"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m eps_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m      5\u001b[0m eps_decay \u001b[38;5;241m=\u001b[39m (eps_end\u001b[38;5;241m/\u001b[39meps_start)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mn_episodes)\n\u001b[1;32m----> 7\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_decay\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 36\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[1;34m(n_episodes, max_steps, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[0;32m     34\u001b[0m state, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps): \n\u001b[1;32m---> 36\u001b[0m     state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[43mtwo_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m, in \u001b[0;36mtwo_step\u001b[1;34m(state, eps)\u001b[0m\n\u001b[0;32m      8\u001b[0m next_state, reward2, done, trun, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     10\u001b[0m two_stateB \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([scale_input(next_state), scale_input(nul_state)])\n\u001b[1;32m---> 11\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtwo_stateA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtwo_stateB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrun\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m next_state, reward1\u001b[38;5;241m+\u001b[39mreward2, done \u001b[38;5;129;01mor\u001b[39;00m trun\n",
      "File \u001b[1;32m~\\Documents\\Repositories\\deep-reinforcement-learning\\lunar_dqn\\DQN_agent.py:89\u001b[0m, in \u001b[0;36mAgent.step\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m BATCH_SIZE:\n\u001b[0;32m     88\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGAMMA\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Repositories\\deep-reinforcement-learning\\lunar_dqn\\DQN_agent.py:145\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self, experiences, gamma)\u001b[0m\n\u001b[0;32m    142\u001b[0m states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m experiences\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# Get max predicted Q values (for next states) from target model\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m Q_target_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqnetwork_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Compute Q targets for current states \u001b[39;00m\n\u001b[0;32m    147\u001b[0m Q_target \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m (gamma \u001b[38;5;241m*\u001b[39m Q_target_next \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones))\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\general\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\Repositories\\deep-reinforcement-learning\\lunar_dqn\\Q_network.py:56\u001b[0m, in \u001b[0;36mQQNetwork.forward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a network that maps state -> action values.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m       Shape of input state should be: (batch_size, 2, state_size)\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     s0 \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     57\u001b[0m     s0 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate0(s0))\n\u001b[0;32m     58\u001b[0m     s1 \u001b[38;5;241m=\u001b[39m state[:,\u001b[38;5;241m1\u001b[39m,:]\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "n_episodes = 1000\n",
    "max_t = 500\n",
    "eps_start = 1.0\n",
    "eps_end = 0.001\n",
    "eps_decay = (eps_end/eps_start)**(1/n_episodes)\n",
    "\n",
    "scores = train_dqn(n_episodes, max_t, eps_start, eps_end, eps_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1331,  0.2205,  0.1934,  0.1535], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_state = np.asarray([scale_input(state), scale_input(state)])\n",
    "evaluate(two_state).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rH9Y0tg0Gx9a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores, 'bo--', linewidth=0.25, markersize=1.0,)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rouVehFdXq6X"
   },
   "outputs": [],
   "source": [
    "torch.save(agent.qnetwork_local.state_dict(), 'dqn_windypoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.randn(8, 2, 4)\n",
    "print(x.shape, x)\n",
    "#y = torch.randn(8, 2, 4)\n",
    "x0 = x[:,0,:].squeeze()\n",
    "x1 = x[:,1,:].squeeze()\n",
    "print(x1.shape, x1)\n",
    "#y = torch.reshape(x, (8,8))\n",
    "y = torch.cat((x0,x1),1)\n",
    "print(y.shape, y)\n",
    "\n",
    "#print(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Observe agent gameplay\n",
    "\n",
    "t = 300\n",
    "smax = nn.Softmax(dim=1)\n",
    "\n",
    "state, _ = env.reset(seed=SEED)\n",
    "frame = env.render()\n",
    "img = plt.imshow(pix_norm(frame))\n",
    "action =  np.random.choice(ACTIONS)   \n",
    "for _ in range(t):\n",
    "    state1, reward1, is_done, is_trunc, info = env.step(action)\n",
    "    frame1 =  pix_norm(env.render())\n",
    "    state2, reward2, is_done, is_trunc, info = env.step(OFF)\n",
    "    frame2 =  pix_norm(env.render())\n",
    "    states = np.asarray([scale_input(state1),\n",
    "                         scale_input(state2)])\n",
    "    probs = np.asarray([agent.act(s) for s in states])\n",
    "    #pols = smax(sigs).detach().cpu().numpy()\n",
    "    #probs = probs.detach().cpu().numpy()\n",
    "    action = np.argmax(probs[-1])\n",
    "    \n",
    "    reward = reward2 + reward1/2\n",
    "    \n",
    "    frame = scale(frame2-frame1/2)\n",
    "    img.set_data(frame)\n",
    "    plt.title(str(action)+\"    \"+str(np.round(reward, 3)))\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    if is_done or is_trunc:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, agent.act(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGxg3gjKsz5G",
    "tags": []
   },
   "source": [
    "### 5. Explore\n",
    "\n",
    "In this exercise, you have implemented a DQN agent and demonstrated how to use it to solve an OpenAI Gym environment.  To continue your learning, you are encouraged to complete any (or all!) of the following tasks:\n",
    "- Amend the various hyperparameters and network architecture to see if you can get your agent to solve the environment faster.  Once you build intuition for the hyperparameters that work well with this environment, try solving a different OpenAI Gym task with discrete actions!\n",
    "- You may like to implement some improvements such as prioritized experience replay, Double DQN, or Dueling DQN! \n",
    "- Write a blog post explaining the intuition behind the DQN algorithm and demonstrating how to use it to solve an RL environment of your choosing.  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
