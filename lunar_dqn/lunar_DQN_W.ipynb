{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArKFzS5ksz5C"
   },
   "source": [
    "# Deep Q-Network (DQN)\n",
    "---\n",
    "In this notebook, you will implement a DQN agent with OpenAI Gym's LunarLander-v2 environment.\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXsjjok7sz5D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# render ai gym environment\n",
    "#!pip install gymnasium[box2d]\n",
    "import gymnasium as gym\n",
    "\n",
    "# install package for displaying animation\n",
    "#!pip install JSAnimation\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "\n",
    "#!pip install progressbar\n",
    "#import progressbar as pb\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "%matplotlib inline\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    print(\"IPython\")\n",
    "    from IPython import display\n",
    "else:  \n",
    "    print(\"PVD\")\n",
    "    #!python -m pip install pyvirtualdisplay\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=True, size=(1400, 900))\n",
    "    display.start()\n",
    "    \n",
    "plt.ion()\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from DQN_utils import *\n",
    "from agent_W import *\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using device: \", device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Q_network import QNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4Sl_Wc_sz5E",
    "tags": []
   },
   "source": [
    "### 2. Instantiate the Environment and Agent\n",
    "\n",
    "Initialize the environment in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print([k for k in gym.envs.registry.keys() if \"Lunar\" in k])  #.all().keys()  #.make\n",
    "#gym.envs.registry['LunarLanderContinuous-v2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "OLD_GYM = False\n",
    "if OLD_GYM:\n",
    "    #import gym\n",
    "    env = gym.make(\"LunarLander-v2\", options={'continuous': False,\n",
    "                                              'gravity': -9.81,\n",
    "                                              'enable_wind': True,\n",
    "                                              'wind_power': 1.5,\n",
    "                                              'turbulence_power': 0.15}\n",
    "                  )\n",
    "    env.seed(123)\n",
    "    state, info = env.reset()\n",
    "    obs = env.render()\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "        state, reward, done, trun, info = env.step(action)\n",
    "        print(action, reward)\n",
    "        obs = env.render(mode=\"rgb_array\")\n",
    "        plt.imshow(obs)\n",
    "\n",
    "    obs_space = env.observation_space.shape\n",
    "    action_size = env.action_space.n\n",
    "    print('State shape: ', obs_space)\n",
    "    print('Number of actions: ', action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SY0GEqHHsz5E",
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### NEW GYM = GYMNASIUM\n",
    "#!pip install gymnasium[box2d]\n",
    "#import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\",    #\"human\",       #\n",
    "                                 continuous= False,\n",
    "                                 gravity= -9.81,\n",
    "                                 enable_wind= True,\n",
    "                                 wind_power= 0.01,\n",
    "                                 turbulence_power= 0.001)\n",
    "state, info = env.reset(seed = 1234)\n",
    "obs = env.render()\n",
    "    \n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    state, reward, done, trun, info = env.step(action)\n",
    "    done = done or trun\n",
    "    #print(action, reward)\n",
    "    obs = env.render()\n",
    "    plt.imshow(obs)\n",
    "\n",
    "state_shape = env.observation_space.shape\n",
    "state_size = state_shape[0]\n",
    "action_size = env.action_space.n\n",
    "print('State shape: ', state_size)\n",
    "print('Number of actions: ', action_size)\n",
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Observation Space\n",
    "\n",
    "The observation space is an 8-dimensional vector: \n",
    "* the coordinates of the lander (x & y), \n",
    "* its linear velocities (x & y), \n",
    "* its angle (radians), \n",
    "* its angular velocity, \n",
    "* and two booleans for whether/not each leg has ground contact.\n",
    "\n",
    "Observation Highs:\n",
    "* [1.5  1.5  5.0  5.0  3.14  5.0  True  True ]\n",
    "\n",
    "Observation Lows:\n",
    "* [-1.5  -1.5  -5.0  -5.0  -3.14  -5.0  False  False ]\n",
    "\n",
    "Wind function:\n",
    "\n",
    "`tanh`(sin(2 k (t+C)) + sin(pi k (t+C))). k is set to 0.01. C is sampled randomly between -9999 and 9999\n",
    "\n",
    "\n",
    "#### Discrete Action Space\n",
    "\n",
    "There are four discrete actions available:\n",
    "\n",
    "* 0: do nothing\n",
    "* 1: fire left orientation engine\n",
    "* 2: fire main engine\n",
    "* 3: fire right orientation engine\n",
    "\n",
    "\n",
    "#### Rewards\n",
    "\n",
    "After every step a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
    "\n",
    "For each step, the reward:\n",
    "\n",
    "* is increased/decreased the closer/further the lander is to the landing pad.\n",
    "* is increased/decreased the slower/faster the lander is moving.\n",
    "* is decreased the more the lander is tilted (angle not horizontal).\n",
    "* is increased by 10 points for each leg that is in contact with the ground.\n",
    "* is decreased by 0.03 points each frame a side engine is firing.\n",
    "* is decreased by 0.3 points each frame the main engine is firing.\n",
    "\n",
    "The episode receives an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
    "\n",
    "An episode is considered a solution if it scores at least 200 points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4ukSEf9sz5E"
   },
   "source": [
    "Before running the next code cell, familiarize yourself with the code in **Step 2** and **Step 3** of this notebook, along with the code in `dqn_agent.py` and `model.py`.  Once you have an understanding of how the different files work together, \n",
    "- Define a neural network architecture in `model.py` that maps states to action values.  This file is mostly empty - it's up to you to define your own deep Q-network!\n",
    "- Finish the `learn` method in the `Agent` class in `dqn_agent.py`.  The sampled batch of experience tuples is already provided for you; you need only use the local and target Q-networks to compute the loss, before taking a step towards minimizing the loss.\n",
    "\n",
    "Once you have completed the code in `dqn_agent.py` and `model.py`, run the code cell below.  (_If you end up needing to make multiple changes and get unexpected behavior, please restart the kernel and run the cells from the beginning of the notebook!_)\n",
    "\n",
    "You can find the solution files, along with saved model weights for a trained agent, in the `solution/` folder.  (_Note that there are many ways to solve this exercise, and the \"solution\" is just one way of approaching the problem, to yield a trained agent._)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYCfgBlUsz5F",
    "tags": []
   },
   "source": [
    "### 3. Train the Agent with DQN\n",
    "\n",
    "Run the code cell below to train the agent from scratch.  You are welcome to amend the supplied values of the parameters in the function, to try to see if you can get better performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from dqn_agent import Agent\n",
    "#torch.save(agent.qnetwork_local.state_dict(), 'dqn_windypoint.pth')\n",
    "\n",
    "seed = 1234\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=seed, \n",
    "              fc1_units=64, fc2_units=64, learn_every=4)\n",
    "\n",
    "try: agent.qnetwork_local.load_state_dict(torch.load('data/slvdpnt.pth'))\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXYQj_P5sz5F"
   },
   "outputs": [],
   "source": [
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    FIRST = True\n",
    "    episode_lengths = []\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    window_size = 100                  # scores to rolling-remember\n",
    "    scores_window = deque(maxlen=window_size)\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state, _ = env.reset(seed=SEED)\n",
    "        score = 0\n",
    "        episteps = 0\n",
    "        for t in range(max_t):  #episteps):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, trun, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done or trun)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            episteps += 1\n",
    "            if done or trun:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        episode_lengths.append(episteps)\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        \n",
    "        cycle_steps = agent.steps%BUFFER_SIZE\n",
    "        buffer_cycle = agent.steps//BUFFER_SIZE\n",
    "\n",
    "        print(\"\\rEpisode {:4d} | Episode Score: {:7.2f} | Eps Steps: {:4d} | Epsilon: {:1.3f} | Buffer cycle:{:7d} +{:3d}\".format(i_episode, \n",
    "                                                                                                                            scores_window[-1], \n",
    "                                                                                                                            episode_lengths[-1],\n",
    "                                                                                                                            eps,\n",
    "                                                                                                                            cycle_steps,\n",
    "                                                                                                                            buffer_cycle), end=\"\")\n",
    "                                                                                                                                        \n",
    "        if i_episode % 100 == 0:\n",
    "            chkpntname = \"data/chkpnt{}.pth\".format(i_episode)\n",
    "            torch.save(agent.qnetwork_local.state_dict(), chkpntname)   \n",
    "            print(\"\\rEpisode {:4d} | Average Score: {:7.2f} | Avg Steps: {:4d} | Epsilon: {:1.3f} | Buffer cycle:{:7d} +{:3d}\".format(i_episode, \n",
    "                                                                                                                            np.mean(scores_window), \n",
    "                                                                                                                            round(np.mean(episode_lengths)),\n",
    "                                                                                                                            eps,\n",
    "                                                                                                                            cycle_steps,\n",
    "                                                                                                                            buffer_cycle))\n",
    "        #episteps = (episteps - 1) if episteps>=100 else max_t\n",
    "        \n",
    "        if np.mean(scores_window)>=200. and FIRST:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:5.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'data/slvdpnt.pth')\n",
    "            FIRST = False\n",
    "        elif np.mean(scores_window)>=250. :\n",
    "            print(\"\\nHigh Score!\")\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'data/hipnt.pth')\n",
    "            break\n",
    "             \n",
    "    return scores, episode_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "veUfdWa1PaXE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores, steps  = dqn(n_episodes=1200, max_t=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rH9Y0tg0Gx9a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores, 'b,--', linewidth=0.25, markersize=1.0,)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the normed scores\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), norm(np.asarray(scores)), 'r,--', linewidth=0.25, markersize=1.0,)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.memory.rewards[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the normed rewards\n",
    "Mrewards = list(agent.memory.rewards)\n",
    "Mreward_mean = np.mean(Mrewards)\n",
    "Mreward_std = np.std(Mrewards)\n",
    "NMrewards = (Mrewards - Mreward_mean)/Mreward_std if Mreward_std!=0. else Mrewards\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(NMrewards)), NMrewards, 'g,', linewidth=0.1, markersize=1.0,)\n",
    "#plt.ylim(0, 0.25)\n",
    "plt.ylabel('Normed Rewards')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nrewards: \n",
    "* 103542\n",
    "\n",
    "Mreward_mean: \n",
    "* -3.834\n",
    "\n",
    "Mreward_std: \n",
    "* 11.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(agent.memory.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "states, actions, rewards, next_states, dones = disentangle(agent.memory.memory)\n",
    "#sum(dones), \n",
    "#np.asarray(rewards).where(dones==1)\n",
    "#finrews = [r for r,d in zip(rewards,dones) if d!=0]\n",
    "[(list(s[-2:]), num2act[a], r) for s, a, r in zip(states, actions, rewards) if r>-100. and r<10]#[-300:-200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#[(list(s[-2:]), num2act[a], r) for s, a, r in zip(states, actions, rewards)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "#if False:\n",
    "    # Expects data to be pre-torched, normed or scaled, and well-shaped\n",
    "    states, actions, rewards, next_states, dones = disentangle(agent.memory.memory)\n",
    "    print(\"Initial states:\", np.asarray(states).shape, np.asarray(states)[0])\n",
    "\n",
    "        \n",
    "    agent.qnetwork_local.eval()\n",
    "    agent.qnetwork_target.eval()\n",
    "    with torch.no_grad():    \n",
    "   \n",
    "        # To tensors\n",
    "        Tstates = torch.from_numpy(np.vstack(states)).float().to(device)\n",
    "        Tactions = torch.from_numpy(np.vstack(actions)).long().to(device)\n",
    "        Trewards = torch.from_numpy(np.vstack(rewards)).float().to(device)\n",
    "        Tnext_states = torch.from_numpy(np.vstack(next_states)).float().to(device)\n",
    "        Tdones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_target_next = agent.qnetwork_target(Tnext_states)\n",
    "        print(\"Q_target(Tnext_states) = Q_target_next:\", Q_target_next.shape, Q_target_next[0])\n",
    "        Q_target_next = Q_target_next.detach().max(1)[0]\n",
    "        print(\"Q_target_next MAX:\", Q_target_next.shape, Q_target_next[0])\n",
    "        Q_target_next = Q_target_next.unsqueeze(1)\n",
    "        print(\"Q_target_next UNSQUEEZED:\", Q_target_next.shape, Q_target_next[0])\n",
    "        \n",
    "        # Q_target_next.shape, Q_target_next[0]\n",
    "\n",
    "        # Compute Q targets for current states \n",
    "        Q_target = Trewards + (GAMMA * Q_target_next * (1 - Tdones))\n",
    "        print(\"Q_target = Trewards + (GAMMA * Q_target_next * (1 - Tdones)):\\n\", Q_target.shape, Q_target[0])\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = agent.qnetwork_local(Tstates)\n",
    "        print(\"Q_local(Tstates) = Q_expected:\", Q_expected.shape, Q_expected[0])\n",
    "        Q_expected = Q_expected.gather(1, Tactions) ## Local Q-value of the action taken\n",
    "        print(\"Q_expected Gathered:\", Q_expected.shape, Q_expected[0])\n",
    "        print(\"Tactions:\", Tactions.shape, Tactions[0])\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_target)\n",
    "        print(\"loss:\", loss)  \n",
    "        \n",
    "    agent.qnetwork_target.train()\n",
    "    agent.qnetwork_local.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the normed rewards\n",
    "Mrewards = list(agent.memory.rewards)\n",
    "Mreward_mean = np.mean(Mrewards)\n",
    "Mreward_std = np.std(Mrewards)\n",
    "NMrewards = (Mrewards - Mreward_mean)/Mreward_std if Mreward_std!=0. else Mrewards\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(NMrewards)), NMrewards, 'g,-', linewidth=0.1, markersize=1.0,)\n",
    "plt.ylabel('Normed Rewards')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Watch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPD8Pq39sz5E",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# watch a pre-trained agent\n",
    "\n",
    "for chkpnt in range(200,1400,200):\n",
    "    chkpntname = \"data/chkpnt{}.pth\".format(chkpnt)\n",
    "    agent.qnetwork_local.load_state_dict(torch.load(chkpntname))\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    img = plt.imshow(env.render())\n",
    "    for j in range(200):\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, trun, _ = env.step(action)\n",
    "        img.set_data(env.render()) \n",
    "        plt.axis('off')\n",
    "        title = \"{:4d} | {:-4.2f} | {:s}\".format(chkpnt, reward, num2act[action])\n",
    "        plt.title(title)\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        if done or trun:\n",
    "            break \n",
    "      \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGxg3gjKsz5G",
    "tags": []
   },
   "source": [
    "### 5. Explore\n",
    "\n",
    "In this exercise, you have implemented a DQN agent and demonstrated how to use it to solve an OpenAI Gym environment.  To continue your learning, you are encouraged to complete any (or all!) of the following tasks:\n",
    "- Amend the various hyperparameters and network architecture to see if you can get your agent to solve the environment faster.  Once you build intuition for the hyperparameters that work well with this environment, try solving a different OpenAI Gym task with discrete actions!\n",
    "- You may like to implement some improvements such as prioritized experience replay, Double DQN, or Dueling DQN! \n",
    "- Write a blog post explaining the intuition behind the DQN algorithm and demonstrating how to use it to solve an RL environment of your choosing.  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
