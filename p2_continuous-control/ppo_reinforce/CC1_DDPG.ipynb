{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Agent Solution\n",
    "\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifications of Deep Deterministic Policy Gradients (DDPG) notebook\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#from ddpg_agent_single import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 17\n",
    "max_steps=1000\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.36 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mjupyter-console 6.4.3 has requirement jupyter-client>=7.0.0, but you'll have jupyter-client 5.2.4 which is incompatible.\u001b[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like:\n",
      " [  0.     -4.      0.      1.     -0.     -0.     -0.      0.      0.      0.\n",
      "   0.      0.      0.      0.    -10.      0.      1.     -0.     -0.     -0.\n",
      "   0.      0.      0.      0.      0.      0.      5.755  -1.      5.557\n",
      "   0.      1.      0.     -0.168]\n",
      "State shape:  (33,)\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "#env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "    \n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations.squeeze()\n",
    "state_size = state.shape[-1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(num_agents, state_size))\n",
    "print('The state for the first agent looks like:\\n', np.round(state, 3))\n",
    "print('State shape: ', state.shape)\n",
    "\n",
    "actions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "### Network hyperparameters\n",
    "BUFFER_SIZE = int(6.4e6)  # replay buffer size = int(1e6)\n",
    "BATCH_SIZE = 128           # minibatch size = 128\n",
    "GAMMA = 0.995             # discount factor = 0.99\n",
    "TAU = 1e-3                # for soft update of target parameters = 1e-3\n",
    "LR_ACTOR = 1e-4           # learning rate of the actor  = 1e-4\n",
    "LR_CRITIC = 1e-3          # learning rate of the critic = 3e-4\n",
    "WEIGHT_DECAY = 1e-5       # L2 weight decay = 0.00001 to 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Actor and Critic models\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=400, fc2_units=300):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=400, fc2_units=300):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorX(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=400, fc2_units=300):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(ActorX, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(fc1_units)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(fc2_units)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.batch_norm2(x)\n",
    "        return torch.tanh(self.fc3(x))\n",
    "    \n",
    "    \n",
    "class CriticX(nn.Module):\n",
    "    \"\"\"Critic aka Value Model. \n",
    "       (state, action) pair -> estimated reward aka Q-value\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, \n",
    "                 state_units=192, action_units=64, hidden_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            state_units (int): Number of nodes in the input layer\n",
    "            action_units (int): Number of nodes in the input layer\n",
    "            hidden_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(CriticX, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc_state = nn.Linear(state_size, state_units)\n",
    "        self.fc_action = nn.Linear(action_size, action_units)\n",
    "        self.fc_hidden = nn.Linear(state_units+action_units, hidden_units)\n",
    "        self.fc_output = nn.Linear(hidden_units, 1)\n",
    "        self.reset_parameters()\n",
    "        self.batch_norm_state = nn.BatchNorm1d(state_units)\n",
    "        self.batch_norm_hidden = nn.BatchNorm1d(hidden_units)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc_state.weight.data.uniform_(*hidden_init(self.fc_state))\n",
    "        self.fc_action.weight.data.uniform_(*hidden_init(self.fc_action))\n",
    "        self.fc_hidden.weight.data.uniform_(*hidden_init(self.fc_hidden))\n",
    "        self.fc_output.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Critic network to map (state, action) pairs -> estimated rewards.\"\"\"\n",
    "        xs = F.relu(self.fc_state(state))\n",
    "        xs = self.batch_norm_state(xs)\n",
    "        xa = F.relu(self.fc_action(action))\n",
    "        x = torch.cat((xs, xa), dim=1)\n",
    "        x = F.relu(self.fc_hidden(x))\n",
    "        x = self.batch_norm_hidden(x)\n",
    "        return self.fc_output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for _ in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "class PriorityReplay:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples and batch them by priority.\n",
    "       Aka 'Collection of Trajectories'? \n",
    "       TO DO: priority determined by...?\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a PriorityReplay object.\n",
    "            Params\n",
    "            ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.seed = random.seed(seed)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"step\", \"state\", \"action\", \"reward\", \"next_state\", \"done\",\n",
    "                                                                \"priority\"])\n",
    "    \n",
    "    def add(self, step, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        ### Priority calculated here:\n",
    "        #priority = ((1000 - step)/1000 + int(done)) * abs(reward)\n",
    "        #######  TO DO just use rewards for priorities ??  Critic MSE Loss?\n",
    "        priority = (1000 - step) * abs(reward)\n",
    "####### print(\"To buffer: \", step, \"\\t\", np.round(reward, 3), \"\\t\", np.round(priority, 3), \"\\t\", len(self.buffer))\n",
    "        e = self.experience(step, state, action, reward, next_state, done, priority)\n",
    "        self.buffer.append(e)\n",
    "                    \n",
    "    def norm(self, rewards):\n",
    "        #rewards = [e.reward for e in experiences]\n",
    "        ravg = np.mean(rewards)\n",
    "        rstd = np.std(rewards)\n",
    "        rsum = np.sum(rewards)\n",
    "        if rstd != 0:\n",
    "            normed_rewards = [(r-ravg)/rstd for r in rewards]\n",
    "        else:\n",
    "            normed_rewards = [r/rsum for r in rewards] \n",
    "        return normed_rewards\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory according to their relative priority.\"\"\"\n",
    "####### TO DO: fix priorities\n",
    "        probs = [e.reward for e in self.buffer]\n",
    "        experiences = random.choices(population=self.buffer, k=self.batch_size, weights=probs)\n",
    "          \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences]).astype(np.uint8)).float().to(device)\n",
    "                    \n",
    "        return (states, actions, normed_rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.buffer)\n",
    "                    \n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples. (ie. \"trajectories\"?)\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "            Params\n",
    "            ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.buffer = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"step\", \"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, step, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(step, state, action, reward, next_state, done)\n",
    "        self.buffer.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.buffer, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.buffer)\n",
    "    \n",
    "class PriorityBufferX:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples. (ie. \"trajectories\"?)\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "            Params\n",
    "            ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.buffer = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"step\", \"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, step, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(step, state, action, reward, next_state, done)\n",
    "        self.buffer.append(e)\n",
    "       \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        probs = [e.reward for e in self.buffer]\n",
    "        experiences = random.choices(population=self.buffer, k=self.batch_size, weights=probs)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def reprioritize(self, priorities, experiences):\n",
    "        #for p, e in zip(priorities, experiences):\n",
    "        #    e.priority = p\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2129371387881267"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Agent made of four networks: Remote and Local Critics (values), Remote and local Actors (actions)\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed, \n",
    "                 add_noise=True, priority_replay=False, learn_every=7):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "            add_noise (bool): if True, add noise to action vectors\n",
    "            priority_replay (bool): if working, else vanilla replay\n",
    "            learn_every (int) : learning rate (between 13 and 23)\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.add_noise = add_noise  \n",
    "        self.priority_replay = priority_replay\n",
    "        self.learn_every = learn_every ## \n",
    "        \n",
    "        # Memory: a replay buffer, or a priority replay buffer\n",
    "        # Trajectory-like \"experiences\"\n",
    "        if priority_replay:\n",
    "            self.replay =  PriorityReplay(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "        else: \n",
    "            self.replay = PriorityBufferX(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "        \n",
    "        ## Four networks per Agent:\n",
    "        # Actor Networks: state --> action\n",
    "        self.actor_local = ActorX(state_size, action_size, random_seed).to(device)     \n",
    "        self.actor_target = ActorX(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Networks: state, action --> reward\n",
    "        self.critic_local = CriticX(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = CriticX(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, \n",
    "                                                               weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise to add to actions\n",
    "        if self.add_noise: self.noise = OUNoise(action_size, random_seed)\n",
    "        else: self.noise = None\n",
    "\n",
    "    def reset(self):\n",
    "        if self.add_noise: self.noise.reset()\n",
    "\n",
    "    def step(self, step, state, action, reward, next_state, done):\n",
    "        \"\"\" Save experience in replay memory; then\n",
    "            Use a random sample from the buffer to train on.\n",
    "        \"\"\"        \n",
    "        if len(self.replay) <= BATCH_SIZE:\n",
    "            self.replay.add(step, state, action, reward, next_state, done)\n",
    "#### TO DO fix repeat   add statement   \n",
    "        elif reward > 0.:\n",
    "            self.replay.add(step, state, action, reward, next_state, done)\n",
    "            \n",
    "            # Start learning from memory batches once n_experiences > batch_size\n",
    "            if step % self.learn_every == 0: \n",
    "                experiences = self.replay.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "                \n",
    "    def act(self, state):\n",
    "        \"\"\"Returns actions for given state per current policy.\n",
    "           \"Current policy\" is set by current local actor weights + some noise.\n",
    "           Action values are clipped to (-1, 1) per Reacher env.\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if self.add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1., 1.)\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (step, s, a, r, s', done) tuples\n",
    "            experiences are one batch_size in size\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        ### TO DO pre-normalize these parameters:\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # --------------------------  from target models ------------------------------ #\n",
    "        # Target actor predicts actions for next-state\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        # Target critic predicts next-reward Q-values:\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Estimate expected Q-values from recorded states, actions, and rewards\n",
    "        #### TO DO: Use Log() of rewards? Pre-scale to [0,1]? Pre-norm by [mean,std]?\n",
    "        Q_targets = rewards + gamma * Q_targets_next * (1 - dones)\n",
    "        \n",
    "        # ---------------------------- update local critic ---------------------------- #\n",
    "        # Compute local critic loss compared to target estimate of rewards from\n",
    "        # the recorded (state, action) pairs\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_mse_loss = F.mse_loss(Q_expected, Q_targets)      \n",
    "        # Minimize critic loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_mse_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update local actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize actor loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # --------------=====----- soft update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "        #### TO DO: return critic_mse_loss.tolist() for priority adjustment?\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        (tau << 0 so θ_target remains mostly θ_target)\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter << 0\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), \n",
    "                                             local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "agent = Agent(state_size, action_size, seed,\n",
    "              add_noise=True, priority_replay=False, learn_every=3)\n",
    "#agent.actor_local.load_state_dict(torch.load('data/checkpoint_actor.pth'))\n",
    "#agent.critic_local.load_state_dict(torch.load('data/checkpoint_critic.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agents with DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 33)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info.vector_observations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01570548 -0.01936131  0.06098157  0.07291143]]\n",
      "[0.0]\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "state = env_info.vector_observations#.squeeze()\n",
    "actions = agent.act(state)\n",
    "scores = env_info.rewards\n",
    "#print([a for a in actions])\n",
    "print(actions)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(n_episodes=100, print_every=100, max_score=-np.inf):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    epi_scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "        state = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        score = 0.                                  # initialize the score (for each agent)\n",
    "        step = 0.\n",
    "        while True:\n",
    "            step += 1\n",
    "            actions = agent.act(state)                       # list-like of actions (onex4 per agent)\n",
    "            env_info = env.step(actions)[brain_name]           # send all 20 actions to tne environment\n",
    "            next_state = env_info.vector_observations#.squeeze()         # get next states (one per agent)\n",
    "            reward = env_info.rewards[0]                         # rewards returned (for each agent)\n",
    "            done = env_info.local_done[0]                        # see if any episodes are finished\n",
    "            agent.step(step, state, actions, reward, next_state, done)\n",
    "            state = next_state                               # roll over states to next time step\n",
    "            score += reward                                  # update the score (for each agent)\n",
    "            if done:                                  # exit loop if any episode finished\n",
    "                break    \n",
    "        scores_deque.append(score)\n",
    "        epi_scores.append(score) \n",
    "        #if max(all_scores).any() > max_score: max_score = max(all_scores).any()\n",
    "        epi_max = np.max(epi_scores)\n",
    "        if max_score < epi_max: max_score = epi_max\n",
    "        avg_score = np.mean(scores_deque)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tHigh Score: {:.2f}'.format(i_episode, avg_score, max_score), end=\"\")\n",
    "        if i_episode % print_every == 0:\n",
    "            torch.save(agent.actor_local.state_dict(), './data/checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), './data/checkpoint_critic.pth')\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tHigh Score: {:.2f}\\n'.format(i_episode, avg_score, max_score))   \n",
    "\n",
    "    return epi_scores, max_score     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.02\tHigh Score: 0.19\n",
      "\n",
      "Episode 200\tAverage Score: 0.02\tHigh Score: 0.58\n",
      "\n",
      "Episode 300\tAverage Score: 0.01\tHigh Score: 0.58\n",
      "\n",
      "Episode 400\tAverage Score: 0.01\tHigh Score: 0.58\n",
      "\n",
      "Episode 500\tAverage Score: 0.01\tHigh Score: 0.58\n",
      "\n",
      "Episode 600\tAverage Score: 0.01\tHigh Score: 0.58\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.0, 0.0, 0.0, 0.0, 0.1099999975413084, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 0.57999998703598976)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores, mx = train_agent(n_episodes=600, print_every=100)\n",
    "#scores += ores\n",
    "scores[-10:], mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(326, (1, 33), (1, 33), False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem = agent.replay.buffer\n",
    "len(mem), mem[-1][1].shape, mem[-1][4].shape, mem[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [m.step for m in mem]\n",
    "states = [m.state for m in mem]\n",
    "actions = [m.action for m in mem]\n",
    "rewards = [m.reward for m in mem]\n",
    "next_st = [m.next_state for m in mem]\n",
    "dones = [m.done for m in mem]\n",
    "\n",
    "steps_df = pd.DataFrame(data=steps)\n",
    "states_df = pd.DataFrame(data=states)\n",
    "actions_df = pd.DataFrame(data=actions)\n",
    "rewards_df = pd.DataFrame(data=rewards)\n",
    "next_st_df = pd.DataFrame(data=next_st)\n",
    "dones_df = pd.DataFrame(data=dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a911cb17ac37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstates_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msteps_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mactions_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrewards_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnext_st_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext_st\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    401\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                     mgr = self._init_ndarray(data, index, columns, dtype=dtype,\n\u001b[0;32m--> 403\u001b[0;31m                                              copy=copy)\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_ndarray\u001b[0;34m(self, values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;31m# by definition an array here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prep_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_prep_ndarray\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m   7446\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7447\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7448\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Must pass 2-d input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7450\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Must pass 2-d input"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 33)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_st[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_df.to_csv('data/rewards.csv')\n",
    "steps_df.to_csv('data/steps2.csv')\n",
    "actions_df.to_csv('data/actions2.csv')\n",
    "states_df.to_csv('data/states2.csv')\n",
    "next_st_df.to_csv('data/next_st2.csv')\n",
    "dones_df.to_csv('data/dones2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9830241080>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Watch a Smart Agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'UnityEnvironment' object has no attribute 'render'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-32a0a313a81d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, add_noise=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'UnityEnvironment' object has no attribute 'render'"
     ]
    }
   ],
   "source": [
    "agent.actor_local.load_state_dict(torch.load('./data/checkpoint_actor.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('./data/checkpoint_critic.pth'))\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "#state = env.reset()\n",
    "for t in range(200):\n",
    "    action = agent.act(state)#, add_noise=False)\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Display from DQN notebook\n",
    "rewards = []\n",
    "steps = []\n",
    "tries = 100\n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "img = plt.imshow(env.render())\n",
    "for j in range(1000):\n",
    "    action = agent.act(state)\n",
    "    state, reward, done, trun, info = env.step(action)\n",
    "    rewards += [reward]\n",
    "    img.set_data(env.render()) \n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    if done or trun:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
