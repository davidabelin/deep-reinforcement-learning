{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#env.close()\n",
    "#one_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import ddpg_agent\n",
    "from ddpg_agent import Agent as Agency\n",
    "from ddpg_agent import SEED, BUFFER_SIZE, BATCH_SIZE, device\n",
    "#from ddpg_train import train\n",
    "print(\"Using device:\", device)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Unity Reacher Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Examine the State and Action Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n"
     ]
    }
   ],
   "source": [
    "# For Environment with 20 Agents:\n",
    "env = UnityEnvironment(file_name='UnityReacher20\\Reacher.exe',  #'../data/Reacher1_Win/Reacher.exe',   # \n",
    "                       worker_id=200, \n",
    "                       seed=SEED, \n",
    "                       no_graphics=False)\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Intialize the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agency = Agency(state_size=state_size, \n",
    "                action_size=action_size, \n",
    "                num_agents=num_agents, \n",
    "                random_seed=SEED,\n",
    "                actor_units_in=256,\n",
    "                actor_units_hid=128,\n",
    "                critic_units_in=512,\n",
    "                critic_units_hid=128,\n",
    "                actor_filename='checkpoints/actor_training_chkpnt60.pth',\n",
    "                critic_filename='checkpoints/critic_training_chkpnt60.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "agemembuff = agency.memory\n",
    "new_agency = Agency(\n",
    "                state_size=state_size, \n",
    "                action_size=action_size, \n",
    "                num_agents=num_agents, \n",
    "                random_seed=SEED,\n",
    "                actor_units_in=256,\n",
    "                actor_units_hid=128,\n",
    "                critic_units_in=512,\n",
    "                critic_units_hid=128,\n",
    "                actor_filename='checkpoints/actor_training_chkpnt20.pth', \n",
    "                critic_filename='checkpoints/critic_training_chkpnt40.pth')\n",
    "new_agency.memory = agemembuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.3189, -3.2616, -0.1260,  ...,  1.0000,  0.0000,  0.8967],\n",
       "         [-2.8133, -2.2166,  1.8010,  ...,  1.0000,  0.0000, -0.2315],\n",
       "         [-1.6966, -3.4761,  1.0319,  ...,  1.0000,  0.0000, -0.9994],\n",
       "         ...,\n",
       "         [ 1.7679, -3.0136,  1.9576,  ...,  1.0000,  0.0000,  0.1579],\n",
       "         [-3.1249, -2.4712, -0.4365,  ...,  1.0000,  0.0000, -0.8832],\n",
       "         [ 2.2964, -2.8342, -1.6574,  ...,  1.0000,  0.0000,  0.9642]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 0.4851, -0.6685, -0.0778,  0.3775],\n",
       "         [-1.0000,  0.1614,  0.1993, -0.2576],\n",
       "         [-0.4477, -0.9367, -0.8535, -0.9253],\n",
       "         [ 0.2288, -0.0385,  0.2306, -0.7333],\n",
       "         [ 0.4650, -0.0714,  0.4095,  0.2005],\n",
       "         [ 1.0000, -1.0000, -0.7354,  0.6631],\n",
       "         [ 1.0000, -0.9030, -0.4023,  0.3287],\n",
       "         [-0.2562, -0.4932,  0.0310, -0.2300],\n",
       "         [-0.5675, -0.8898,  1.0000,  1.0000],\n",
       "         [ 0.0441,  0.6256, -0.1842,  0.0894],\n",
       "         [-0.2652,  1.0000, -0.1489, -0.7557],\n",
       "         [ 1.0000, -0.0345, -1.0000, -0.6705],\n",
       "         [-1.0000, -1.0000,  0.1682,  0.7682],\n",
       "         [ 1.0000,  0.7055,  0.0691,  0.1331],\n",
       "         [-0.6910,  0.3365,  0.2219, -0.5124],\n",
       "         [-0.7285,  1.0000, -0.5465,  0.2929],\n",
       "         [ 0.4236,  0.1249, -0.2023,  0.3878],\n",
       "         [-0.4854,  0.7254, -0.1206, -0.7377],\n",
       "         [-1.0000, -0.7625, -0.6128, -0.9475],\n",
       "         [ 1.0000, -1.0000, -0.0620, -0.3055],\n",
       "         [ 0.8419, -0.3948, -0.0947, -0.0529],\n",
       "         [ 0.6692, -0.7377,  1.0000, -0.4029],\n",
       "         [ 0.1866, -0.7317, -0.0587, -0.1572],\n",
       "         [-0.9217, -0.6122, -1.0000, -1.0000],\n",
       "         [-1.0000, -0.2424,  0.0652,  0.9080],\n",
       "         [ 0.9959,  0.4785,  0.4148, -0.0856],\n",
       "         [ 0.7501, -1.0000,  0.5354,  0.6329],\n",
       "         [-1.0000, -0.2761, -0.2650,  0.4696],\n",
       "         [ 0.5647, -0.1872,  0.4714,  0.5789],\n",
       "         [ 0.9314, -0.6521, -0.1599,  0.0124],\n",
       "         [ 0.5534,  0.9055, -0.8861,  0.6354],\n",
       "         [ 0.5964, -1.0000,  1.0000,  1.0000],\n",
       "         [ 0.7744, -0.0305,  0.6920, -0.2802],\n",
       "         [-1.0000, -0.1582, -0.4585, -1.0000],\n",
       "         [-0.0132, -0.4508,  0.6811,  0.0549],\n",
       "         [ 0.0025, -0.5623,  0.0183,  0.6166],\n",
       "         [-0.5559, -0.9422, -0.6748,  0.5184],\n",
       "         [ 0.5199, -0.5943, -0.4904,  0.5511],\n",
       "         [-0.6414, -0.7307,  0.3010,  1.0000],\n",
       "         [-0.1767,  1.0000,  1.0000, -1.0000],\n",
       "         [-0.0568, -0.7880,  0.2877,  0.1403],\n",
       "         [ 0.8503, -0.3161, -0.4268,  0.0890],\n",
       "         [-0.4762,  1.0000, -0.2051,  0.4938],\n",
       "         [ 1.0000,  1.0000, -0.0612, -0.2608],\n",
       "         [ 0.0218,  0.8177,  0.3931, -0.9167],\n",
       "         [-0.3910, -0.2050,  0.5707, -0.6651],\n",
       "         [-0.7269,  0.0691, -0.2333,  0.8169],\n",
       "         [ 0.7315, -1.0000,  0.3171,  0.6056],\n",
       "         [ 0.8194, -0.4931, -0.2937,  0.3075],\n",
       "         [ 0.7163, -0.9403,  0.1987,  0.3557],\n",
       "         [ 0.4513,  1.0000, -0.7710,  0.0203],\n",
       "         [ 0.4561,  0.4215,  0.4370, -0.1895],\n",
       "         [-0.9938, -1.0000,  0.3295, -0.1839],\n",
       "         [ 0.3813, -0.8825,  0.6299,  0.8900],\n",
       "         [-0.7122,  0.2805, -0.4120, -0.2687],\n",
       "         [-0.0995, -0.0128,  0.0857, -0.3725],\n",
       "         [ 0.6666,  0.7798, -0.5063,  0.2917],\n",
       "         [-1.0000,  0.6020, -0.3287,  0.2642],\n",
       "         [ 0.6806, -0.0539, -0.4043,  0.7199],\n",
       "         [ 1.0000, -0.1232,  0.5288, -0.4779],\n",
       "         [-0.9203,  0.3138,  0.2467,  0.0693],\n",
       "         [-0.2007, -0.2818,  0.7024, -0.2192],\n",
       "         [-0.4175, -0.2193,  0.1725, -0.7273],\n",
       "         [ 0.4322, -0.8090,  0.3735,  0.2523],\n",
       "         [-0.0550, -0.7392, -0.5635, -0.2446],\n",
       "         [-1.0000, -1.0000, -1.0000,  0.8764],\n",
       "         [-0.5238, -0.1453,  0.6562,  0.7689],\n",
       "         [ 0.2288,  0.4339,  0.2377, -0.1147],\n",
       "         [ 0.4505, -0.0430,  0.2694, -0.1730],\n",
       "         [-1.0000, -0.8184, -0.1063, -1.0000],\n",
       "         [-0.2343,  1.0000,  0.7722,  1.0000],\n",
       "         [ 0.5264,  0.8024, -0.2769,  0.0412],\n",
       "         [-1.0000,  0.5854,  0.9304,  0.3872],\n",
       "         [-0.5097, -0.3623, -0.0451,  0.0818],\n",
       "         [ 0.0554, -0.3033,  0.2945, -0.6230],\n",
       "         [ 0.2240,  0.3506, -0.1388, -0.1096],\n",
       "         [ 1.0000,  0.3534, -0.6898,  0.7000],\n",
       "         [-0.6398, -0.8321,  0.8903, -0.8402],\n",
       "         [ 0.6591,  0.9616,  0.4118,  0.0066],\n",
       "         [ 0.8706, -0.1698,  0.6284,  0.3488],\n",
       "         [-0.0456, -1.0000,  0.3185, -0.4670],\n",
       "         [-1.0000,  0.0353, -0.3707, -0.6031],\n",
       "         [ 0.3323, -1.0000, -0.0161, -0.3126],\n",
       "         [ 1.0000,  1.0000, -1.0000, -0.0538],\n",
       "         [ 0.5276,  0.5066, -0.3243, -0.2828],\n",
       "         [ 1.0000, -0.5822,  0.6695, -0.0827],\n",
       "         [-0.2735, -0.5950, -0.4361, -0.5665],\n",
       "         [ 0.0135, -1.0000,  0.1100,  0.3145],\n",
       "         [ 0.1610,  0.6327, -0.6226, -0.1193],\n",
       "         [-0.3815, -0.6428,  0.5234,  0.0300],\n",
       "         [ 0.8463,  0.0821,  0.1409, -0.1018],\n",
       "         [-0.3158,  0.5693, -0.2663, -0.6657],\n",
       "         [-0.9223,  0.3729, -0.9473, -0.9739],\n",
       "         [-0.0863,  0.7621, -0.0128, -0.5219],\n",
       "         [ 0.4258, -0.9500,  0.5781,  0.2851],\n",
       "         [-0.1895, -0.8360, -0.4616, -1.0000],\n",
       "         [ 0.3808, -0.9304, -0.1504,  0.7610],\n",
       "         [ 0.3302,  1.0000,  0.9351,  0.8290],\n",
       "         [ 0.7002,  0.8881, -0.4321,  1.0000],\n",
       "         [-1.0000,  0.9107, -0.1696,  0.2057],\n",
       "         [ 1.0000, -0.4304,  0.6132,  0.2650],\n",
       "         [ 0.3802, -0.1721, -0.9037,  1.0000],\n",
       "         [-0.0678, -0.0632, -0.1628, -0.6502],\n",
       "         [ 0.2869, -0.7506,  1.0000,  0.1431],\n",
       "         [ 1.0000, -1.0000,  0.3099,  0.6514],\n",
       "         [ 0.9725,  0.0246,  0.3492, -0.3813],\n",
       "         [-0.3017,  0.0338, -0.3496,  0.1154],\n",
       "         [-0.5062, -0.0783, -0.0260, -0.2735],\n",
       "         [ 0.2240, -1.0000,  0.3382, -0.6195],\n",
       "         [ 0.8541,  0.1375,  0.0138, -0.2327],\n",
       "         [ 0.8051,  1.0000, -0.0623,  0.2321],\n",
       "         [-0.9717,  0.1687, -0.5812,  0.2197],\n",
       "         [ 1.0000,  0.2038, -0.3913, -0.0194],\n",
       "         [ 0.7758,  0.9130,  0.6393, -0.5407],\n",
       "         [ 0.8582,  1.0000,  0.5535,  0.2916],\n",
       "         [-1.0000, -0.3118, -0.8986,  0.7929],\n",
       "         [-1.0000, -1.0000,  0.0670, -0.4632],\n",
       "         [ 1.0000,  0.3320,  0.2380,  0.3271],\n",
       "         [-0.1081, -0.0402,  0.0733,  1.0000],\n",
       "         [-1.0000, -0.6410,  0.4117, -0.6888],\n",
       "         [ 0.6683, -0.9253, -1.0000,  0.4931],\n",
       "         [-0.6088, -0.8938,  0.6245,  0.5564],\n",
       "         [ 0.1376, -0.6769,  0.4968,  0.6055],\n",
       "         [ 0.4766, -0.1941,  0.1865,  0.1635],\n",
       "         [-0.5462,  0.3835, -0.4525, -0.2315],\n",
       "         [-0.3118,  0.6705,  0.0018, -0.0225],\n",
       "         [-1.0000, -0.7749,  0.0018,  0.2792],\n",
       "         [ 1.0000,  0.4014,  0.4198,  1.0000]], device='cuda:0'),\n",
       " tensor([[0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0000],\n",
       "         [0.0100],\n",
       "         [0.0000],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0300],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0000],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0000],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0000],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0000],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0300],\n",
       "         [0.0400],\n",
       "         [0.0000],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400],\n",
       "         [0.0400]], device='cuda:0'),\n",
       " tensor([[-2.3771, -3.2220, -0.0040,  ...,  1.0000,  0.0000,  0.8967],\n",
       "         [-2.8526, -2.1794,  1.7849,  ...,  1.0000,  0.0000, -0.2315],\n",
       "         [-2.0082, -3.3338,  0.9433,  ...,  1.0000,  0.0000, -0.9994],\n",
       "         ...,\n",
       "         [ 1.7810, -3.0175,  1.9397,  ...,  1.0000,  0.0000,  0.1579],\n",
       "         [-3.1577, -2.4110, -0.5291,  ...,  1.0000,  0.0000, -0.8832],\n",
       "         [ 2.0460, -2.8798, -1.8904,  ...,  1.0000,  0.0000,  0.9642]],\n",
       "        device='cuda:0'),\n",
       " tensor([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]], device='cuda:0'))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_agency.memory.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Random Agent Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.67 | 4.00 | 3.98 | 3.98 | 3.40 | 4.05 | 4.03 | 4.05 | 4.01 | 4.04 | 4.05 | 3.83 | 3.95 | 3.91 | 4.02 | 3.91 | 4.01 | 3.97 | 4.00 | 4.02\n",
      "Average score over all 20 agents and the last 111 learnable steps:   3.894\n",
      "4.09 | 3.96 | 4.09 | 3.62 | 3.90 | 4.02 | 4.07 | 4.03 | 3.98 | 3.96 | 3.73 | 4.06 | 4.07 | 3.99 | 4.01 | 3.95 | 4.08 | 3.99 | 4.03 | 4.10\n",
      "Average score over all 20 agents and the last 111 learnable steps:   3.986\n",
      "4.00 | 3.96 | 4.02 | 3.97 | 4.05 | 3.97 | 4.05 | 3.99 | 4.00 | 3.96 | 3.97 | 3.93 | 4.01 | 3.88 | 3.80 | 4.02 | 3.99 | 3.97 | 3.96 | 4.07\n",
      "Average score over all 20 agents and the last 111 learnable steps:   3.978\n"
     ]
    }
   ],
   "source": [
    "rapisodes = 3\n",
    "for rap in range(rapisodes):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]         # reset the environment    \n",
    "    rap_states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    rap_scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    rap_steps = 111\n",
    "    for rep in range(rap_steps):\n",
    "    #    rap_actions = np.random.randn(num_agents, action_size)     # select an action (for each agent)\n",
    "    #    rap_actions = np.clip(rap_actions, -1, 1)                  # all actions between -1 and 1\n",
    "        rap_actions, rap_noises = agency.act(rap_states, return_noise=True)\n",
    "        env_info = env.step(rap_actions)[brain_name]           # send all actions to tne environment\n",
    "        rap_next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rap_rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        rap_dones = env_info.local_done                        # see if episode finished\n",
    "        rap_scores += env_info.rewards                         # update the score (for each agent)\n",
    "        rap_states = rap_next_states                           # roll over states to next time step\n",
    "        #print(\"\\rTotal cumulative score (averaged over all agents) these {:3d} steps: {:7.3f}\\n\".format(rep, np.mean(rap_scores)))\n",
    "        print(\"\\r{:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f}\".format(*rap_scores), end='')\n",
    "        if np.any(rap_dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    #print(\"\\n{:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f}\".format(*rap_scores))\n",
    "    print(\"\\nAverage score over all 20 agents and the last {} learnable steps: {:7.3f}\".format(rap_steps, np.mean(rap_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#from ddpg_agent import *\n",
    "\n",
    "def train(env, agency, n_episodes=2000, max_t=1000, hiscore=30):\n",
    "    \"\"\"Deep Q-Learning for a Continuous Action Space\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        env (environment): Here, a Unity Env. Originally, a Gym(nasium) env\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        hiscore (int): the metric score to achieve to solve the env \n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    episode_times = []; episode_lengths = []; scores = []; agent_scores = []\n",
    "    action_steps = []; noise_steps= []; actor_loss = []; critic_loss = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        epistart = time.time()\n",
    "        score = 0. ; episteps = 0.; episcores = np.zeros(agency.num_agents)\n",
    "        a_loss = 0. ; c_loss = 0.\n",
    "        env_info = env.reset(train_mode=True)['ReacherBrain']\n",
    "        states = env_info.vector_observations\n",
    "        agency.reset()\n",
    "        for t in range(max_t):\n",
    "            actions, noises = agency.act(states, return_noise=True)\n",
    "            action_steps.append(actions)\n",
    "            noise_steps.append(noises)\n",
    "            actions = np.clip(actions+noises, -1, 1)\n",
    "            env_info = env.step(actions)['ReacherBrain']\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            al, cl = agency.step(states, actions, rewards, next_states, dones, return_loss=True)\n",
    "            if al != None: \n",
    "                a_loss += al; c_loss += cl; \n",
    "            states = next_states; episteps+=1\n",
    "            score += np.mean(rewards); episcores += rewards\n",
    "            if np.any(dones):\n",
    "                break  \n",
    "        # Data appendage\n",
    "        scores_window.append(score)       \n",
    "        scores.append(score)\n",
    "        episode_lengths.append(episteps)\n",
    "        actor_loss.append(a_loss/episteps)          \n",
    "        critic_loss.append(c_loss/episteps)\n",
    "        episode_times.append(time.time()-epistart)\n",
    "        cycle_steps = agency.steps%BUFFER_SIZE\n",
    "        buffer_cycle = agency.steps//BUFFER_SIZE\n",
    "\n",
    "        print(\"\\rEpisode {:4d} | Score: {:8.2f} | Actor Loss: {:8.2f} | Critic Loss: {:8.2f} | Average: {:5d} Steps in {:5.3f} sec | Memory Buffer:{:7d} into cycle {:3d}\".format(\n",
    "                                        i_episode,\n",
    "                                        score, actor_loss[-1], critic_loss[-1],\n",
    "                                        int(episteps), time.time()-epistart,\n",
    "                                        cycle_steps, buffer_cycle), end=\"\")\n",
    "        #print(\"\\r{:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f}\".format(*individual_scores), end=\"\")\n",
    "                                                                                                                                        \n",
    "        if i_episode % 10 == 0:\n",
    "            chkpntname = \"checkpoints/actor_chkpnt{}.pth\".format(i_episode)\n",
    "            torch.save(agency.fast_actor.state_dict(), chkpntname)  \n",
    "            chkpntname = \"checkpoints/critic_chkpnt{}.pth\".format(i_episode)\n",
    "            torch.save(agency.fast_critic.state_dict(), chkpntname) \n",
    "            print(\"\\rEpisode {:4d} | Score: {:8.2f} | Actor Loss: {:8.2f} | Critic Loss: {:8.2f} | Average: {:5d} Steps in {:5.3f} sec | Memory Buffer:{:7d} into cycle {:3d}\".format(\n",
    "                                        i_episode, \n",
    "                                        np.mean(scores_window), np.mean(actor_loss), np.mean(critic_loss),\n",
    "                                        int(np.round(np.mean(episode_lengths))), np.mean(episode_times),\n",
    "                                        cycle_steps, buffer_cycle))\n",
    "        #print(\"\\r{:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f}\".format(*individual_scores))\n",
    "        \n",
    "        if np.mean(scores_window)>=hiscore:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:5.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agency.fast_actor.state_dict(), 'checkpoints/actor_slvdpnt.pth')\n",
    "            torch.save(agency.fast_critic.state_dict(), 'checkpoints/critic_slvdpnt.pth')\n",
    "            break\n",
    "            \n",
    "    t_time = time.time() - start\n",
    "    print(\"Total time: {:3d} minutes {:4.2f} seconds \\tAvg. Episode time: {:5.3f} seconds \\tAvg. Episode steps: {:5.3f}\".format(int(\n",
    "            t_time//60),\n",
    "            t_time%60,\n",
    "            t_time/i_episode,\n",
    "            agency.steps/i_episode))\n",
    "    \n",
    "    return scores, episode_lengths, episode_times, action_steps, noise_steps, actor_loss, critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   10 | Score:    14.83 | Actor Loss:    -2.31 | Critic Loss:     0.00 | Average: 400.0 Steps in 5.156 sec | Memory Buffer: 608000 into cycle   0\n",
      "Episode   20 | Score:    14.88 | Actor Loss:    -2.35 | Critic Loss:     0.00 | Average: 400.0 Steps in 5.200 sec | Memory Buffer: 688000 into cycle   0\n",
      "Total time:   1 minutes 44.01 seconds \tAvg. Episode time: 5.201 seconds \tAvg. Episode steps: 34400.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores, episode_lengths, episode_times, action_steps, noise_steps, actor_loss, critic_loss = train(env, new_agency, n_episodes=20, max_t=400, hiscore=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Episodes: 200\tTime Limit: 1001.00 ###\n",
      "Episode   10 | Score:    20.84 | Actor Loss:    -1.07 | Critic Loss:     0.00 | Average: 1001.0 Steps in 14.860 sec | Memory Buffer: 200200 into cycle   0\n",
      "Episode   20 | Score:    29.62 | Actor Loss:    -1.26 | Critic Loss:     0.00 | Average: 1001.0 Steps in 15.661 sec | Memory Buffer: 400400 into cycle   0\n",
      "Episode   30 | Score:    32.58 | Actor Loss:    -1.44 | Critic Loss:     0.00 | Average: 1001.0 Steps in 16.408 sec | Memory Buffer: 600600 into cycle   0\n",
      "Episode   33 | Score:    38.15 | Actor Loss:    -2.04 | Critic Loss:     0.00 | Average: 1001 Steps in 18.180 sec | Memory Buffer: 660660 into cycle   0\n",
      "Environment solved in -67 episodes!\tAverage Score: 33.09\n",
      "Total time:   9 minutes 7.58 seconds \tAvg. Episode time: 16.593 seconds \tAvg. Episode steps: 20020.000\n"
     ]
    }
   ],
   "source": [
    "############### import and Train with loss and noise collected\n",
    "n_episodes=200\n",
    "max_t=1001\n",
    "print('\\r### Episodes: {}\\tTime Limit: {:.2f} ###'.format(n_episodes, max_t))\n",
    "new_scores, new_els, new_ets, new_as, new_ns, new_al, new_cl = train(env, new_agency, n_episodes, max_t, hiscore=33)\n",
    "try:\n",
    "    scores += new_scores ; episode_lengths += new_els ; episode_times += new_ets ; action_steps += new_as ; noise_steps+=new_ns ; actor_loss += new_al ; critic_loss += new_cl\n",
    "except:\n",
    "    scores = new_scores ; episode_lengths = new_els ; episode_times = new_ets ; action_steps = new_as ; noise_steps = new_ns ; actor_loss = new_al ; critic_loss = new_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.10315025, -0.11538596,  0.24034703, -0.40579664],\n",
       "        [ 0.68717949,  0.36316028, -0.25292701,  0.92996039],\n",
       "        [-0.20207494, -0.23506958,  0.11658915, -0.15447941],\n",
       "        [-0.27374019,  0.07822219,  0.07304374,  0.31153692],\n",
       "        [ 0.31771925, -1.04301392,  0.01273255, -0.03675583],\n",
       "        [-0.204526  , -0.03359422,  0.2028703 , -0.55562252],\n",
       "        [ 0.11545115, -0.06138617,  0.49934915, -0.35176294],\n",
       "        [-0.41407389, -0.22499499,  0.0461911 ,  0.13429254],\n",
       "        [-0.48284497,  0.13496894,  0.35822824, -0.06251971],\n",
       "        [ 0.34178109,  0.70259053,  0.13196168,  0.17633476],\n",
       "        [ 0.23219802,  0.00435628, -0.18387529, -0.07305168],\n",
       "        [ 0.29572118, -0.81703505, -0.3140536 ,  0.3438374 ],\n",
       "        [-0.49099325, -0.89312001,  0.34397768,  0.4906701 ],\n",
       "        [-0.10267421,  0.4316444 ,  0.26733916, -0.70209248],\n",
       "        [-0.66111101,  0.45922269,  0.06497996, -0.65953411],\n",
       "        [-0.19146905, -0.23959989, -0.21589022,  0.11882251],\n",
       "        [ 0.15474411, -0.15080087, -1.00924234,  0.31594614],\n",
       "        [ 0.45534323,  0.28709794, -0.0328477 , -0.56214865],\n",
       "        [-0.00421085, -0.63757744, -1.05081732, -0.27256817],\n",
       "        [-0.91245407,  0.06730105, -0.14750456,  0.19811808]]),\n",
       " array([[ 0.38661218, -0.95719826, -0.7415518 , -0.10499944],\n",
       "        [ 0.9999107 , -0.38343775, -0.9957032 , -0.39072406],\n",
       "        [-0.99876016,  0.99419624, -0.85337865,  0.31404746],\n",
       "        [ 0.9932695 , -0.8033514 , -0.21942312,  0.87282866],\n",
       "        [-0.04284666,  0.99967617,  0.59536624,  0.57851976],\n",
       "        [ 0.6819044 ,  0.82100636, -0.7562691 ,  0.61377466],\n",
       "        [-0.9931294 ,  0.8712983 , -0.45128477,  0.3715112 ],\n",
       "        [ 0.92722404,  0.4927353 , -0.7722535 , -0.19152   ],\n",
       "        [ 0.91069317, -0.743565  , -0.05387217, -0.16360693],\n",
       "        [-0.3764977 ,  0.50284225,  0.95356625, -0.78811353],\n",
       "        [-0.9042089 , -0.13589081,  0.09258933, -0.05824799],\n",
       "        [ 0.99921066,  0.12855336,  0.83564913, -0.79276645],\n",
       "        [ 0.91004705,  0.6919297 , -0.7001555 , -0.29551953],\n",
       "        [-0.9611663 , -0.5376354 , -0.9581673 ,  0.8887497 ],\n",
       "        [ 0.9728312 , -0.7010708 , -0.8450302 ,  0.84724176],\n",
       "        [ 0.9999991 ,  0.99731576,  0.82684726, -0.9990886 ],\n",
       "        [-0.94383633,  0.65563405,  0.5844224 ,  0.6187128 ],\n",
       "        [ 0.9666089 ,  0.9992753 ,  0.94544685,  0.94846326],\n",
       "        [ 0.9402113 , -0.9629326 ,  0.99546987,  0.47073573],\n",
       "        [ 0.9999998 , -0.1267461 ,  0.40525934,  0.4623783 ]],\n",
       "       dtype=float32),\n",
       " array([[ 0.28346192, -1.07258423, -0.50120479, -0.51079608],\n",
       "        [ 1.6870902 , -0.02027747, -1.24863023,  0.53923633],\n",
       "        [-1.2008351 ,  0.75912665, -0.73678951,  0.15956805],\n",
       "        [ 0.71952932, -0.72512921, -0.14637937,  1.18436559],\n",
       "        [ 0.27487259, -0.04333776,  0.60809879,  0.54176394],\n",
       "        [ 0.47737838,  0.78741214, -0.55339879,  0.05815214],\n",
       "        [-0.87767822,  0.80991214,  0.04806438,  0.01974825],\n",
       "        [ 0.51315015,  0.2677403 , -0.72606242, -0.05722747],\n",
       "        [ 0.42784819, -0.60859608,  0.30435607, -0.22612663],\n",
       "        [-0.0347166 ,  1.20543277,  1.08552794, -0.61177877],\n",
       "        [-0.67201088, -0.13153453, -0.09128596, -0.13129968],\n",
       "        [ 1.29493183, -0.68848169,  0.52159553, -0.44892905],\n",
       "        [ 0.4190538 , -0.20119031, -0.35617782,  0.19515057],\n",
       "        [-1.06384053, -0.10599098, -0.69082815,  0.18665723],\n",
       "        [ 0.31172018, -0.24184809, -0.78005023,  0.18770765],\n",
       "        [ 0.80853006,  0.75771587,  0.61095704, -0.88026607],\n",
       "        [-0.78909223,  0.50483317, -0.42481993,  0.93465892],\n",
       "        [ 1.42195211,  1.28637327,  0.91259915,  0.38631461],\n",
       "        [ 0.93600045, -1.60051003, -0.05534745,  0.19816756],\n",
       "        [ 0.08754575, -0.05944505,  0.25775478,  0.66049637]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_steps[-2], action_steps[-2], noise_steps[-2] + action_steps[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Change of setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=500):\n",
    "    start = time.time()\n",
    "    episode_times = []\n",
    "    episode_lengths = []\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    agent_scores = []\n",
    "    for i_episode in (range(1, n_episodes+1)):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations\n",
    "        agency.reset()\n",
    "        score = 0.\n",
    "        individual_scores = np.zeros(num_agents)\n",
    "        episteps = 1\n",
    "        epistart = time.time()\n",
    "        while True:\n",
    "            action = agency.act(state)      #action, noise = agency.act(state, return_noise=True)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations\n",
    "            reward = env_info.rewards\n",
    "            done = env_info.local_done\n",
    "            agency.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += np.mean(reward)\n",
    "            individual_scores += reward\n",
    "            if np.any(done):\n",
    "                break \n",
    "            episteps+=1\n",
    "        episode_times.append(time.time()-epistart)\n",
    "        episode_lengths.append(episteps)\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        agent_scores.append(individual_scores)\n",
    "        cycle_steps = agency.steps%ddpg_agent.BUFFER_SIZE\n",
    "        buffer_cycle = agency.steps//ddpg_agent.BUFFER_SIZE\n",
    "        \n",
    "        print('\\rEps{:5d}  Reward: {:8.2f}\\tAvg Reward: {:8.2f}\\tEps Time: {:6.3f} sec\\tBuffer Cycle: {:3d} rem {:7d}'.format(\n",
    "                i_episode, \n",
    "                score, \n",
    "                np.mean(scores_deque),\n",
    "                episode_times[-1],\n",
    "                buffer_cycle,\n",
    "                cycle_steps), end=\"\")\n",
    "\n",
    "        if i_episode % 20 == 0:\n",
    "            chkpntname = \"checkpoints/actor_training_chkpnt{:d}.pth\".format(i_episode)\n",
    "            torch.save(agency.actor_local.state_dict(), chkpntname)  \n",
    "            chkpntname = \"checkpoints/critic_training_chkpnt{:d}.pth\".format(i_episode)\n",
    "            torch.save(agency.critic_local.state_dict(), chkpntname)\n",
    "        \n",
    "        if i_episode % 10 == 0:\n",
    "            print('\\rEps{:5d}\\tReward: {:8.2f}\\tAvg Reward: {:8.2f}\\t Avg Eps Time: {:6.3f} sec\\tBuffer Cycle: {:3d} rem {:7d}'.format(\n",
    "                    i_episode,\n",
    "                    score,\n",
    "                    np.mean(scores_deque),\n",
    "                    np.mean(episode_times),\n",
    "                    buffer_cycle,\n",
    "                    cycle_steps))\n",
    "            print(\"{:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f} | {:4.2f}\".format(*individual_scores))\n",
    "\n",
    "        if np.mean(scores_deque)>=30:\n",
    "            print('\\nScore averaged over 100 episodes and 20 Agents stably above metric of 30.\\nEnvironment officially solved at episode {:d}!\\tAverage Reward: {:6.2f}'.format(\n",
    "                    i_episode-100, \n",
    "                    np.mean(scores_deque)))\n",
    "            torch.save(agency.actor_local.state_dict(), 'checkpoints/slvdpnt_actor.pth')\n",
    "            torch.save(agency.critic_local.state_dict(), 'checkpoints/slvdpnt_critic.pth')\n",
    "            break\n",
    "\n",
    "    torch.save(agency.actor_local.state_dict(), \"checkpoints/actor_trained.pth\")\n",
    "    torch.save(agency.critic_local.state_dict(), \"checkpoints/critic_trained.pth\") \n",
    "    print('\\nFINAL\\nEpisode{:5d}\\tReward: {:8.2f}\\tAverage Reward: {:8.2f}\\tBuffer Cycle: {:3d} rem {:7d}'.format(\n",
    "            i_episode,\n",
    "            score,\n",
    "            np.mean(scores_deque),\n",
    "            buffer_cycle,\n",
    "            cycle_steps))\n",
    "    \n",
    "    t_time = time.time() - start\n",
    "    print(\"Total time: {:3d} minutes {:4.2f} seconds \\t Avg. Episode time: {:5.3f} seconds \\t Avg. Steps per Episode: {:5.1f}\".format(\n",
    "           int(t_time//60),\n",
    "           t_time%60,\n",
    "           t_time/i_episode,\n",
    "           round(np.mean(episode_lengths))))\n",
    "    \n",
    "    return scores, agent_scores, episode_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### TRAIN AGENTS\n",
    "sc, ag, et = ddpg(n_episodes=40)\n",
    "try:\n",
    "    scores+=sc\n",
    "    agent_scores+=ag\n",
    "    episode_times+=et\n",
    "except:\n",
    "    scores=sc\n",
    "    agent_scores=ag\n",
    "    episode_times=et\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "#### Best Config\n",
    "\n",
    "`CriticA(fcs1_units=512, fc2_units=128)`\n",
    "\n",
    "* BUFFER_SIZE = int(5.12e5)  # replay buffer size\n",
    "* BATCH_SIZE = 128        # minibatch size\n",
    "* GAMMA = 0.99            # discount factor\n",
    "* TAU = 1e-3              # for soft update of target parameters\n",
    "* LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "* LR_CRITIC = 1e-4        # learning rate of the critic\n",
    "* WEIGHT_DECAY = 0.       # L2 weight decay\n",
    "\n",
    "> * Episode   10\tReward:   1.00\tAverage Reward:   0.61\tBuffer Cycle:  0 mod 200200\n",
    "> * Episode   20\tReward:   4.11\tAverage Reward:   1.58\tBuffer Cycle:  0 mod 400400\n",
    "> * Episode   30\tReward:   8.67\tAverage Reward:   3.15\tBuffer Cycle:  1 mod  88600\n",
    "> * Episode   40\tReward:  17.64\tAverage Reward:   5.44\tBuffer Cycle:  1 mod 288800\n",
    "> * Episode   50\tReward:  28.35\tAverage Reward:   8.99\tBuffer Cycle:  1 mod 489000\n",
    "> * Episode   60\tReward:  33.21\tAverage Reward:  12.81\tBuffer Cycle:  2 mod 177200\n",
    "> * Episode   70\tReward:  35.12\tAverage Reward:  15.97\tBuffer Cycle:  2 mod 377400\n",
    "> * Episode   80\tReward:  35.23\tAverage Reward:  18.44\tBuffer Cycle:  3 mod  65600\n",
    "> * Episode   90\tReward:  36.47\tAverage Reward:  20.42\tBuffer Cycle:  3 mod 265800\n",
    "> * Episode  100\tReward:  36.46\tAverage Reward:  21.97\tBuffer Cycle:  3 mod 466000\n",
    "> * Episode  110\tReward:  36.44\tAverage Reward:  25.54\tBuffer Cycle:  4 mod 154200\n",
    "> * Episode  120\tReward:  35.09\tAverage Reward:  28.90\tBuffer Cycle:  4 mod 354400\n",
    "> * Episode  124\tReward:  36.91\tAverage Reward:  30.16\tBuffer Cycle:  4 mod 434480\n",
    "\n",
    "**Solved**\n",
    "\n",
    "Environment officially solved at episode 24\tAverage Reward:  30.16\n",
    "\n",
    "---\n",
    "\n",
    "Eps   10\tReward:    15.95\tAvg Reward:    24.20\t Avg Eps Time: 10.485 sec\tBuffer Cycle:   0 mod   10010\n",
    "Eps   20\tReward:    22.38\tAvg Reward:    21.96\t Avg Eps Time: 10.786 sec\tBuffer Cycle:   0 mod   20020\n",
    "Eps   30\tReward:    18.91\tAvg Reward:    21.33\t Avg Eps Time: 10.768 sec\tBuffer Cycle:   0 mod   30030\n",
    "Eps   40\tReward:    27.72\tAvg Reward:    22.27\t Avg Eps Time: 10.752 sec\tBuffer Cycle:   0 mod   40040\n",
    "Eps   50\tReward:    28.59\tAvg Reward:    22.33\t Avg Eps Time: 10.750 sec\tBuffer Cycle:   0 mod   50050\n",
    "Eps   60\tReward:    20.83\tAvg Reward:    21.82\t Avg Eps Time: 10.794 sec\tBuffer Cycle:   0 mod   60060\n",
    "Eps   70\tReward:    20.66\tAvg Reward:    21.67\t Avg Eps Time: 10.781 sec\tBuffer Cycle:   0 mod   70070\n",
    "Eps   80\tReward:    30.70\tAvg Reward:    22.39\t Avg Eps Time: 10.777 sec\tBuffer Cycle:   0 mod   80080\n",
    "Eps   90\tReward:    26.80\tAvg Reward:    22.80\t Avg Eps Time: 10.781 sec\tBuffer Cycle:   0 mod   90090\n",
    "Eps  100\tReward:    17.80\tAvg Reward:    22.86\t Avg Eps Time: 10.789 sec\tBuffer Cycle:   0 mod  100100\n",
    "Eps  110\tReward:    23.14\tAvg Reward:    22.99\t Avg Eps Time: 10.840 sec\tBuffer Cycle:   0 mod  110110\n",
    "Eps  120\tReward:    27.44\tAvg Reward:    23.50\t Avg Eps Time: 10.841 sec\tBuffer Cycle:   0 mod  120120\n",
    "Eps  130\tReward:    17.93\tAvg Reward:    23.65\t Avg Eps Time: 10.844 sec\tBuffer Cycle:   0 mod  130130\n",
    "Eps  140\tReward:    16.29\tAvg Reward:    23.19\t Avg Eps Time: 10.869 sec\tBuffer Cycle:   0 mod  140140\n",
    "Eps  150\tReward:    19.08\tAvg Reward:    22.79\t Avg Eps Time: 10.874 sec\tBuffer Cycle:   0 mod  150150\n",
    "Eps  160\tReward:    26.10\tAvg Reward:    23.10\t Avg Eps Time: 10.883 sec\tBuffer Cycle:   0 mod  160160\n",
    "Eps  170\tReward:    29.94\tAvg Reward:    23.26\t Avg Eps Time: 10.891 sec\tBuffer Cycle:   0 mod  170170\n",
    "Eps  180\tReward:    18.89\tAvg Reward:    22.73\t Avg Eps Time: 10.899 sec\tBuffer Cycle:   0 mod  180180\n",
    "Eps  190\tReward:    26.30\tAvg Reward:    22.30\t Avg Eps Time: 10.908 sec\tBuffer Cycle:   0 mod  190190\n",
    "Eps  200\tReward:    33.29\tAvg Reward:    22.80\t Avg Eps Time: 10.942 sec\tBuffer Cycle:   0 mod  200200\n",
    "Eps  210\tReward:    28.95\tAvg Reward:    22.81\t Avg Eps Time: 10.951 sec\tBuffer Cycle:   0 mod  210210\n",
    "Eps  220\tReward:    33.37\tAvg Reward:    22.99\t Avg Eps Time: 10.975 sec\tBuffer Cycle:   0 mod  220220\n",
    "Eps  230\tReward:    22.64\tAvg Reward:    23.86\t Avg Eps Time: 10.983 sec\tBuffer Cycle:   0 mod  230230\n",
    "Eps  240\tReward:    31.39\tAvg Reward:    25.10\t Avg Eps Time: 10.998 sec\tBuffer Cycle:   0 mod  240240\n",
    "Eps  250\tReward:    32.45\tAvg Reward:    26.21\t Avg Eps Time: 11.007 sec\tBuffer Cycle:   0 mod  250250\n",
    "Eps  260\tReward:    23.49\tAvg Reward:    26.93\t Avg Eps Time: 11.019 sec\tBuffer Cycle:   0 mod  260260\n",
    "Eps  270\tReward:    21.00\tAvg Reward:    27.56\t Avg Eps Time: 11.030 sec\tBuffer Cycle:   0 mod  270270\n",
    "Eps  280\tReward:    30.22\tAvg Reward:    28.44\t Avg Eps Time: 11.050 sec\tBuffer Cycle:   0 mod  280280\n",
    "Eps  290\tReward:    31.85\tAvg Reward:    29.35\t Avg Eps Time: 11.070 sec\tBuffer Cycle:   0 mod  290290\n",
    "Eps  300\tReward:    34.63\tAvg Reward:    29.40\t Avg Eps Time: 11.088 sec\tBuffer Cycle:   0 mod  300300\n",
    "Eps  309\tReward:    36.09\tAvg Reward:    30.10\tEps Time: 11.354 sec\tBuffer Cycle:   0 mod  309309\n",
    "Score averaged over 100 episodes and 1 Agent stably above metric of 30.\n",
    "Environment officially solved at episode 209!\tAverage Reward:  30.10\n",
    "\n",
    "FINAL\n",
    "Episode  309\tReward:    36.09\tAverage Reward:    30.10\tBuffer Cycle:   0 mod  309309\n",
    "Total time:  57 minutes 9.14 seconds \t Avg. Episode time: 11.098 seconds \t Avg. Steps per Episode: 1001.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(agency.actor_local.state_dict(), 'trained_actor_1.pth')\n",
    "#torch.save(agency.critic_local.state_dict(), 'trained_critic_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    env.close()\n",
    "    print(\"Closed the active environment.\")\n",
    "except:\n",
    "    print(\"No active environment detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # For Environment with ONE Agent:\n",
    "    one_env = UnityEnvironment(file_name='../data/Reacher1_Win/Reacher.exe',   # 'UnityReacher20\\Reacher.exe',  #\n",
    "                           worker_id=10, \n",
    "                           seed=SEED, \n",
    "                           no_graphics=False)\n",
    "\n",
    "    # get the default brain\n",
    "    one_brain_name = one_env.brain_names[0]\n",
    "    one_brain = one_env.brains[one_brain_name]\n",
    "\n",
    "    # reset the environment\n",
    "    one_env_info = one_env.reset(train_mode=True)[one_brain_name]\n",
    "\n",
    "    # size of each action\n",
    "    one_action_size = one_brain.vector_action_space_size\n",
    "    print('Size of each action:', one_action_size)\n",
    "\n",
    "    # examine the state space \n",
    "    one_state = one_env_info.vector_observations\n",
    "    one_state_size = one_state.shape[1]\n",
    "    print('There are {} agents. Each observes a state with length: {}'.format(one_state.shape[0], one_state_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(scores)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), scores, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#000\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean Rewards for 20 Agents\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     ax\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43magent_scores\u001b[49m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), [s[i] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m agent_scores], alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n\u001b[0;32m      6\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(scores)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), scores, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#000\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.75\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'agent_scores' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Learning Curve\n",
    "scores\n",
    "fig, ax = plt.subplots(figsize=(16, 5))\n",
    "ax.plot(np.arange(1, len(scores)+1), scores, c='#000', alpha=0.25, linewidth=2.0, label='Mean Rewards for 20 Agents')\n",
    "for i in range(1,20):\n",
    "    ax.plot(np.arange(1, len(agent_scores)+1), [s[i] for s in agent_scores], alpha=0.5, linewidth=0.8)\n",
    "ax.plot(np.arange(1, len(scores)+1), scores, c='#000', alpha=1.0, linewidth=0.75)\n",
    "plt.legend()\n",
    "ax.set_xlabel('Episode #', fontsize=14)\n",
    "ax.set_ylabel('Reward', fontsize=14)\n",
    "ax.set_title('Reacher_20 Environment solved (30+ avg)', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'actions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Actions + noises curves\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mactions\u001b[49m\n\u001b[0;32m      3\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m      4\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(scores)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), scores, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean Reward across 20 Agents\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'actions' is not defined"
     ]
    }
   ],
   "source": [
    "# Actions + noises curves\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.plot(np.arange(1, len(scores)+1), scores, color='r', label='Mean Reward across 20 Agents')\n",
    "for i in range(20):\n",
    "    ax.plot(np.arange(1, len(scores)+1), [s[i] for s in agent_scores], alpha=0.1)\n",
    "plt.legend()\n",
    "ax.set_xlabel('Episode #', fontsize=14)\n",
    "ax.set_ylabel('Reward', fontsize=14)\n",
    "ax.set_title('Unity Reacher Environment using DDPG', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Run some test runs...\n",
    "for _ in range(1):    \n",
    "    env_info = env.reset(train_mode=False)[brain_name]        # reset the environment   \n",
    "    tr_states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    tr_scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        tr_actions = agency.act(tr_states, add_noise=False)   # select an action (for each agent)\n",
    "        env_info = env.step(tr_actions)[brain_name]           # send all actions to tne environment\n",
    "        tr_next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        #tr_rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        tr_dones = env_info.local_done                        # see if episode finished\n",
    "        tr_scores += env_info.rewards                         # update the score (for each agent)\n",
    "        tr_states = tr_next_states                            # roll over states to next time step\n",
    "        if np.any(tr_dones):                                  # exit loop if episode finished\n",
    "            print(np.round(tr_scores, 3), end='\\n')\n",
    "            break\n",
    "        print(np.round(tr_scores, 3), end='')        \n",
    "    print('Total score (averaged over {} agents) this episode: {:3.6f}'.format(num_agents, np.mean(tr_scores)))\n",
    "\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_scores, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Close and Restart\n",
    "try: \n",
    "    env.close()\n",
    "    print(\"Closed the active environment.\")\n",
    "except:\n",
    "    print(\"No active environment detected.\")\n",
    "    \n",
    "if True: # restart_bool\n",
    "    from unityagents import UnityEnvironment\n",
    "    import numpy as np\n",
    "    import random\n",
    "    import torch\n",
    "    from collections import deque\n",
    "    import matplotlib.pyplot as plt\n",
    "    import ddpg_agent\n",
    "    from ddpg_agent import Agent as Agency\n",
    "    from ddpg_agent import SEED, BUFFER_SIZE, BATCH_SIZE, device\n",
    "    print(\"Using device:\", device)\n",
    "    %matplotlib inline    \n",
    "    \n",
    "print(\"Starting new Reacher environment...\")    \n",
    "#env = UnityEnvironment(file_name='UnityReacher20\\Reacher.exe', worker_id=111, seed=SEED, no_graphics=False)\n",
    "env = UnityEnvironment(file_name='../data/Reacher1_Win/Reacher.exe', worker_id=1, seed=SEED, no_graphics=False)\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "#print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!dir checkpoints\n",
    "chkpnt=100\n",
    "chkpnt_name = \"checkpoints/actor_training_chkpnt{}.pth\".format(chkpnt)\n",
    "agent.actor_local.load_state_dict(torch.load(chkpnt_name))\n",
    "chkpnt_name = \"checkpoints/critic_training_chkpnt{}.pth\".format(chkpnt)\n",
    "agent.critic_local.load_state_dict(torch.load(chkpnt_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for chkpnt in [20,80,120]:  #range(100,1000,300):\n",
    "#for agent in enumerate(agents): actor_training_chkpnt40.pth   \n",
    "    chkpnt_name = \"checkpoints/actor_training_chkpnt{}.pth\".format(chkpnt)\n",
    "    agent.actor_local.load_state_dict(torch.load(chkpnt_name))\n",
    "    chkpnt_name = \"checkpoints/critic_training_chkpnt{}.pth\".format(chkpnt)\n",
    "    agent.critic_local.load_state_dict(torch.load(chkpnt_name))\n",
    "    for run in range(2):\n",
    "        score = 0\n",
    "        env_info = env.reset(train_mode=False)[brain_name]      # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        while True:\n",
    "            actions = agent.act(states, add_noise=False)      # select an action (for each agent)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "        print(run+1, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actor_local_params = [p for p in agent.actor_local.parameters()]\n",
    "actor_target_params = [p for p in agent.actor_target.parameters()]\n",
    "#print(actor_local_params[-1])\n",
    "#print(actor_target_params[-1])\n",
    "for x, y in zip(agent.actor_local.parameters(), agent.actor_target.parameters()):\n",
    "    print(\"Shape\", x.shape, \"  Equal\", sum(x==y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "for run in range(5):\n",
    "    actions = agent.act(states, add_noise=False)      # select an action (for each agent)\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "    print(run+1, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Post-Training Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Configuration One WITHOUT NOISE ####\n",
    "## Check Point: 40\n",
    "## Add Noise: False\n",
    "\n",
    "agent = Agency(state_size=state_size, \n",
    "                action_size=action_size, \n",
    "                num_agents=1, \n",
    "                random_seed=SEED,\n",
    "                actor_units_in=256,\n",
    "                actor_units_hid=128,\n",
    "                critic_units_in=512,\n",
    "                critic_units_hid=128, \n",
    "                actor_filename='checkpoints/actor_training_chkpnt40.pth', \n",
    "                critic_filename='checkpoints/critic_training_chkpnt40.pth')\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "for _ in range(3):\n",
    "    actions = agent.act(states, add_noise=False)      # select an action (for each agent)\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Configuration Two WITH NOISE ####\n",
    "## Check Point: 80\n",
    "## Add Noise: True\n",
    "\n",
    "agency = Agency(state_size=state_size, \n",
    "                action_size=action_size, \n",
    "                num_agents=num_agents, \n",
    "                random_seed=SEED,\n",
    "                actor_units_in=256,\n",
    "                actor_units_hid=128,\n",
    "                critic_units_in=512,\n",
    "                critic_units_hid=128, \n",
    "                actor_filename='checkpoints/actor_training_chkpnt80.pth', \n",
    "                critic_filename='checkpoints/critic_training_chkpnt80.pth')\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "for _ in range(3):\n",
    "    actions = agency.act(states, add_noise=True)       # select an action (for each agent)\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### WANDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"reacher v1\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"critic_model\": \"Critic\",\n",
    "    \"actor_model\": \"Actor\",\n",
    "    \"episodes\": 1000,\n",
    "    \"max_t\": 1000\n",
    "    }\n",
    ")\n",
    "\n",
    "# simulate training\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "    \n",
    "    # log metrics to wandb\n",
    "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "    \n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Configuration X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = agent.act(states, add_noise=False)       # select an action (for each agent)\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
