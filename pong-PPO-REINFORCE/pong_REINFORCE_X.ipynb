{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKH0-yBwyZA4"
   },
   "source": [
    "# Welcome!\n",
    "Below, we will learn to implement and train a policy to play atari-pong, using only the pixels as input. We will use convolutional neural nets, multiprocessing, and pytorch to implement and train our policy. Let's get started!\n",
    "\n",
    "(I strongly recommend you to try this notebook on the Udacity workspace first before running it locally on your desktop/laptop, as performance might suffer in different environments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lk3IydYxyZA5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: JSAnimation in c:\\programdata\\anaconda3\\lib\\site-packages (0.1)\n",
      "Requirement already satisfied: gymnasium[accept-rom-license] in c:\\programdata\\anaconda3\\lib\\site-packages (0.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium[accept-rom-license]) (4.5.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium[accept-rom-license]) (2.2.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium[accept-rom-license]) (6.0.0)\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium[accept-rom-license]) (0.0.1)\n",
      "Requirement already satisfied: jax-jumpy>=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium[accept-rom-license]) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium[accept-rom-license]) (1.24.2)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium[accept-rom-license]) (0.4.2)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (8.1.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.28.2)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in c:\\programdata\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (0.5.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium[accept-rom-license]) (3.15.0)\n",
      "Requirement already satisfied: libtorrent in c:\\programdata\\anaconda3\\lib\\site-packages (from AutoROM.accept-rom-license->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.0.7)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (1.26.14)\n",
      "Requirement already satisfied: gymnasium[atari] in c:\\programdata\\anaconda3\\lib\\site-packages (0.27.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (6.0.0)\n",
      "Requirement already satisfied: jax-jumpy>=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (0.2.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (4.5.0)\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (0.0.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (1.24.2)\n",
      "Requirement already satisfied: shimmy[atari]<1.0,>=0.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (0.2.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium[atari]) (3.15.0)\n",
      "Requirement already satisfied: ale-py~=0.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (0.8.1)\n",
      "Requirement already satisfied: importlib-resources in c:\\programdata\\anaconda3\\lib\\site-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (5.12.0)\n",
      "Requirement already satisfied: gymnasium in c:\\programdata\\anaconda3\\lib\\site-packages (0.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium) (4.5.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium) (6.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium) (1.24.2)\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium) (0.0.1)\n",
      "Requirement already satisfied: jax-jumpy>=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium) (0.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.15.0)\n",
      "Requirement already satisfied: progressbar in c:\\programdata\\anaconda3\\lib\\site-packages (2.5)\n",
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random as rand\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "%matplotlib inline\n",
    "\n",
    "# install package for displaying animation\n",
    "!pip install JSAnimation\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from IPython.display import display\n",
    "\n",
    "# render ai gym environment\n",
    "!pip install gymnasium[accept-rom-license]\n",
    "!pip install gymnasium[atari]\n",
    "!pip install gymnasium\n",
    "import gymnasium as gym\n",
    "\n",
    "!pip install progressbar\n",
    "import progressbar as pb\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using device: \",device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "id": "qLYOyrI47L-c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pong-v0',\n",
       " 'PongDeterministic-v0',\n",
       " 'PongNoFrameskip-v0',\n",
       " 'Pong-v4',\n",
       " 'PongDeterministic-v4',\n",
       " 'PongNoFrameskip-v4',\n",
       " 'Pong-ram-v0',\n",
       " 'Pong-ramDeterministic-v0',\n",
       " 'Pong-ramNoFrameskip-v0',\n",
       " 'Pong-ram-v4',\n",
       " 'Pong-ramDeterministic-v4',\n",
       " 'Pong-ramNoFrameskip-v4',\n",
       " 'ALE/Pong-v5',\n",
       " 'ALE/Pong-ram-v5']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "[k for k in list(gym.envs.registry.keys()) if \"Pong\" in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bvHK4cgGyZA6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of available actions:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "# PongDeterministic does not contain random frameskip\n",
    "# so is faster to train than the vanilla Pong-v4 environment\n",
    "\n",
    "env = gym.make('PongDeterministic-v4', full_action_space=False, render_mode='rgb_array')#, render_fps=12)\n",
    "#env = gym.make('Pong-v4', full_action_space=False, frameskip=1, render_mode='rgb_array', old_step_api=True)#, obs_type='grayscale'\n",
    "\n",
    "#print(env.metadata['render_fps'])\n",
    "print(\"List of available actions: \", env.unwrapped.get_action_meanings())\n",
    "\n",
    "# we will only use the actions 'RIGHTFIRE' = 4 and 'LEFTFIRE\" = 5\n",
    "# the 'FIRE' part ensures that the game starts again after losing a life\n",
    "# the actions are hard-coded in pong_utils.py\n",
    "RIGHT=4\n",
    "LEFT=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'render_modes': ['human', 'rgb_array'],\n",
       " 'obs_types': {'grayscale', 'ram', 'rgb'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ueCaKFD2rLw"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nWJcISvX2uNm"
   },
   "outputs": [],
   "source": [
    "def output_volume(in_dims, k, s, p, transpose=False):\n",
    "    ''' Calculate a convolutional layer's output size params:\n",
    "        in_dims = input size \n",
    "        k = kernel size \n",
    "        s = stride (int)\n",
    "        p = padding (int)\n",
    "    '''\n",
    "    if not transpose:\n",
    "        out_size = (in_dims - k + 2*p)/s + 1 #convolution out\n",
    "    else:\n",
    "        out_size = (in_dims - 1)*s + k - 2*p #deconvolution out\n",
    "    return int(np.floor(out_size))\n",
    "\n",
    "# preprocess a single frame\n",
    "# crop image and downsample to 80x80\n",
    "# stack two frames together as input\n",
    "def preprocess_single(image, bkg_color = np.array([144, 72, 17])):\n",
    "    img = np.mean(image[34:-16:2,::2]-bkg_color, axis=-1)/255.\n",
    "    return img\n",
    "\n",
    "# convert outputs of parallelEnv to inputs to pytorch neural net\n",
    "# this is useful for batch processing especially on the GPU\n",
    "def preprocess_batch(images, bkg_color = np.array([144, 72, 17])):\n",
    "    list_of_images = np.asarray(images)\n",
    "    if len(list_of_images.shape) < 5:\n",
    "        list_of_images = np.expand_dims(list_of_images, 1)\n",
    "    # subtract bkg and crop\n",
    "    list_of_images_prepro = np.mean(list_of_images[:,:,34:-16:2,::2]-bkg_color,\n",
    "                                    axis=-1)/255.\n",
    "    batch_input = np.swapaxes(list_of_images_prepro,0,1)\n",
    "    return torch.from_numpy(batch_input).float().to(device)\n",
    "\n",
    "\n",
    "# function to animate a list of frames\n",
    "def animate_frames(frames):\n",
    "    plt.axis('off')\n",
    "\n",
    "    # color option for plotting\n",
    "    # use Greys for greyscale\n",
    "    cmap = None if len(frames[0].shape)==3 else 'Greys'\n",
    "    patch = plt.imshow(frames[0], cmap=cmap)  \n",
    "\n",
    "    fanim = animation.FuncAnimation(plt.gcf(), \\\n",
    "        lambda x: patch.set_data(frames[x]), frames = len(frames), interval=30)\n",
    "    \n",
    "    display(display_animation(fanim, default_mode='once'))\n",
    "    \n",
    "# play a game and display the animation\n",
    "# nrand = number of random steps before using the policy\n",
    "def play(env, policy, time=2000, preprocess=None, nrand=5):\n",
    "    env.reset()\n",
    "\n",
    "    # start game\n",
    "    env.step(1)\n",
    "    \n",
    "    # perform nrand random steps in the beginning\n",
    "    for _ in range(nrand):\n",
    "        frame1, reward1, is_done, is_trunc, info = env.step(np.random.choice([RIGHT,LEFT]))\n",
    "        frame2, reward2, is_done, is_trunc, info = env.step(0)\n",
    "        is_done = is_done or is_trunc\n",
    "    \n",
    "    anim_frames = []\n",
    "    \n",
    "    for _ in range(time):\n",
    "        \n",
    "        frame_input = preprocess_batch([frame1, frame2])\n",
    "        prob = policy(frame_input)\n",
    "        \n",
    "        # RIGHT = 4, LEFT = 5\n",
    "        action = RIGHT if rand.random() < prob else LEFT\n",
    "        frame1, _, is_done, is_trunc, _ = env.step(action)\n",
    "        frame2, _, is_done, is_trunc, _ = env.step(0)\n",
    "        is_done = is_done or is_trunc\n",
    "\n",
    "        if preprocess is None:\n",
    "            anim_frames.append(frame1)\n",
    "        else:\n",
    "            anim_frames.append(preprocess(frame1))\n",
    "\n",
    "        if is_done:\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    animate_frames(anim_frames)\n",
    "    return \n",
    "\n",
    "\n",
    "# collect trajectories for a parallelized parallelEnv object\n",
    "def collect_trajectories(envs, policy, tmax=200, nrand=5):\n",
    "    \n",
    "    # number of parallel instances\n",
    "    n=len(envs.ps)\n",
    "\n",
    "    #initialize returning lists and start the game!\n",
    "    state_list=[]\n",
    "    reward_list=[]\n",
    "    prob_list=[]\n",
    "    action_list=[]\n",
    "\n",
    "    envs.reset()\n",
    "    \n",
    "    # start all parallel agents\n",
    "    envs.step([1]*n)\n",
    "    \n",
    "    # perform nrand random steps\n",
    "    for _ in range(nrand):\n",
    "        fr1, re1, _, _ = envs.step(np.random.choice([RIGHT, LEFT],n))\n",
    "        fr2, re2, _, _ = envs.step([0]*n)\n",
    "    \n",
    "    for t in range(tmax):\n",
    "\n",
    "        # prepare the input\n",
    "        # preprocess_batch properly converts two frames into \n",
    "        # shape (n, 2, 80, 80), the proper input for the policy\n",
    "        # this is required when building CNN with pytorch\n",
    "        batch_input = preprocess_batch([fr1,fr2])\n",
    "        \n",
    "        # probs will only be used as the pi_old\n",
    "        # no gradient propagation is needed\n",
    "        # so we move it to the cpu\n",
    "        probs = policy(batch_input).squeeze().cpu().detach().numpy()\n",
    "        \n",
    "        action = np.where(np.random.rand(n) < probs, RIGHT, LEFT)\n",
    "        probs = np.where(action==RIGHT, probs, 1.0-probs)\n",
    "        \n",
    "        \n",
    "        # advance the game (0=no action)\n",
    "        # we take one action and skip game forward\n",
    "        fr1, re1, is_done, _ = envs.step(action)\n",
    "        fr2, re2, is_done, _ = envs.step([0]*n)\n",
    "\n",
    "        reward = re1 + re2\n",
    "        \n",
    "        # store the result\n",
    "        state_list.append(batch_input)\n",
    "        reward_list.append(reward)\n",
    "        prob_list.append(probs)\n",
    "        action_list.append(action)\n",
    "        \n",
    "        # stop if any of the trajectories is done\n",
    "        # we want all the lists to be retangular\n",
    "        if is_done.any():\n",
    "            break\n",
    "\n",
    "\n",
    "    # return pi_theta, states, actions, rewards, probability\n",
    "    return prob_list, state_list, action_list, reward_list\n",
    "\n",
    "# convert states to probability, passing through the policy\n",
    "def states_to_prob(policy, states):\n",
    "    states = torch.stack(states)\n",
    "    policy_input = states.view(-1,*states.shape[-3:])\n",
    "    return policy(policy_input).view(states.shape[:-3])\n",
    "\n",
    "# return sum of log-prob divided by T\n",
    "# same thing as -policy_loss\n",
    "def surrogate(policy, old_probs, states, actions, rewards,\n",
    "              discount = 0.995, beta=0.01):\n",
    "\n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "    # convert rewards to future rewards\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == RIGHT, new_probs, 1.0-new_probs)\n",
    "\n",
    "    ratio = new_probs/old_probs\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    return torch.mean(ratio*rewards + beta*entropy)\n",
    "\n",
    "    \n",
    "# clipped surrogate function\n",
    "# similar as -policy_loss for REINFORCE, but for PPO\n",
    "def clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "                      discount=0.995,\n",
    "                      epsilon=0.1, beta=0.01):\n",
    "\n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "    # convert rewards to future rewards\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == RIGHT, new_probs, 1.0-new_probs)\n",
    "    \n",
    "    # ratio for clipping\n",
    "    ratio = new_probs/old_probs\n",
    "\n",
    "    # clipped function\n",
    "    clip = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "    clipped_surrogate = torch.min(ratio*rewards, clip*rewards)\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    \n",
    "    # this returns an average of all the entries of the tensor\n",
    "    # effective computing L_sur^clip / T\n",
    "    # averaged over time-step and number of trajectories\n",
    "    # this is desirable because we have normalized our rewards\n",
    "    return torch.mean(clipped_surrogate + beta*entropy)\n",
    "\n",
    "class Policy_Solution(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Policy_Solution, self).__init__()\n",
    "        # 80x80x2 to 38x38x4\n",
    "        # 2 channel from the stacked frame\n",
    "        self.conv1 = nn.Conv2d(2, 4, kernel_size=6, stride=2, bias=False)\n",
    "        # 38x38x4 to 9x9x32\n",
    "        self.conv2 = nn.Conv2d(4, 16, kernel_size=6, stride=4)\n",
    "        self.size=9*9*16\n",
    "        \n",
    "        # two fully connected layer\n",
    "        self.fc1 = nn.Linear(self.size, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "        # Sigmoid to \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1,self.size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.sig(self.fc2(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpACk9hYyZA6"
   },
   "source": [
    "# Preprocessing\n",
    "To speed up training, we can simplify the input by cropping the images and use every other pixel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzCwM5nUYeQY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24e809b3a30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, info = env.reset()\n",
    "env.step(1)\n",
    "plt.imshow(preprocess_single(obs), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8-SlWJkNZQRA"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(preprocess_single(frame), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGreys\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "frame = env.render()\n",
    "plt.imshow(preprocess_single(frame), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "4-GMFDmryZA7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# show what a preprocessed image looks like\n",
    "obs, info = env.reset()\n",
    "_, _, _, _, _ = env.step(0)\n",
    "# get a frame after 20 steps\n",
    "for _ in range(20):\n",
    "    frame, _, _, _, _ = env.step(5)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('original image')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('preprocessed image')\n",
    "\n",
    "# 80 x 80 black and white image\n",
    "plt.imshow(preprocess_single(frame), cmap='Greys')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yutVg1-yZA7"
   },
   "source": [
    "# Policy\n",
    "\n",
    "## Exercise 1: Implement your policy\n",
    " \n",
    "Here, we define our policy. The input is the stack of two different frames (which captures the movement), and the output is a number $P_{\\rm right}$, the probability of moving left. Note that $P_{\\rm left}= 1-P_{\\rm right}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ccj3v2MDo39z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_volume(80,5,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1IxzofWy5rzq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_volume(26,4,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "cZ8Y5aadyZA7"
   },
   "outputs": [],
   "source": [
    "# set up a convolutional neural net\n",
    "# the output is the probability of moving right\n",
    "# P(left) = 1-P(right)\n",
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "    ########\n",
    "    ## \n",
    "    ## Modify your neural network\n",
    "    ##\n",
    "    ########\n",
    "        \n",
    "        # 80x80 to outputsize x outputsize\n",
    "        # outputsize = (inputsize - kernel_size + stride)/stride \n",
    "        # (round up if not an integer)\n",
    "\n",
    "        # output = 9x9 here\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=5, stride=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 16, kernel_size=4, stride=3, padding=1)\n",
    "        self.size=16*9*9\n",
    "        \n",
    "        # 1 fully connected layer\n",
    "        self.fc1 = nn.Linear(self.size, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "    ########\n",
    "    ## \n",
    "    ## Modify your neural network\n",
    "    ##\n",
    "    ########\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # flatten the tensor\n",
    "        x = x.view(-1,self.size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.sig(self.fc2(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MhkD58VYz_jh"
   },
   "outputs": [],
   "source": [
    "# use your own policy!\n",
    "policy=Policy().to(device)\n",
    "\n",
    "#import pong_utils\n",
    "#policy=pong_utils.Policy().to(device)\n",
    "\n",
    "# we use the adam optimizer with learning rate 2e-4\n",
    "# optim.SGD is also possible\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "id": "HY_lXi4iiqRt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.step(1) [[[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ...\n",
      "  [144  72  17]\n",
      "  [144  72  17]\n",
      "  [144  72  17]]\n",
      "\n",
      " [[144  72  17]\n",
      "  [144  72  17]\n",
      "  [144  72  17]\n",
      "  ...\n",
      "  [144  72  17]\n",
      "  [144  72  17]\n",
      "  [144  72  17]]\n",
      "\n",
      " [[144  72  17]\n",
      "  [144  72  17]\n",
      "  [144  72  17]\n",
      "  ...\n",
      "  [144  72  17]\n",
      "  [144  72  17]\n",
      "  [144  72  17]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[236 236 236]\n",
      "  [236 236 236]\n",
      "  [236 236 236]\n",
      "  ...\n",
      "  [236 236 236]\n",
      "  [236 236 236]\n",
      "  [236 236 236]]\n",
      "\n",
      " [[236 236 236]\n",
      "  [236 236 236]\n",
      "  [236 236 236]\n",
      "  ...\n",
      "  [236 236 236]\n",
      "  [236 236 236]\n",
      "  [236 236 236]]\n",
      "\n",
      " [[236 236 236]\n",
      "  [236 236 236]\n",
      "  [236 236 236]\n",
      "  ...\n",
      "  [236 236 236]\n",
      "  [236 236 236]\n",
      "  [236 236 236]]] 0.0 False False {'lives': 0, 'episode_frame_number': 88, 'frame_number': 88}\n"
     ]
    }
   ],
   "source": [
    "x, y, z, u, w = env.step(1)\n",
    "print(\"env.step(1)\",x, y, z, u, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMuJlNZbyZA7"
   },
   "source": [
    "# Game visualization\n",
    "pong_utils contain a play function given the environment and a policy. An optional preprocess function can be supplied. Here we define a function that plays a game and shows learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\JSAnimation\\html_writer.py:281: MatplotlibDeprecationWarning: \n",
      "The 'clear_temp' parameter of setup() was deprecated in Matplotlib 3.3 and will be removed two minor releases later. If any parameter follows 'clear_temp', they should be passed as keyword, not positionally.\n",
      "  super(HTMLWriter, self).setup(fig, outfile, dpi,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HTMLWriter' object has no attribute '_temp_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\JSAnimation\\IPython_display.py\u001b[0m in \u001b[0;36manim_to_html\u001b[1;34m(anim, fps, embed_frames, default_mode)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m#with tempfile.NamedTemporaryFile(suffix='.html') as f:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_NameOnlyTemporaryFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'.html'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             anim.save(f.name,  writer=HTMLWriter(fps=fps,\n\u001b[0m\u001b[0;32m     75\u001b[0m                                                  \u001b[0membed_frames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membed_frames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                                                  default_mode=default_mode))\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\animation.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filename, writer, fps, dpi, codec, bitrate, extra_args, metadata, extra_anim, savefig_kwargs, progress_callback)\u001b[0m\n\u001b[0;32m   1175\u001b[0m                         \u001b[0mprogress_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe_number\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_frames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m                         \u001b[0mframe_number\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m                 \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrab_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0msavefig_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\animation.py\u001b[0m in \u001b[0;36msaving\u001b[1;34m(self, fig, outfile, dpi, *args, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\animation.py\u001b[0m in \u001b[0;36mfinish\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    503\u001b[0m         \u001b[1;31m# Call run here now that all frame grabbing is done. All temp files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[1;31m# are available to be assembled.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    506\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Will call clean-up\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\JSAnimation\\html_writer.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[0mof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mJS_INCLUDE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m             of.write(DISPLAY_TEMPLATE.format(id=self.new_id(),\n\u001b[1;32m--> 323\u001b[1;33m                                              \u001b[0mNframes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_temp_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m                                              \u001b[0mfill_frames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_frames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m                                              \u001b[0minterval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HTMLWriter' object has no attribute '_temp_names'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.animation.FuncAnimation at 0x209d6387fa0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import js_examples as jsx\n",
    "jsx.basic_animation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hAGcazFHvszl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "frame, reward, is_done, is_trunc, info = env.step(1)\n",
    "print(\"reward:\", reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2Za18MpsyZA7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\JSAnimation\\html_writer.py:281: MatplotlibDeprecationWarning: \n",
      "The 'clear_temp' parameter of setup() was deprecated in Matplotlib 3.3 and will be removed two minor releases later. If any parameter follows 'clear_temp', they should be passed as keyword, not positionally.\n",
      "  super(HTMLWriter, self).setup(fig, outfile, dpi,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HTMLWriter' object has no attribute '_temp_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16740/4139873998.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mrand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreprocess_single\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# try to add the option \"preprocess=pong_utils.preprocess_single\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# to see what the agent sees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16740/1149953824.py\u001b[0m in \u001b[0;36mplay\u001b[1;34m(env, policy, time, preprocess, nrand)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0manimate_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manim_frames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16740/1149953824.py\u001b[0m in \u001b[0;36manimate_frames\u001b[1;34m(frames)\u001b[0m\n\u001b[0;32m     44\u001b[0m         lambda x: patch.set_data(frames[x]), frames = len(frames), interval=30)\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisplay_animation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfanim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'once'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;31m# play a game and display the animation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\JSAnimation\\IPython_display.py\u001b[0m in \u001b[0;36mdisplay_animation\u001b[1;34m(anim, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;34m\"\"\"Display the animation with an IPython HTML object\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mHTML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manim_to_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\JSAnimation\\IPython_display.py\u001b[0m in \u001b[0;36manim_to_html\u001b[1;34m(anim, fps, embed_frames, default_mode)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m#with tempfile.NamedTemporaryFile(suffix='.html') as f:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_NameOnlyTemporaryFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'.html'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             anim.save(f.name,  writer=HTMLWriter(fps=fps,\n\u001b[0m\u001b[0;32m     75\u001b[0m                                                  \u001b[0membed_frames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membed_frames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                                                  default_mode=default_mode))\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\animation.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filename, writer, fps, dpi, codec, bitrate, extra_args, metadata, extra_anim, savefig_kwargs, progress_callback)\u001b[0m\n\u001b[0;32m   1175\u001b[0m                         \u001b[0mprogress_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe_number\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_frames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m                         \u001b[0mframe_number\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m                 \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrab_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0msavefig_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\animation.py\u001b[0m in \u001b[0;36msaving\u001b[1;34m(self, fig, outfile, dpi, *args, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\animation.py\u001b[0m in \u001b[0;36mfinish\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    503\u001b[0m         \u001b[1;31m# Call run here now that all frame grabbing is done. All temp files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[1;31m# are available to be assembled.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    506\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Will call clean-up\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\JSAnimation\\html_writer.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[0mof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mJS_INCLUDE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m             of.write(DISPLAY_TEMPLATE.format(id=self.new_id(),\n\u001b[1;32m--> 323\u001b[1;33m                                              \u001b[0mNframes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_temp_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m                                              \u001b[0mfill_frames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_frames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m                                              \u001b[0minterval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HTMLWriter' object has no attribute '_temp_names'"
     ]
    }
   ],
   "source": [
    "import random as rand\n",
    "play(env, policy, time=100, preprocess=preprocess_single) \n",
    "# try to add the option \"preprocess=pong_utils.preprocess_single\"\n",
    "# to see what the agent sees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0BYuvlKAxkHB"
   },
   "outputs": [],
   "source": [
    "RIGHT=4;LEFT=5\n",
    "for _ in range(5):\n",
    "    frame1, reward1, is_done, is_trunc, info = env.step(np.random.choice([RIGHT,LEFT]))\n",
    "    frame2, reward2, is_done, is_trunc, info = env.step(0)\n",
    "    is_done = is_done or is_trunc\n",
    "    if is_done: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOAWmC1lV-HD"
   },
   "outputs": [],
   "source": [
    "seed=12345\n",
    "state, _ = env.reset(seed=seed)\n",
    "img = plt.imshow(env.render())    \n",
    "for j in range(100):\n",
    "    action = agent.act(state)\n",
    "    state, reward, done, dun, _ = env.step(action)\n",
    "    done = done or dun\n",
    "    img.set_data(env.render()) \n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    if done:\n",
    "        break \n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UfGiK-IyZA8"
   },
   "source": [
    "# Rollout\n",
    "Before we start the training, we need to collect samples. To make things efficient we use parallelized environments to collect multiple examples at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlW2FrkM8Xkj"
   },
   "outputs": [],
   "source": [
    "# taken from openai/baseline\n",
    "# with minor edits\n",
    "# see https://github.com/openai/baselines/baselines/common/vec_env/subproc_vec_env.py\n",
    "# \n",
    "# from parallelEnv.py\n",
    "\n",
    "from multiprocessing import Process, Pipe\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class CloudpickleWrapper(object):\n",
    "    \"\"\"\n",
    "    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "\n",
    "    def __getstate__(self):\n",
    "        import cloudpickle\n",
    "        return cloudpickle.dumps(self.x)\n",
    "\n",
    "    def __setstate__(self, ob):\n",
    "        import pickle\n",
    "        self.x = pickle.loads(ob)\n",
    "\n",
    "class VecEnv(ABC):\n",
    "    \"\"\"\n",
    "    An abstract asynchronous, vectorized environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_envs, observation_space, action_space):\n",
    "        self.num_envs = num_envs\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset all the environments and return an array of\n",
    "        observations, or a dict of observation arrays.\n",
    "        If step_async is still doing work, that work will\n",
    "        be cancelled and step_wait() should not be called\n",
    "        until step_async() is invoked again.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step_async(self, actions):\n",
    "        \"\"\"\n",
    "        Tell all the environments to start taking a step\n",
    "        with the given actions.\n",
    "        Call step_wait() to get the results of the step.\n",
    "        You should not call this if a step_async run is\n",
    "        already pending.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step_wait(self):\n",
    "        \"\"\"\n",
    "        Wait for the step taken with step_async().\n",
    "        Returns (obs, rews, dones, infos):\n",
    "         - obs: an array of observations, or a dict of\n",
    "                arrays of observations.\n",
    "         - rews: an array of rewards\n",
    "         - dones: an array of \"episode done\" booleans\n",
    "         - infos: a sequence of info objects\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Clean up the environments' resources.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Step the environments synchronously.\n",
    "        This is available for backwards compatibility.\n",
    "        \"\"\"\n",
    "        self.step_async(actions)\n",
    "        return self.step_wait()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        #logger.warn('Render not defined for %s' % self)\n",
    "        pass\n",
    "        \n",
    "    @property\n",
    "    def unwrapped(self):\n",
    "        if isinstance(self, VecEnvWrapper):\n",
    "            return self.venv.unwrapped\n",
    "        else:\n",
    "            return self\n",
    "\n",
    "def worker(remote, parent_remote, env_fn_wrapper):\n",
    "    parent_remote.close()\n",
    "    env = env_fn_wrapper.x\n",
    "    while True:\n",
    "        cmd, data = remote.recv()\n",
    "        if cmd == 'step':\n",
    "            ob, reward, done, trunc, info = env.step(data)\n",
    "            done = done or trunc\n",
    "            if done:\n",
    "                ob = env.reset()\n",
    "            remote.send((ob, reward, done, info))\n",
    "        elif cmd == 'reset':\n",
    "            ob = env.reset()\n",
    "            remote.send(ob)\n",
    "        elif cmd == 'reset_task':\n",
    "            ob = env.reset_task()\n",
    "            remote.send(ob)\n",
    "        elif cmd == 'close':\n",
    "            remote.close()\n",
    "            break\n",
    "        elif cmd == 'get_spaces':\n",
    "            remote.send((env.observation_space, env.action_space))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "class parallelEnv(VecEnv):\n",
    "    def __init__(self, env_name='PongDeterministic-v4',\n",
    "                 n=4, seed=None,\n",
    "                 spaces=None):\n",
    " \n",
    "        env_fns = [ gym.make('PongDeterministic-v4', full_action_space=False, render_mode='rgb_array') for _ in range(n) ] #gym.make(env_name)\n",
    "\n",
    "        if seed is not None:\n",
    "            for i,e in enumerate(env_fns):\n",
    "                e.seed(i+seed)\n",
    "        \n",
    "        \"\"\"\n",
    "        envs: list of gym environments to run in subprocesses\n",
    "        adopted from openai baseline\n",
    "        \"\"\"\n",
    "        self.waiting = False\n",
    "        self.closed = False\n",
    "        nenvs = len(env_fns)\n",
    "        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])\n",
    "        self.ps = [Process(target=worker, args=(work_remote, remote, CloudpickleWrapper(env_fn)))\n",
    "            for (work_remote, remote, env_fn) in zip(self.work_remotes, self.remotes, env_fns)]\n",
    "        for p in self.ps:\n",
    "            p.daemon = True # if the main process crashes, we should not cause things to hang\n",
    "            p.start()\n",
    "        for remote in self.work_remotes:\n",
    "            remote.close()\n",
    "\n",
    "        self.remotes[0].send(('get_spaces', None))\n",
    "        observation_space, action_space = self.remotes[0].recv()\n",
    "        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n",
    "\n",
    "    def step_async(self, actions):\n",
    "        for remote, action in zip(self.remotes, actions):\n",
    "            remote.send(('step', action))\n",
    "        self.waiting = True\n",
    "\n",
    "    def step_wait(self):\n",
    "        results = [remote.recv() for remote in self.remotes]\n",
    "        self.waiting = False\n",
    "        obs, rews, dones, infos = zip(*results)\n",
    "        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n",
    "\n",
    "    def reset(self):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('reset', None))\n",
    "        return np.stack([remote.recv() for remote in self.remotes])\n",
    "\n",
    "    def reset_task(self):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('reset_task', None))\n",
    "        return np.stack([remote.recv() for remote in self.remotes])\n",
    "\n",
    "    def close(self):\n",
    "        if self.closed:\n",
    "            return\n",
    "        if self.waiting:\n",
    "            for remote in self.remotes:            \n",
    "                remote.recv()\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('close', None))\n",
    "        for p in self.ps:\n",
    "            p.join()\n",
    "        self.closed = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2x_DGGGyZA8"
   },
   "outputs": [],
   "source": [
    "envs = parallelEnv('PongDeterministic-v4', n=4, seed=12345)\n",
    "prob, state, action, reward = collect_trajectories(envs, policy, tmax=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCVhvhANyZA8"
   },
   "source": [
    "# Function Definitions\n",
    "Here you will define key functions for training. \n",
    "\n",
    "## Exercise 2: write your own function for training\n",
    "(this is the same as policy_loss except the negative sign)\n",
    "\n",
    "### REINFORCE\n",
    "you have two choices (usually it's useful to divide by the time since we've normalized our rewards and the time of each trajectory is fixed)\n",
    "\n",
    "1. $\\frac{1}{T}\\sum^T_t R_{t}^{\\rm future}\\log(\\pi_{\\theta'}(a_t|s_t))$\n",
    "2. $\\frac{1}{T}\\sum^T_t R_{t}^{\\rm future}\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}$ where $\\theta'=\\theta$ and make sure that the no_grad is enabled when performing the division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8gfuHvAyZA8"
   },
   "outputs": [],
   "source": [
    "def surrogate(policy, old_probs, states, actions, rewards,\n",
    "              discount = 0.995, beta=0.01):\n",
    "\n",
    "    ########\n",
    "    ## \n",
    "    ## WRITE YOUR OWN CODE HERE\n",
    "    ##\n",
    "    ########\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == RIGHT, new_probs, 1.0-new_probs)\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # which prevents policy to become exactly 0 or 1\n",
    "    # this helps with exploration\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    return torch.mean(beta*entropy)\n",
    "\n",
    "Lsur= surrogate(policy, prob, state, action, reward)\n",
    "\n",
    "print(Lsur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHN9oeSnyZA8"
   },
   "source": [
    "# Training\n",
    "We are now ready to train our policy!\n",
    "WARNING: make sure to turn on GPU, which also enables multicore processing. It may take up to 45 minutes even with GPU enabled, otherwise it will take much longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSQ17VPwyZA9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from parallelEnv import parallelEnv\n",
    "# WARNING: running through all 800 episodes will take 30-45 minutes\n",
    "\n",
    "# training loop max iterations\n",
    "episode = 100\n",
    "# episode = 800\n",
    "\n",
    "# widget bar to display progress\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "# initialize environment\n",
    "envs = parallelEnv('PongDeterministic-v4', n=8, seed=1234)\n",
    "\n",
    "discount_rate = .99\n",
    "beta = .01\n",
    "tmax = 320\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = collect_trajectories(envs, policy, tmax=tmax)\n",
    "        \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "    # this is the SOLUTION!\n",
    "    # use your own surrogate function\n",
    "    # L = -surrogate(policy, old_probs, states, actions, rewards, beta=beta)\n",
    "    \n",
    "    L = -surrogate(policy, old_probs, states, actions, rewards, beta=beta)\n",
    "    optimizer.zero_grad()\n",
    "    L.backward()\n",
    "    optimizer.step()\n",
    "    del L\n",
    "        \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcifaOgcyZA9"
   },
   "outputs": [],
   "source": [
    "# play game after training!\n",
    "play(env, policy, time=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gm3n6N4byZA9"
   },
   "outputs": [],
   "source": [
    "plt.plot(mean_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_aALEueyZA9"
   },
   "outputs": [],
   "source": [
    "# save your policy!\n",
    "torch.save(policy, 'PPO.policy')\n",
    "\n",
    "# load your policy if needed\n",
    "# policy = torch.load('REINFORCE_OG.policy')\n",
    "\n",
    "# try and test out the solution!\n",
    "# policy = torch.load('PPO_OG.policy')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "BKH0-yBwyZA4",
    "8ueCaKFD2rLw",
    "wpACk9hYyZA6",
    "7yutVg1-yZA7",
    "aMuJlNZbyZA7",
    "0UfGiK-IyZA8"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
