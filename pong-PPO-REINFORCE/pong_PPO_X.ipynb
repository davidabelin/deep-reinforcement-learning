{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKH0-yBwyZA4"
   },
   "source": [
    "# Welcome!\n",
    "Below, we will learn to implement and train a policy to play atari-pong, using only the pixels as input. We will use convolutional neural nets, multiprocessing, and pytorch to implement and train our policy. Let's get started!\n",
    "\n",
    "(I strongly recommend you to try this notebook on the Udacity workspace first before running it locally on your desktop/laptop, as performance might suffer in different environments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lk3IydYxyZA5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tusing Ipython\n",
      "\tusing device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# render open_ai gym environment\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import progressbar as pb\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "\n",
    "%matplotlib inline\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    print(\"\\tusing Ipython\")\n",
    "    from IPython.display import display\n",
    "else:\n",
    "    print(\"pyvirtualdisplay\")\n",
    "    #!python -m pip install pyvirtualdisplay\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(1400, 900))\n",
    "    display.start()\n",
    "ion_context = plt.ion()\n",
    "\n",
    "from parallelEnv_X import parallelEnv\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"\\tusing device:\", device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qLYOyrI47L-c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parallelEnv_X.parallelEnv"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[k for k in list(gym.envs.registry.keys()) if \"Pong\" in k]\n",
    "parallelEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bvHK4cgGyZA6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of available actions:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "# PongDeterministic does not contain random frameskip\n",
    "# so is faster to train than the vanilla Pong-v4 environment\n",
    "\n",
    "env = gym.make('PongDeterministic-v4', render_mode='rgb_array')\n",
    "#env = gym.make('Pong-v4', full_action_space=False, frameskip=1, render_mode='rgb_array', old_step_api=True)#, obs_type='grayscale'\n",
    "\n",
    "print(\"List of available actions: \", env.unwrapped.get_action_meanings())\n",
    "\n",
    "# we will only use the actions 'RIGHTFIRE' = 4 and 'LEFTFIRE\" = 5\n",
    "# the 'FIRE' part ensures that the game starts again after losing a life\n",
    "# the actions are hard-coded in pong_utils.py\n",
    "RIGHT=4\n",
    "LEFT=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ueCaKFD2rLw",
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nWJcISvX2uNm"
   },
   "outputs": [],
   "source": [
    "def output_volume(in_dims, k, s, p, transpose=False):\n",
    "    ''' Calculate a convolutional layer's output size from input size\n",
    "        and the layer's parameters:\n",
    "        in_dims (int or tuple): input dimensions\n",
    "        k (int or tuple): kernel size \n",
    "        s (int or tuple): stride\n",
    "        p (int or tuple): padding \n",
    "    '''\n",
    "    if not transpose:\n",
    "    #convolution out\n",
    "        try:\n",
    "            out_size = int(np.floor((in_dims - k + 2*p)/s + 1))\n",
    "        except:\n",
    "            out_size = tuple(int(np.floor((idid - kk + 2*pp)/ss + 1)) if ss!=0 else None for idid, kk, ss, pp in zip(in_dims,k,s,p))\n",
    "    \n",
    "    else: \n",
    "    #deconvolution out\n",
    "        try:\n",
    "            out_size = int(np.floor((in_dims - 1)*s + k - 2*p))\n",
    "        except:\n",
    "            out_size = tuple(int(np.floor((idid - 1)*ss + kk - 2*pp)) for idid, kk, ss, pp in zip(in_dims,k,s,p))\n",
    "        \n",
    "    return out_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lives': 0, 'episode_frame_number': 4, 'frame_number': 20}\n",
      "{'lives': 0, 'episode_frame_number': 4, 'frame_number': 20}\n",
      "{'lives': 0, 'episode_frame_number': 4, 'frame_number': 20}\n",
      "{'lives': 0, 'episode_frame_number': 4, 'frame_number': 20}\n",
      "{'lives': 0, 'episode_frame_number': 4, 'frame_number': 20}\n",
      "{'lives': 0, 'episode_frame_number': 4, 'frame_number': 20}\n",
      "{'lives': 0, 'episode_frame_number': 4, 'frame_number': 20}\n",
      "{'lives': 0, 'episode_frame_number': 4, 'frame_number': 20}\n",
      "{'lives': 0, 'episode_frame_number': 4, 'frame_number': 20}\n",
      "{'lives': 0, 'episode_frame_number': 4, 'frame_number': 20}\n",
      "{'lives': 0, 'episode_frame_number': 4, 'frame_number': 20}\n",
      "{'lives': 0, 'episode_frame_number': 4, 'frame_number': 28}\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[WinError 232] The pipe is being closed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m     env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(env\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m4\u001b[39m)[\u001b[38;5;241m4\u001b[39m])\n\u001b[1;32m----> 4\u001b[0m \u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnenvs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Repositories\\deep-reinforcement-learning\\pong-PPO-REINFORCE\\parallelEnv_X.py:84\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions):\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m    Step the environments synchronously.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    This is available for backwards compatibility.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_wait()\n",
      "File \u001b[1;32m~\\Documents\\Repositories\\deep-reinforcement-learning\\pong-PPO-REINFORCE\\parallelEnv_X.py:166\u001b[0m, in \u001b[0;36mparallelEnv.step_async\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions):\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m remote, action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremotes, actions):\n\u001b[1;32m--> 166\u001b[0m         \u001b[43mremote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstep\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\general\\lib\\multiprocessing\\connection.py:211\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_writable()\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\general\\lib\\multiprocessing\\connection.py:285\u001b[0m, in \u001b[0;36mPipeConnection._send_bytes\u001b[1;34m(self, buf)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_send_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, buf):\n\u001b[1;32m--> 285\u001b[0m     ov, err \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWriteFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlapped\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mERROR_IO_PENDING:\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [WinError 232] The pipe is being closed"
     ]
    }
   ],
   "source": [
    "for env in envs.env_fns:\n",
    "    env.reset()\n",
    "    print(env.step(4)[4])\n",
    "envs.step([4]*nenvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess a single frame: stack two imgs together, crop and downsample to 80x80\n",
    "def preprocess_single(image, bkg_color = np.array([144, 72, 17])):\n",
    "    img = np.mean(image[34:-16:2, ::2]-bkg_color, axis=-1)/255.\n",
    "    return img # as an np.array\n",
    "\n",
    "# convert outputs of parallelEnv to inputs to pytorch neural net\n",
    "# this is useful for batch processing especially on the GPU\n",
    "def preprocess_batch(images, bkg_color = np.array([144, 72, 17])):\n",
    "    '''Specified to \"properly convert two frames into shape (n, 2, 80, 80)\" or (N,2,W,H)\n",
    "        Called by: play(), collect_trajectories()\n",
    "        PARAMS\n",
    "        images (np.array): array of size (N, 2, W, H, C) where:\n",
    "                                          N = number of contributing envs\n",
    "                                          2 = two images, each shaped (W,H,C)\n",
    "                                          W = Width of image\n",
    "                                          H = height of image\n",
    "                                          C = # color channels (1=None, or 3)                                      \n",
    "        bkg_color (np.array): adjustment to subtract from each color channel shape (3,)\n",
    "        RETURNS\n",
    "        batch_input (np.array): shaped (N,2,80,80) and scaled to (0,1) values,\n",
    "                                ready to be torched and sent to policy\n",
    "    '''\n",
    "    if len(images.shape) < 5:              # if shape not (N,2,W,H,C) then:\n",
    "        images = np.expand_dims(images, 0) # expand to (1,2,W,H,C) \n",
    "    # subtract bkg and crop\n",
    "    batch_input = images[:, :, 34:-16:2, ::2]-bkg_color\n",
    "    # average over last axis (the r, g, and b color channels)\n",
    "    batch_input = np.mean(batch_input, axis=-1)/255. #shape now (N,2,80,80)\n",
    "    return batch_input  # returns an np.array \n",
    "                        # note NOT torch.from_numpy().float().to(device)\n",
    "\n",
    "# convert states to probability, passing through the policy\n",
    "def states_to_probs(policy, states):\n",
    "    '''Only called from clipped_surrogate\n",
    "       Only this fxn and collect_T() call Policy\n",
    "       !!! Backwards grad only applied here !!!\n",
    "       Takes states, a tensor of preprocessed rgb-tensors on cuda, and\n",
    "             policy, a CNN on cuda\n",
    "       Returns policy outputs, still as a tensor on cuda\n",
    "       '''\n",
    "    policy_input = states.view(-1, *states.shape[-3:]) #\n",
    "    probs = policy(policy_input)\n",
    "    return probs.view(states.shape[:-3])  #cuda tensor -->\n",
    "\n",
    "# collect trajectories for a parallelized parallelEnv object\n",
    "def collect_trajectories(envs, policy, max_steps=500, nrand=4):\n",
    "    \n",
    "    # number of parallel instances\n",
    "    n=envs.n\n",
    "\n",
    "    #initialize returning lists and start the game!\n",
    "    state_list=[]\n",
    "    reward_list=[]\n",
    "    prob_list=[]\n",
    "    polprob_list=[]\n",
    "    action_list=[]\n",
    "\n",
    "    for env in envs:\n",
    "        env.reset(seed=env.seed)\n",
    "    \n",
    "    # start all parallel agents\n",
    "    _, _, _, _ = envs.step([RIGHT]*n)\n",
    "    \n",
    "    # perform nrand random steps\n",
    "    for _ in range(nrand):\n",
    "        st1, re1, _, _ = envs.step(np.random.choice([RIGHT, LEFT, 0], n))\n",
    "        st2, re2, _, _ = envs.step([0]*n)                              # n (s1, s2)\n",
    "    \n",
    "    for t in range(max_steps):\n",
    "\n",
    "        # prepare the input into array of shape (N,2,W,H,3) where N is n_envs\n",
    "        states = np.asarray( [[s1, s2] for s1, s2 in zip(st1, st2)] )\n",
    "        #= np.asarray( [np.asarray([s1, s2]) for s1, s2 in zip(st1, st2)] )\n",
    "        \n",
    "        # preprocess_batch will convert two frames from\n",
    "        # each env, so (N, 2, W, H, 3), into shape (N, 2, 80, 80), \n",
    "        # as required by the input size for the pytorch CNN policy\n",
    "        states = preprocess_batch(states)                           # n states\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        \n",
    "        # probs will only be used as the pi_old..?\n",
    "        # no gradient propagation is needed so why not move it to the cpu..?\n",
    "        \n",
    "        ### ###### Turn off no_grad\n",
    "        #policy.eval()\n",
    "        #with torch.no_grad():\n",
    "        polprobs = policy(states)# why.squeeze()? #) # n polprobs <-- from policy_input (N,2,80,80)\n",
    "        #policy.train()\n",
    "        actions = torch.where(torch.rand(n).to(device) < polprobs, RIGHT, LEFT)       # n actions.to(device)\n",
    "        ### WHY THIS??\n",
    "        probs = torch.where(actions==RIGHT, polprobs, 1.0-polprobs)        # n probs\n",
    "        \n",
    "        # take one action and advance the game (0=no action)\n",
    "        # states = batch_input > > use, wait > > do .detach().cpu().numpy() here:\n",
    "        acts = actions.detach().cpu()\n",
    "        st1, re1, is_done, _ = envs.step(acts.numpy())                  # n acts: numpy cpu\n",
    "        st2, re2, is_done, _ = envs.step([0]*n)                         # n dones: bools\n",
    "        #rewards = re1 + re2                                             # n rewards: floats < Doubled here!\n",
    "        rewards = (re1 + 2*re2)/3  # <<< try what not doubling reward does!\n",
    "        # store the result                                              # n each of:\n",
    "        state_list.append(states)\n",
    "        action_list.append(actions)\n",
    "        reward_list.append(rewards)\n",
    "        polprob_list.append(polprobs)\n",
    "        prob_list.append(probs)\n",
    "        \n",
    "        # stop if any of the trajectories is done so all the lists are rectangular\n",
    "        if is_done.any():\n",
    "            break\n",
    "    \n",
    "    # Torch and move to cude before returning (except rewards still np cpu)\n",
    "    states = torch.stack(state_list).to(device)\n",
    "    actions = torch.stack(action_list).to(device)\n",
    "    polprobs = torch.stack(polprob_list).to(device)\n",
    "    probs = torch.stack(prob_list).to(device)\n",
    "    \n",
    "    # back to main training loop:\n",
    "    return  states, actions, reward_list, polprobs, probs\n",
    "\n",
    "# clipped surrogate function\n",
    "# similar as -policy_loss for REINFORCE, but for PPO\n",
    "# called from training \"SGD-loop\" with returns from one set of collect-Ts\n",
    "# returns a tensor of scaled and clipped  Ratio x Rewards\n",
    "def clipped_surrogate(policy, old_probs, \n",
    "                      states, actions, rewards,\n",
    "                      discount=0.995,\n",
    "                      epsilon=0.1, \n",
    "                      beta=0.01):\n",
    "    ''' Inputs expected to all on cuda device and torch'd, except for rewards. \n",
    "        Each (state/action/reward) tensor is N single steps, one step for each env\n",
    "        Returns (probability_ratio x expected_rewards) averaged across envs (tensor on cuda)\n",
    "    '''\n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "    # convert rewards to future rewards -- still numpys\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    #actions = torch.tensor(np.asarray(actions), dtype=torch.int8, device=device)\n",
    "    #old_probs = torch.tensor(np.asarray(old_probs), dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = states_to_probs(policy, states)\n",
    "    new_probs = torch.where(actions == RIGHT, new_probs, 1.0-new_probs)\n",
    "        \n",
    "    # ratio for clipping\n",
    "    ratio = new_probs/old_probs  # both tensors on cuda\n",
    "\n",
    "    # clipped function\n",
    "    clip = torch.clamp(ratio, min=ratio-epsilon, max=ratio+epsilon)\n",
    "    clipped_surrogate = torch.min(ratio*rewards, clip*rewards)\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    if old_probs.any()<0. or old_probs.any()>1.:\n",
    "        old_probs = old_probs/torch.max(old_probs, axis=0)\n",
    "    entropy = -( new_probs*torch.log(old_probs) + (1.0-new_probs)*torch.log(1.0-old_probs))\n",
    "\n",
    "    \n",
    "    # this returns an average of all the entries of the tensor\n",
    "    # effective computing L_sur^clip / T\n",
    "    # averaged over time-step and number of trajectories\n",
    "    # this is desirable because we have normalized our rewards\n",
    "    return torch.mean(clipped_surrogate + beta*entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to animate a list of frames\n",
    "def animate_frames(frames):\n",
    "    plt.axis('off')\n",
    "\n",
    "    # color option for plotting\n",
    "    # use Greys for greyscale\n",
    "    cmap = None if len(frames[0].shape)==3 else 'Greys'\n",
    "    patch = plt.imshow(frames[0], cmap=cmap)  \n",
    "\n",
    "    fanim = animation.FuncAnimation(plt.gcf(), \\\n",
    "                        lambda x: patch.set_data(frames[x]), frames = len(frames), interval=30)\n",
    "    \n",
    "    display(display_animation(fanim, default_mode='once'))\n",
    "    \n",
    "    \n",
    "# play a game and display the animation\n",
    "# nrand = number of random steps before using the policy\n",
    "def play(env, policy, time=2000, preprocess=None, nrand=5):\n",
    "    env.reset()\n",
    "\n",
    "    # start game\n",
    "    env.step(4)\n",
    "    \n",
    "    # perform nrand random steps in the beginning\n",
    "    for _ in range(nrand):\n",
    "        frame1, reward1, is_done, is_trunc, info = env.step(np.random.choice([RIGHT,LEFT]))\n",
    "        frame2, reward2, is_done, is_trunc, info = env.step(0)\n",
    "        is_done = is_done or is_trunc\n",
    "    \n",
    "    anim_frames = []\n",
    "    \n",
    "    for _ in range(time):\n",
    "        \n",
    "        frame_input = preprocess_batch(np.asarray([frame1, frame2]))\n",
    "        frame_input = torch.from_numpy(frame_input).float().to(device)\n",
    "        prob = policy(frame_input)\n",
    "        \n",
    "        # RIGHT = 4, LEFT = 5\n",
    "        action = RIGHT if random.random() < prob else LEFT\n",
    "        frame1, _, is_done, is_trunc, _ = env.step(action)\n",
    "        frame2, _, is_done, is_trunc, _ = env.step(0)\n",
    "        is_done = is_done or is_trunc\n",
    "\n",
    "        if preprocess is None:\n",
    "            anim_frames.append(frame1)\n",
    "        else:\n",
    "            anim_frames.append(preprocess(frame1))\n",
    "\n",
    "        if is_done:\n",
    "            break\n",
    "    \n",
    "    # ? env.close()\n",
    "    \n",
    "    animate_frames(anim_frames)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpACk9hYyZA6",
    "tags": []
   },
   "source": [
    "# Preprocessing\n",
    "To speed up training, we can simplify the input by cropping the images and use every other pixel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RzCwM5nUYeQY",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26bdff55c40>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs, info = env.reset()\n",
    "#env.step(1)\n",
    "plt.imshow(preprocess_single(obs))#, cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4-GMFDmryZA7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# show what a preprocessed image looks like\n",
    "obs, info = env.reset()\n",
    "_, _, _, _, _ = env.step(0)\n",
    "# get a frame after 20 steps\n",
    "for _ in range(20):\n",
    "    frame, _, _, _, _ = env.step(np.random.choice([0,4,5]))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('original image')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('preprocessed image')\n",
    "\n",
    "# 80 x 80 black and white image\n",
    "plt.imshow(preprocess_single(frame), cmap='Greys')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yutVg1-yZA7",
    "tags": []
   },
   "source": [
    "# Policy\n",
    "\n",
    "## Exercise 1: Implement your policy\n",
    " \n",
    "Here, we define our policy. The input is the stack of two different frames (which captures the movement), and the output is a number $P_{\\rm right}$, the probability of moving left. Note that $P_{\\rm left}= 1-P_{\\rm right}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cZ8Y5aadyZA7"
   },
   "outputs": [],
   "source": [
    "# set up a convolutional neural net\n",
    "# the output is the probability of moving right\n",
    "# P(left) = 1-P(right)\n",
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "    ########\n",
    "    ## \n",
    "    ## Modify your neural network\n",
    "    ##\n",
    "    ########\n",
    "        \n",
    "        # 80x80 to outputsize x outputsize\n",
    "        # outputsize = (inputsize - kernel_size + stride)/stride \n",
    "        # (round up if not an integer)\n",
    "\n",
    "        # output = 9x9 here\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=5, stride=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 16, kernel_size=4, stride=3, padding=1)\n",
    "        self.size=16*9*9\n",
    "        \n",
    "        # 1 fully connected layer\n",
    "        self.fc1 = nn.Linear(self.size, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "    # incoming size like: [478, 12, 2, 80, 80]\n",
    "    ########\n",
    "    ## \n",
    "    ## Modify your neural network\n",
    "    ##\n",
    "    ########\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # flatten the tensor\n",
    "        x = x.view(-1,self.size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.sig(self.fc2(x))\n",
    "    \n",
    "\n",
    "class Policy_Solution(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy_Solution, self).__init__()\n",
    "        # 80x80x2 to 38x38x4\n",
    "        # 2 channel from the stacked frame\n",
    "        self.conv1 = nn.Conv2d(2, 4, kernel_size=6, stride=2, bias=False)\n",
    "        # 38x38x4 to 9x9x32\n",
    "        self.conv2 = nn.Conv2d(4, 16, kernel_size=6, stride=4)\n",
    "        self.size=9*9*16\n",
    "        \n",
    "        # two fully connected layer\n",
    "        self.fc1 = nn.Linear(self.size, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "\n",
    "        # Sigmoid to \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1,self.size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.sig(self.fc2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MhkD58VYz_jh"
   },
   "outputs": [],
   "source": [
    "# use your own policy!\n",
    "policy=Policy().to(device)\n",
    "\n",
    "#import pong_utils\n",
    "#policy=pong_utils.Policy().to(device)\n",
    "\n",
    "# we use the adam optimizer with learning rate 2e-4\n",
    "# optim.SGD is also possible\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMuJlNZbyZA7"
   },
   "source": [
    "# Game visualization\n",
    "pong_utils contain a play function given the environment and a policy. An optional preprocess function can be supplied. Here we define a function that plays a game and shows learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2Za18MpsyZA7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#play(env, policy, time=100) \n",
    "# try to add the option \"preprocess=pong_utils.preprocess_single\"\n",
    "# to see what the agent sees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TOAWmC1lV-HD",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 1 done at step 451\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed=1234\n",
    "steps = 1800\n",
    "frame, _ = env.reset(seed=seed)\n",
    "frame = preprocess_single(frame)\n",
    "img = plt.imshow(frame)\n",
    "action =  np.random.choice([RIGHT, LEFT])   \n",
    "\n",
    "for t in range(steps):\n",
    "#try:\n",
    "    frame1, reward1, is_done, is_trunc, info = env.step(action)\n",
    "    if is_done or is_trunc:\n",
    "        print(\"Frame 1 done at step\", t)\n",
    "        break \n",
    "    frame2, reward2, is_done, is_trunc, info = env.step(0)\n",
    "#except:\n",
    "#    print(\"Env Exception\")\n",
    "#    break\n",
    "#finally:\n",
    "#    try:\n",
    "    frame = preprocess_batch(np.asarray([frame1, frame2]))\n",
    "    framesor = torch.from_numpy(frame).float().to(device)\n",
    "\n",
    "    # probs will only be used as the pi_old no gradient propagation is needed \"so we move it to the cpu\"\n",
    "    # where and where not to no_grad..?\n",
    "    policy.eval()\n",
    "    with torch.no_grad():\n",
    "        prob = policy(framesor).squeeze()#) # n polprobs\n",
    "    policy.train()\n",
    "\n",
    "    action = RIGHT if random.random() < prob else LEFT\n",
    "    frame = np.mean(frame.squeeze(), axis=0) #now shape is (80,80)\n",
    "#    except:\n",
    "#        print(\"Frame Exception\")\n",
    "#        break\n",
    "\n",
    "    #try:\n",
    "    img.set_data(frame)   #frame.detach().cpu().numpy()) \n",
    "    plt.axis('off')\n",
    "    display(plt.gcf())   #\n",
    "    display(clear=True)\n",
    "\n",
    "    if is_done or is_trunc:\n",
    "        print(\"Game over at step\", t)\n",
    "        break \n",
    "    #except:\n",
    "    #    print(\"Display Exception\")\n",
    "    #    break\n",
    "\n",
    "    #finally:\n",
    "    #    print(\"Finally\")\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UfGiK-IyZA8",
    "tags": []
   },
   "source": [
    "# Rollout  >>> `obs, rews, dones, infos = zip(*results)` <<<\n",
    "\n",
    "Before we start the training, we need to collect samples. To make things efficient we use parallelized environments to collect multiple examples at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "i2x_DGGGyZA8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Envs time:   4.289 sec\n",
      "Collection problem...\n",
      "Collection time:   0.462 sec\n",
      "N=12 Total time elapsed:   4.751 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "nenvs = 10\n",
    "envs = parallelEnv('PongDeterministic-v4', n=nenvs, seed=1234)\n",
    "envtime = time.time()\n",
    "print(\"Envs time: {:7.3f} sec\".format(envtime-start))\n",
    "\n",
    "try:\n",
    "    states, actions, reward_list, polprobs, probs = collect_trajectories(envs, policy, max_steps=1000, nrand=8)\n",
    "    #pass\n",
    "except:\n",
    "    print(\"Collection problem...\")\n",
    "    \n",
    "coltime = time.time()\n",
    "print(\"Collection time: {:7.3f} sec\".format(coltime-envtime))\n",
    "\n",
    "stop = time.time()\n",
    "print(\"N={} Total time elapsed: {:7.3f} sec\".format(nenvs, stop-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"N={} Time elapsed: {:7.3f} sec\".format(nenvs, stop-start))\n",
    "# N=30 Time elapsed:  23.397 sec\n",
    "# N=20 Time elapsed:  15.233 sec  17.732 sec\n",
    "# N=15 Time elapsed:  11.687 sec  23.888 sec\n",
    "# N=10 Time elapsed:   9.831 sec\n",
    "# N=5 Time elapsed:   7.880 sec\n",
    "# N=3 Time elapsed:   7.149 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "BrokenPipeError",
     "evalue": "[WinError 232] The pipe is being closed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRIGHT\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnenvs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#plt.imshow(frames[-1])\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\Repositories\\deep-reinforcement-learning\\pong-PPO-REINFORCE\\parallelEnv_X.py:84\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions):\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m    Step the environments synchronously.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    This is available for backwards compatibility.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_wait()\n",
      "File \u001b[1;32m~\\Documents\\Repositories\\deep-reinforcement-learning\\pong-PPO-REINFORCE\\parallelEnv_X.py:166\u001b[0m, in \u001b[0;36mparallelEnv.step_async\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions):\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m remote, action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremotes, actions):\n\u001b[1;32m--> 166\u001b[0m         \u001b[43mremote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstep\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\general\\lib\\multiprocessing\\connection.py:211\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_writable()\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\general\\lib\\multiprocessing\\connection.py:285\u001b[0m, in \u001b[0;36mPipeConnection._send_bytes\u001b[1;34m(self, buf)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_send_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, buf):\n\u001b[1;32m--> 285\u001b[0m     ov, err \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWriteFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlapped\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mERROR_IO_PENDING:\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [WinError 232] The pipe is being closed"
     ]
    }
   ],
   "source": [
    "envs.step([RIGHT]*nenvs)\n",
    "#plt.imshow(frames[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCVhvhANyZA8"
   },
   "source": [
    "# Function Definitions\n",
    "Here you will define key functions for training. \n",
    "\n",
    "## Exercise 2: write your own function for training\n",
    "(\"this is the same as policy_loss sort of\")\n",
    "\n",
    "### REINFORCE\n",
    "you have two choices (usually it's useful to divide by the time since we've normalized our rewards and the time of each trajectory is fixed)\n",
    "\n",
    "1. $\\frac{1}{T}\\sum^T_t R_{t}^{\\rm future}\\log(\\pi_{\\theta'}(a_t|s_t))$\n",
    "2. $\\frac{1}{T}\\sum^T_t R_{t}^{\\rm future}\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}$ where $\\theta'=\\theta$ and make sure that the no_grad is enabled when performing the division\n",
    "\n",
    "### PPO\n",
    "Later on, you'll implement the PPO algorithm as well, and the scalar function is given by\n",
    "$\\frac{1}{T}\\sum^T_t \\min\\left\\{R_{t}^{\\rm future}\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)},R_{t}^{\\rm future}{\\rm clip}_{\\epsilon}\\!\\left(\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}\\right)\\right\\}$\n",
    "\n",
    "the ${\\rm clip}_\\epsilon$ function is implemented in pytorch as ```torch.clamp(ratio, 1-epsilon, 1+epsilon)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(envs\u001b[38;5;241m.\u001b[39mps)\n\u001b[1;32m----> 2\u001b[0m \u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# start all parallel agents\u001b[39;00m\n\u001b[0;32m      5\u001b[0m _, _, _, _ \u001b[38;5;241m=\u001b[39m envs\u001b[38;5;241m.\u001b[39mstep([RIGHT]\u001b[38;5;241m*\u001b[39mn)\n",
      "File \u001b[1;32m~\\Documents\\Repositories\\deep-reinforcement-learning\\pong-PPO-REINFORCE\\parallelEnv.py:170\u001b[0m, in \u001b[0;36mparallelEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m remote \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremotes:\n\u001b[0;32m    169\u001b[0m     remote\u001b[38;5;241m.\u001b[39msend((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreset\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mremote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mremote\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremotes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\general\\lib\\site-packages\\numpy\\core\\shape_base.py:458\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m overrides\u001b[38;5;241m.\u001b[39mARRAY_FUNCTION_ENABLED:\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;66;03m# raise warning if necessary\u001b[39;00m\n\u001b[0;32m    456\u001b[0m     _arrays_for_stack_dispatcher(arrays, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 458\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [asanyarray(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed at least one array to stack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\general\\lib\\site-packages\\numpy\\core\\shape_base.py:458\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m overrides\u001b[38;5;241m.\u001b[39mARRAY_FUNCTION_ENABLED:\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;66;03m# raise warning if necessary\u001b[39;00m\n\u001b[0;32m    456\u001b[0m     _arrays_for_stack_dispatcher(arrays, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 458\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [\u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed at least one array to stack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "n=len(envs.ps)\n",
    "envs.reset()\n",
    "    \n",
    "# start all parallel agents\n",
    "_, _, _, _ = envs.step([RIGHT]*n)\n",
    "\n",
    "# perform nrand random steps\n",
    "for _ in range(5):\n",
    "    st1, re1, _, _ = envs.step(np.random.choice([RIGHT, LEFT, 0], n))\n",
    "    st2, re2, _, _ = envs.step([0]*n)                              # n (s1, s2)\n",
    "\n",
    "#for t in range(tmax):\n",
    "\n",
    "    # prepare the input first as array of shape (N,2,W,H,3)\n",
    "states = np.asarray( [[s1, s2] for s1, s2 in zip(st1, st2)] )    ###  np.asarray([s1, s2])\n",
    "states.shape, preprocess_batch(states).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each pair of two states/images: (2, 210, 160, 3)\n",
      "input_batch: (4, 2, 210, 160, 3) 2 rgb images each from N envs\n",
      "array_of_images: (4, 2, 210, 160, 3)  >>> tensor >>> policy fxn nn >>> action_probs\n",
      "skip/crop: (4, 2, 80, 80, 3) only every other pixel, ignore top and bottom\n",
      "bkg_color: (3,) to be subtracted from the 3 color channels in last dim\n",
      "minus bkg: (4, 2, 80, 80, 3) >>> averaged over the 3 r, g & b channels in last dim >>>\n",
      "mean and out: (4, 2, 80, 80) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 2, 80, 80)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "bkg_color = np.array([144, 72, 17])\n",
    "input_batch = []\n",
    "env.reset()\n",
    "for _ in range(11):\n",
    "    frame1, reward1, is_done, is_trunc, info = env.step(np.random.choice([RIGHT,LEFT,0]))\n",
    "    frame2, reward2, is_done, is_trunc, info = env.step(0)\n",
    "    if is_done or is_trunc: break\n",
    "for _ in range(4):\n",
    "    frame1, reward1, is_done, is_trunc, info = env.step(np.random.choice([RIGHT,LEFT,0]))\n",
    "    frame2, reward2, is_done, is_trunc, info = env.step(0)\n",
    "    input_batch.append(np.asarray([frame1, frame2]))\n",
    "    if is_done or is_trunc: \n",
    "        break\n",
    "\n",
    "print(\"Each pair of two states/images:\", input_batch[0].shape)\n",
    "#print(\"image input: (2, {}, {}, {}\".format(*frame1.shape))\n",
    "#list_of_images = np.asarray([frame1, frame2])\n",
    "new_images = np.asarray(input_batch)#[0]\n",
    "print(\"input_batch:\", new_images.shape, \"2 rgb images each from N envs\")\n",
    "if len(new_images.shape) < 5:\n",
    "    new_images = np.expand_dims(new_images, 0)\n",
    "    print(\"expand input:\", new_images.shape, \"2 rgb images from 1 env\")\n",
    "print(\"array_of_images:\", new_images.shape,\" >>> tensor >>> policy fxn nn >>> action_probs\")\n",
    "list_of_images_skip = new_images[:, :, 34:-16:2, ::2]\n",
    "print(\"skip/crop:\", list_of_images_skip.shape, \"only every other pixel, ignore top and bottom\")\n",
    "print(\"bkg_color:\", bkg_color.shape, \"to be subtracted from the 3 color channels in last dim\")\n",
    "list_of_images_minus = list_of_images_skip - bkg_color\n",
    "print(\"minus bkg:\", list_of_images_minus.shape, \">>> averaged over the 3 r, g & b channels in last dim >>>\")\n",
    "list_of_images_mean = np.mean(list_of_images_minus, axis=-1)/255.\n",
    "print(\"mean and out:\", list_of_images_mean.shape, \"\")\n",
    "#batch_input = np.swapaxes(list_of_images_mean,0,1)\n",
    "#print(\"swap and out:\", batch_input.shape)\n",
    "preprocess_batch(np.asarray(input_batch)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\general\\lib\\multiprocessing\\connection.py:317\u001b[0m, in \u001b[0;36mPipeConnection._recv_bytes\u001b[1;34m(self, maxsize)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m     nread, err \u001b[38;5;241m=\u001b[39m \u001b[43mov\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetOverlappedResult\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [WinError 109] The pipe has been ended",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m states, actions, rewards, polprobs, old_probs \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m\"\u001b[39m, states\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m\"\u001b[39m, actions\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrewards np(\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(rewards),\u001b[38;5;241m*\u001b[39mrewards[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mold_probs\u001b[39m\u001b[38;5;124m\"\u001b[39m, old_probs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m new_probs \u001b[38;5;241m=\u001b[39m states_to_probs(policy, states)\n",
      "Cell \u001b[1;32mIn[15], line 51\u001b[0m, in \u001b[0;36mcollect_trajectories\u001b[1;34m(envs, policy, max_steps, nrand)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# take one action and advance the game (0=no action)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# states = batch_input > > use, wait > > do .detach().cpu().numpy() here:\u001b[39;00m\n\u001b[0;32m     50\u001b[0m acts \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m---> 51\u001b[0m st1, re1, is_done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43macts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m                  \u001b[38;5;66;03m# n acts: numpy cpu\u001b[39;00m\n\u001b[0;32m     52\u001b[0m st2, re2, is_done, _ \u001b[38;5;241m=\u001b[39m envs\u001b[38;5;241m.\u001b[39mstep([\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39mn)                         \u001b[38;5;66;03m# n dones: bools\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m#rewards = re1 + re2                                             # n rewards: floats < Doubled here!\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\Repositories\\deep-reinforcement-learning\\pong-PPO-REINFORCE\\parallelEnv_X.py:85\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03mStep the environments synchronously.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03mThis is available for backwards compatibility.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Repositories\\deep-reinforcement-learning\\pong-PPO-REINFORCE\\parallelEnv_X.py:164\u001b[0m, in \u001b[0;36mparallelEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 164\u001b[0m     results \u001b[38;5;241m=\u001b[39m [remote\u001b[38;5;241m.\u001b[39mrecv() \u001b[38;5;28;01mfor\u001b[39;00m remote \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremotes]\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     obs, rews, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)\n",
      "File \u001b[1;32m~\\Documents\\Repositories\\deep-reinforcement-learning\\pong-PPO-REINFORCE\\parallelEnv_X.py:164\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 164\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\u001b[43mremote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m remote \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremotes]\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     obs, rews, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\general\\lib\\multiprocessing\\connection.py:255\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[1;32m--> 255\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\general\\lib\\multiprocessing\\connection.py:326\u001b[0m, in \u001b[0;36mPipeConnection._recv_bytes\u001b[1;34m(self, maxsize)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwinerror \u001b[38;5;241m==\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mERROR_BROKEN_PIPE:\n\u001b[1;32m--> 326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mEOFError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "states, actions, rewards, polprobs, old_probs = collect_trajectories(envs, policy, max_steps=50)\n",
    "print(\"states\", states.shape, \"actions\", actions.shape, \"rewards np({},{})\".format(len(rewards),*rewards[0].shape), \"old_probs\", old_probs.shape)\n",
    "new_probs = states_to_probs(policy, states)\n",
    "print(\"new_probs\", new_probs.shape)\n",
    "new_probs = torch.where(actions == RIGHT, new_probs, 1.0-new_probs)\n",
    "print(\"avg(axis=1)\", torch.mean(new_probs, axis=1).shape, \"avg(axis=0)\", torch.mean(new_probs, axis=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# return sum of log-prob divided by T\n",
    "# same thing as -policy_loss\n",
    "def surrogate(policy, old_probs, states, actions, rewards,\n",
    "              discount = 0.995, beta=0.01):\n",
    "    pass\n",
    "\n",
    "# clipped surrogate function\n",
    "# similar as -policy_loss for REINFORCE, but for PPO\n",
    "def the_other_clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "                      discount=0.995,\n",
    "                      epsilon=0.1, \n",
    "                      beta=0.01):\n",
    "\n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "    # convert rewards to future rewards\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == RIGHT, new_probs, 1.0-new_probs)\n",
    "    \n",
    "    # ratio for clipping\n",
    "    ratio = new_probs/old_probs\n",
    "\n",
    "    # clipped function\n",
    "    clip = torch.clamp(ratio, ratio-epsilon, ratio+epsilon)\n",
    "    clipped_surrogate = torch.min(ratio*rewards, clip*rewards)\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "               (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    # this returns an average of all the entries of the tensor\n",
    "    # effective computing L_sur^clip / T\n",
    "    # averaged over time-step and number of trajectories\n",
    "    # this is desirable because we have normalized our rewards\n",
    "    return torch.mean(clipped_surrogate + beta*entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHN9oeSnyZA8"
   },
   "source": [
    "# Training\n",
    "We are now ready to train our policy!\n",
    "WARNING: make sure to turn on GPU, which also enables multicore processing. It may take up to 45 minutes even with GPU enabled, otherwise it will take much longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "S5QGfS02p-XB",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                          | ETA:  --:--:--\r"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[WinError 232] The pipe is being closed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 27\u001b[0m\n\u001b[0;32m     22\u001b[0m mean_rewards \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# collect trajectories -- lists of tensors on cuda\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     states, actions, rewards, polprobs, old_probs \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmax\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# rewards and total_rewards are the only numpys    \u001b[39;00m\n\u001b[0;32m     29\u001b[0m     total_rewards \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(rewards, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[48], line 14\u001b[0m, in \u001b[0;36mcollect_trajectories\u001b[1;34m(envs, policy, max_steps, nrand)\u001b[0m\n\u001b[0;32m     11\u001b[0m polprob_list\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     12\u001b[0m action_list\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m---> 14\u001b[0m \u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# start all parallel agents\u001b[39;00m\n\u001b[0;32m     17\u001b[0m _, _, _, _ \u001b[38;5;241m=\u001b[39m envs\u001b[38;5;241m.\u001b[39mstep([RIGHT]\u001b[38;5;241m*\u001b[39mn)\n",
      "File \u001b[1;32m~\\Documents\\Repositories\\deep-reinforcement-learning\\pong-PPO-REINFORCE\\parallelEnv_X.py:171\u001b[0m, in \u001b[0;36mparallelEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m remote \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremotes:\n\u001b[1;32m--> 171\u001b[0m         \u001b[43mremote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack([remote\u001b[38;5;241m.\u001b[39mrecv() \u001b[38;5;28;01mfor\u001b[39;00m remote \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremotes])\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\general\\lib\\multiprocessing\\connection.py:211\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_writable()\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\general\\lib\\multiprocessing\\connection.py:285\u001b[0m, in \u001b[0;36mPipeConnection._send_bytes\u001b[1;34m(self, buf)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_send_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, buf):\n\u001b[1;32m--> 285\u001b[0m     ov, err \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWriteFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlapped\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mERROR_IO_PENDING:\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [WinError 232] The pipe is being closed"
     ]
    }
   ],
   "source": [
    "##################### PPO ###############################\n",
    "#### WARNING: running through all 800 episodes will take 30-45 minutes\n",
    "\n",
    "# training loop max iterations\n",
    "episodes = 100\n",
    "# episodes = 800\n",
    "\n",
    "# widget bar to display progress\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episodes).start()\n",
    "\n",
    "#envs = parallelEnv('PongDeterministic-v4', n=20, seed=1234)\n",
    "\n",
    "discount_rate = .99\n",
    "epsilon = 0.15\n",
    "beta = .008\n",
    "tmax = 800\n",
    "SGD_epoch = 5\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episodes):\n",
    "\n",
    "    # collect trajectories -- lists of tensors on cuda\n",
    "    states, actions, rewards, polprobs, old_probs = collect_trajectories(envs, policy, max_steps=tmax)\n",
    "    # rewards and total_rewards are the only numpys    \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "    # gradient ascent step -- few enough to get up-to-date response from nn\n",
    "    for _ in range(SGD_epoch):\n",
    "        \n",
    "        # utilize your own clipped function!\n",
    "        L = -clipped_surrogate(policy, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L             # !oh no!\n",
    "    \n",
    "    # the clipping parameter reduces as episodes are collected\n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces with use\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.998\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1, np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcifaOgcyZA9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# play game after training!\n",
    "#play(env, policy, time=100) \n",
    "polprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gm3n6N4byZA9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(mean_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_aALEueyZA9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save your policy!\n",
    "#torch.save(policy, 'data/in_training_PPO.policy')\n",
    "#torch.save(policy, 'data/trained_PPO.policy')\n",
    "\n",
    "# load your policy if needed\n",
    "#policy = torch.load('data/PPO_high.policy')\n",
    "\n",
    "# try and test out the solution!\n",
    "#policy = torch.load('data/PPO_OG.policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "TOAWmC1lV-HD",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game over at step 395\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFrklEQVR4nO3cwYkTYRiA4VlJF4GcPNiAJ0G3AM9iAQpiAVuEFQhagORsAVHwZAOeBa0iON5ecBEmrklmR5/ntvATvtO8fPz8ezGO4zgAwDAMd+YeAIDbQxQAiCgAEFEAIKIAQEQBgIgCABEFALI69ODDx69OOQcAJ/bx/dXkGZsCABEFACIKAEQUAMjBF80A3F67t29++fvy2fMb/Y5NAYCIAgARBQCy6DuFr09+TJ7ZbHUP4FC+mABEFACIKAAQUQAgi75ofvfo9eSZq+3LM0wCcD7XH6oNw80fq11nUwAgogBARAGAiAIAEQUAIgoARBQAiCgAkEU/XgP4Hx3rodrv2BQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAWc09wN94+uHF5JnNGeYA+FfYFACIKAAQUQAgogBAFn3RvNlqGsAx+aoCEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKu5BwBYmm+X05/O9W5/hkmOz6YAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAyGruAQCW5v6DL5Nnvu/unmGS47MpABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIA8V9SAf7Q50/3Js+sh/0ZJjk+mwIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgq7kHAFia9W4/9wgnY1MAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFADIxTiO49xDAHA72BQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAMhPBacoSiFFyq8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed=1234\n",
    "steps = 1800\n",
    "frame, _ = env.reset(seed=seed)\n",
    "frame = preprocess_single(frame)\n",
    "img = plt.imshow(frame)\n",
    "action =  np.random.choice([RIGHT, LEFT])   \n",
    "\n",
    "for t in range(steps):\n",
    "    frame1, reward1, is_done, is_trunc, info = env.step(action)\n",
    "    if is_done or is_trunc:\n",
    "        print(\"Frame 1 done at step\", t)\n",
    "        break \n",
    "    frame2, reward2, is_done, is_trunc, info = env.step(0)\n",
    "\n",
    "    frame = preprocess_batch(np.asarray([frame1, frame2]))\n",
    "    framesor = torch.from_numpy(frame).float().to(device)\n",
    "\n",
    "    # probs will only be used as the pi_old no gradient propagation is needed \"so we move it to the cpu\"\n",
    "    # where and where not to no_grad..?\n",
    "    policy.eval()\n",
    "    with torch.no_grad():\n",
    "        prob = policy(framesor).squeeze()#) # n polprobs\n",
    "    policy.train()\n",
    "\n",
    "    action = RIGHT if random.random() < prob else LEFT\n",
    "    frame = np.mean(frame.squeeze(), axis=0) #now shape is (80,80)\n",
    "\n",
    "    img.set_data(frame)   #frame.detach().cpu().numpy()) \n",
    "    plt.axis('off')\n",
    "    display(plt.gcf())   #\n",
    "    display(clear=True)\n",
    "\n",
    "    if is_done or is_trunc:\n",
    "        print(\"Game over at step\", t)\n",
    "        break \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1234\n",
    "steps = 1800\n",
    "env = gym.make('PongDeterministic-v4', render_mode='rgb_array')\n",
    "frame, _ = env.reset()#seed=seed)\n",
    "#print(frames.shape)\n",
    "frame = preprocess_single(frame)\n",
    "#print(frame.shape)\n",
    "img = plt.imshow(frame)\n",
    "action =  np.random.choice([RIGHT, LEFT])   \n",
    "for t in range(steps):\n",
    "    frame1, reward1, is_done, is_trunc, info = env.step(action)\n",
    "    frame2, reward2, is_done, is_trunc, info = env.step(0)\n",
    "    is_done = is_done or is_trunc\n",
    "    \n",
    "    frame = preprocess_batch(np.asarray([frame1//2, frame2]))\n",
    "    framesor = torch.from_numpy(frame).float().to(device)\n",
    "        \n",
    "    # probs will only be used as the pi_old no gradient propagation is needed so we move it to the cpu\n",
    "    policy.eval()\n",
    "    with torch.no_grad():\n",
    "        prob = policy(framesor).squeeze()#) # n polprobs\n",
    "    policy.train()\n",
    "    \n",
    "    action = RIGHT if random.random() < prob else LEFT\n",
    "\n",
    "    frame = np.mean(frame.squeeze(), axis=0) #now shape is (80,80)\n",
    "    img.set_data(frame)   #frame.detach().cpu().numpy()) \n",
    "    plt.axis('off')\n",
    "    display(plt.gcf())   #\n",
    "    display(clear=True)\n",
    "\n",
    "    if is_done:\n",
    "        print(\"Game over at step\", t)\n",
    "        break "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "BKH0-yBwyZA4",
    "wpACk9hYyZA6",
    "7yutVg1-yZA7",
    "yCVhvhANyZA8"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc-autonumbering": true,
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
