{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b058ADVF-G_"
      },
      "source": [
        "# Deep Deterministic Policy Gradients (DDPG)\n",
        "---\n",
        "In this notebook, we train DDPG with OpenAI Gym's BipedalWalker-v2 environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import the Necessary Packages"
      ],
      "metadata": {
        "id": "9LaI2vyuogCz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OotjxPANF-HB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f51a38-d57e-4223-d29e-1dba75854099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.8/dist-packages (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[mujoco]) (1.21.6)\n",
            "Requirement already satisfied: gymnasium-notices>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from gymnasium[mujoco]) (0.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[mujoco]) (6.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[mujoco]) (4.4.0)\n",
            "Requirement already satisfied: jax-jumpy>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[mujoco]) (0.2.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium[mujoco]) (2.2.1)\n",
            "Requirement already satisfied: mujoco>=2.3.1.post1 in /usr/local/lib/python3.8/dist-packages (from gymnasium[mujoco]) (2.3.2)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.8/dist-packages (from gymnasium[mujoco]) (2.25.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.8/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (9.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gymnasium[mujoco]) (3.12.1)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.8/dist-packages (from mujoco>=2.3.1.post1->gymnasium[mujoco]) (3.1.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from mujoco>=2.3.1.post1->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.8/dist-packages (from mujoco>=2.3.1.post1->gymnasium[mujoco]) (2.5.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[mujoco]\n",
        "#[box2d]\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display\n",
        "\n",
        "#from ddpg_agent_reacher_X import Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Models"
      ],
      "metadata": {
        "id": "j4dJ8lJ4nuxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def hidden_init(layer):\n",
        "    fan_in = layer.weight.data.size()[0]\n",
        "    lim = 1. / np.sqrt(fan_in)\n",
        "    return (-lim, lim)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    \"\"\"Actor (Policy) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=400, fc2_units=300):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fc1_units (int): Number of nodes in first hidden layer\n",
        "            fc2_units (int): Number of nodes in second hidden layer\n",
        "        \"\"\"\n",
        "        super(Actor, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
        "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
        "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return torch.tanh(self.fc3(x))\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    \"\"\"Critic (Value) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, fcs1_units=400, fc2_units=300):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fcs1_units (int): Number of nodes in the first hidden layer\n",
        "            fc2_units (int): Number of nodes in the second hidden layer\n",
        "        \"\"\"\n",
        "        super(Critic, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
        "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, 1)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
        "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
        "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
        "        xs = F.relu(self.fcs1(state))\n",
        "        x = torch.cat((xs, action), dim=1)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "iRlJRiFKnyDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Agent"
      ],
      "metadata": {
        "id": "Aix1NrPCoOJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "#.cpu().data.numpy()\n",
        "\n",
        "BUFFER_SIZE = int(6.4e5)  # replay buffer size = int(1e6)\n",
        "BATCH_SIZE = 64        # minibatch size = 128\n",
        "GAMMA = 0.999          # discount factor = 0.99\n",
        "TAU = 3e-3              # for soft update of target parameters = 1e-3\n",
        "LR_ACTOR = 1e-4         # learning rate of the actor  = 1e-4\n",
        "LR_CRITIC = 1e-3        # learning rate of the critic = 3e-4\n",
        "WEIGHT_DECAY = 1e-6     # L2 weight decay = 0.0001 to 0.\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Agent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_size, action_size, random_seed):#, state_highs):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "        \n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            random_seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(random_seed)\n",
        "\n",
        "        # Actor Network (w/ Target Network)\n",
        "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
        "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
        "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
        "\n",
        "        # Critic Network (w/ Target Network)\n",
        "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
        "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
        "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, \n",
        "                                           weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "        # Noise process\n",
        "        self.noise = OUNoise(action_size, random_seed)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = PriorityReplay(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)#, state_highs)\n",
        "    \n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
        "        # Save experience / reward\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # Learn, if enough samples are available in memory\n",
        "        if len(self.memory) > BATCH_SIZE:\n",
        "            experiences, batch_ids = self.memory.sample()\n",
        "            priority = self.learn(experiences, GAMMA)\n",
        "            priority = [p[0] for p in priority]\n",
        "            self.memory.update_priorities(batch_ids, priority)\n",
        "\n",
        "    def act(self, state, add_noise=False):\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "        state = torch.from_numpy(state).float().to(device)\n",
        "        ### Using LOCAL actor ###\n",
        "        self.actor_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action = self.actor_local(state).cpu().data.numpy()\n",
        "        self.actor_local.train()\n",
        "        if add_noise:\n",
        "            #action += self.noise.sample() * 0.75 # fraction bc clipping alot\n",
        "            pass\n",
        "        return np.clip(action, -1, 1)\n",
        "\n",
        "    def reset(self):\n",
        "        self.noise.reset()\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
        "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
        "        where:\n",
        "            actor_target(state) -> action\n",
        "            critic_target(state, action) -> Q-value\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # ---------------------------- update critic ---------------------------- #\n",
        "        # Get predicted next-state actions and Q values from target models\n",
        "        actions_next = self.actor_target(next_states)\n",
        "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
        "        # Compute Q targets for current states (y_i)\n",
        "        ####### TO DO What to do about the dones? \n",
        "        ####### Log of all rewards? Pre-scale?\n",
        "        Q_targets = (rewards + gamma * Q_targets_next)# * ((1 - dones) + dones/100.)\n",
        "        # Compute critic loss\n",
        "        Q_expected = self.critic_local(states, actions)\n",
        "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        # Minimize critic loss\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "        \n",
        "        # Compute *critic* priority == *only* critic experience loss??\n",
        "        critic_priority = torch.abs(Q_expected - Q_targets) + 0.00001\n",
        "\n",
        "        # ---------------------------- update actor ---------------------------- #\n",
        "        # Compute actor loss\n",
        "        actions_pred = self.actor_local(states)\n",
        "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
        "        # Minimize actor loss\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        ####### TO DO critic_priority * actor_priority...? \n",
        "        actor_priority = torch.abs(self.critic_local(states, actions_pred).mean()) + 0.00001\n",
        "\n",
        "        # ------------------- soft update target networks ----------------------- #\n",
        "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
        "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
        "\n",
        "        #priority = (actor_priority + critic_priority) / 2.\n",
        "        return critic_priority.tolist()\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            local_model: PyTorch model (weights will be copied from)\n",
        "            target_model: PyTorch model (weights will be copied to)\n",
        "            tau (float): interpolation parameter \n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
        "\n",
        "class OUNoise:\n",
        "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
        "\n",
        "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
        "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
        "        self.mu = mu * np.ones(size)\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.seed = random.seed(seed)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
        "        self.state = copy.copy(self.mu)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
        "        x = self.state\n",
        "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
        "        self.state = x + dx\n",
        "        return self.state\n",
        "\n",
        "class PriorityReplay:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):#, highs):\n",
        "        \"\"\"Initialize a PriorityReplay object.\n",
        "        Params\n",
        "        ======\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
        "        self.max_priority = 1.1\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \n",
        "                                                                \"action\", \n",
        "                                                                \"reward\", \n",
        "                                                                \"next_state\", \n",
        "                                                                \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "        #self.state_highs = highs\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        # rescale states by states.high\n",
        "        #state = [s/h for s,h in zip(state, self.state_highs)]\n",
        "        #next_state = [s/h for s,h in zip(next_state, self.state_highs)]\n",
        "        init_priority = np.random.random()*self.max_priority\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append([init_priority, e])\n",
        "\n",
        "    def update_priorities(self, batch_ids, new_priority):\n",
        "        if max(new_priority) > self.max_priority:\n",
        "            self.max_priority = max(new_priority)\n",
        "            print(\"\\n>>>>>>>> Max priority =\", self.max_priority,\"<<<<<<<<\")\n",
        "            print(\">>>>>>>> Min priority =\", min(new_priority),\"<<<<<<<<\")\n",
        "            print(\">>>>>>>> Avg priority =\", sum(new_priority)/len(new_priority),\"<<<<<<<<\")\n",
        "        for id, new_p in zip(batch_ids, new_priority):\n",
        "            self.memory[id][0] = 0.3*new_p + 0.7*self.memory[id][0]\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        exp_ids = list(range(len(self.memory)))\n",
        "        priority = [m[0] for m in self.memory]#/ self.max_priority\n",
        "        sum_priority = sum(priority)#sum(norm_priority)\n",
        "        probs = [p / sum_priority for p in priority]\n",
        "        batch_ids = np.random.choice(exp_ids, size=self.batch_size, replace=False, p=probs)\n",
        "        experiences = [self.memory[id][1] for id in batch_ids]\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "        \n",
        "        return (states, actions, rewards, next_states, dones), batch_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "oIxNGHrQoXXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(state_size=state_size, action_size=action_size, random_seed=seed, state_highs=HIGHS)"
      ],
      "metadata": {
        "id": "6NNwYG-54QW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.memory.sample()"
      ],
      "metadata": {
        "id": "fWHyKUh7oGro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores, high_score = ddpg(n_episodes=100, max_t=1600, max_score=-10000)"
      ],
      "metadata": {
        "id": "9TahPD4Y5GSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "buffer = agent.memory.memory\n",
        "priorities = [b[0] for b in buffer]\n",
        "experiences = [b[1] for b in buffer]\n",
        "#[p for p in priorities if p>3]"
      ],
      "metadata": {
        "id": "TlH9zs5CE0Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[(e.reward, p) for e, p in zip(experiences, priorities) if not e.done and p<10 and p>5]"
      ],
      "metadata": {
        "id": "pne8lBlHN6H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[(e.reward, p) for e, p in zip(experiences, priorities) if e.done and p<10 and p>5]"
      ],
      "metadata": {
        "id": "V4Tuw0OyQpGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[e for e in experiences if e.done]"
      ],
      "metadata": {
        "id": "Fw7pT-h9UF8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L21oAff8F-HC"
      },
      "source": [
        "## 2. Instantiate the Environment and Agent\n",
        "\n",
        "**Reacher Rewards**\n",
        "\n",
        "The reward consists of two parts:\n",
        "* *reward_distance*: This reward is a measure of how far the fingertip of the reacher (the unattached end) is from the target, with a more negative value assigned for when the reacher’s fingertip is further away from the target. It is calculated as the negative vector norm of (position of the fingertip - position of target), or `-norm(“fingertip” - “target”)`.\n",
        "* *reward_control*: A negative reward for penalising the reacher if it takes actions that are too large. It is measured as the negative squared Euclidean norm of the action, i.e. as `-sum(action**2)`.\n",
        "\n",
        "The total reward returned is `reward = reward_distance + reward_control`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BVpbenjF-HC"
      },
      "outputs": [],
      "source": [
        "from ddpg_agent_reacher_X import Agent\n",
        "\n",
        "#env = gym.make('BipedalWalker-v3', render_mode=\"rgb_array\")\n",
        "env = gym.make('Reacher-v4', render_mode=\"rgb_array\")\n",
        "seed=0\n",
        "state_size=env.observation_space.shape[0]\n",
        "action_size=env.action_space.shape[0]\n",
        "#HIGHS=env.observation_space.high\n",
        "agent = Agent(state_size=state_size, action_size=action_size, random_seed=seed)#, state_highs=HIGHS)\n",
        "print(state_size, action_size)#, \"\\n\", HIGHS)\n",
        "\n",
        "### Uncomment to run saved agents of CPU\n",
        "#agent.actor_local.load_state_dict(torch.load('highscore_actorA_bip.pth', map_location=torch.device('cpu')))\n",
        "#agent.critic_local.load_state_dict(torch.load('highscore_criticA_bip.pth', map_location=torch.device('cpu')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVzlW7asF-HD"
      },
      "source": [
        "## 3. Train the Agent with DDPG\n",
        "\n",
        "Run the code cell below to train the agent from scratch.  Alternatively, you can skip to the next code cell to load the pre-trained weights from file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRWlT6LhF-HD"
      },
      "outputs": [],
      "source": [
        "def ddpg(n_episodes=2000, max_t=1600, max_score=-10000.):\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    #max_score = -10000 #-np.Inf\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state, info = env.reset(seed=seed)\n",
        "        agent.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, trun, info = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done or trun)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done or trun:\n",
        "                if done: print(\"\\nDone. Step: {}\\t Reward: {}\".format(t, reward))\n",
        "                else: print(\"\\nTruncated. Step: {}\\t Reward: {}\".format(t, reward))\n",
        "                break \n",
        "        scores_deque.append(score)\n",
        "        scores.append(score)\n",
        "        if score >= max_score + 15:\n",
        "            torch.save(agent.actor_local.state_dict(), 'highscore_actor_bip.pth')\n",
        "            torch.save(agent.critic_local.state_dict(), 'highscore_critic_bip.pth')\n",
        "            print('\\rEpisode {}\\tNEW HIGH SCORE! {:.2f}'.format(i_episode,score))\n",
        "            max_score = score           \n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, np.mean(scores_deque), score), end=\"\")\n",
        "        if i_episode % 100 == 0:\n",
        "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor_bip.pth')\n",
        "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic_bip.pth')\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tHigh Score: {:.2f}'.format(i_episode, np.mean(scores_deque), max_score))   \n",
        "    return scores, max_score \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#scores = ddpg(n_episodes=2000, max_t=800)\n",
        "# ~23 min for N=1000 T=500\n",
        "scores = []; new_scores = []; high_score = -10000.\n",
        "n_episodes=[400, 400, 400]#, 300]#, 600,  300, 300, 600]\n",
        "max_t=[100, 1200]#, 200, 600]#, 1200, 200, 400, 800]\n",
        "n_episodes=[ne//4 for ne in n_episodes]\n",
        "\n",
        "for ne, mt in zip(n_episodes, max_t):\n",
        "    mt=1600\n",
        "    print('\\r### Episodes: {}\\tTime Limit: {:.2f} ###'.format(ne,mt))\n",
        "    new_scores, high_score = ddpg(n_episodes=ne, max_score=high_score)#, max_t\n",
        "    scores += new_scores\n"
      ],
      "metadata": {
        "id": "13qXAsmAqx0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_scores = []\n",
        "new_scores, high_score = ddpg(n_episodes=128, max_t=1200, max_score=high_score)#, max_score=-10000), experiences\n",
        "scores += new_scores"
      ],
      "metadata": {
        "id": "DvwxP3d1urg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(agent.actor_local.state_dict(), 'checkpoint_actorB_bip.pth')\n",
        "torch.save(agent.critic_local.state_dict(), 'checkpoint_criticB_bip.pth')\n",
        "files.download('checkpoint_criticB_bip.pth')\n",
        "files.download('checkpoint_actorB_bip.pth')"
      ],
      "metadata": {
        "id": "hlLJLwroNdzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Scores plot\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(1, len(scores)+1), scores)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bu-bNkXYWwJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq-Y0saUF-HE"
      },
      "source": [
        "## 4. Observe\n",
        "\n",
        "In the next code cell, you will load the trained weights from file to watch a smart agent!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.actor_local.load_state_dict(torch.load('highscore_actorA_bip.pth'))\n",
        "agent.critic_local.load_state_dict(torch.load('highscore_criticA_bip.pth'))"
      ],
      "metadata": {
        "id": "TMEOLXYbVIOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuWpi8dcsz5G"
      },
      "outputs": [],
      "source": [
        "# load the weights from file\n",
        "#agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
        "#frames = deque(maxlen=300)\n",
        "frames = [] #np.zeros(300,)\n",
        "n_steps = 300\n",
        "\n",
        "for i in range(1):\n",
        "    state, info = env.reset(seed=seed)\n",
        "    frames.append(env.render())\n",
        "    #img = plt.imshow(env.render())    \n",
        "    for j in range(n_steps):\n",
        "        action = agent.act(state)\n",
        "        state, reward, done, trun, info = env.step(action)\n",
        "        frames.append(env.render())\n",
        "        #img.set_data(env.render()) \n",
        "        #plt.axis('off')\n",
        "        #display.display(plt.gcf())\n",
        "        #display.clear_output(wait=True)\n",
        "        if done or trun:\n",
        "            break \n",
        "frames = np.asarray(frames)\n",
        "print(frames.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = plt.imshow(frames[0])\n",
        "for f in frames[1:]:\n",
        "    img.set_data(f) \n",
        "    plt.axis('off')\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "\n",
        "\n",
        "    \n",
        "#env.close()"
      ],
      "metadata": {
        "id": "E0Y1IUjs-V7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action"
      ],
      "metadata": {
        "id": "cozryFUIQSMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Get data\n",
        "rewards = []\n",
        "final_rewards = []\n",
        "steps = []\n",
        "epi_acts = []\n",
        "actions = []\n",
        "tries = 1000\n",
        "max_t = 1000\n",
        "for i in range(tries):\n",
        "    step_count = 0\n",
        "    reward_sum = 0\n",
        "    state, info = env.reset(seed=seed)\n",
        "    for j in range(max_t):\n",
        "        action = agent.act(state)\n",
        "        epi_acts += [action]\n",
        "        state, reward, done, trun, info = env.step(action)\n",
        "        if done or trun:\n",
        "            final_rewards += [reward]\n",
        "            break \n",
        "        else:\n",
        "            final_rewards += [0]\n",
        "            reward_sum += reward\n",
        "            step_count += 1\n",
        "    actions += [epi_acts]\n",
        "    steps += [step_count]\n",
        "    rewards += [reward_sum]\n",
        "\n",
        "actions = np.asarray(actions)\n",
        "data = np.asarray([(int(s),int(r),int(f)) for s,r,f in zip(steps, np.round(rewards), final_rewards)])"
      ],
      "metadata": {
        "id": "SHxu9O8aaw6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.DataFrame(data, columns=[\"Steps\", \"Rewards\", \"Final Rewards\"])\n",
        "data_df.describe()"
      ],
      "metadata": {
        "id": "nGIWhy0IRm2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(12,6))\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(1, len(rewards)+1), rewards)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode')\n",
        "plt.show()\n",
        "print(\"Total Rewards[:-1]\", sum(data[1]), \"Average Reward:\", np.mean(data[1]), \"Avg. Final Rewards:\", np.mean(data[-1]))"
      ],
      "metadata": {
        "id": "DQLQdLKzgFaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1wigUfbF-HF"
      },
      "source": [
        "# 5. Explore\n",
        "\n",
        "In this exercise, we have provided a sample DDPG agent and demonstrated how to use it to solve an OpenAI Gym environment.  To continue your learning, you are encouraged to complete any (or all!) of the following tasks:\n",
        "- **Amend the various hyperparameters and network architecture to see if you can get your agent to solve the environment faster than this benchmark implementation.**  Once you build intuition for the hyperparameters that work well with this environment, try solving a different OpenAI Gym task!\n",
        "- Write your own DDPG implementation.  Use this code as reference only when needed -- try as much as you can to write your own algorithm from scratch.\n",
        "- You may also like to implement **prioritized experience replay**, to see if it speeds learning.  \n",
        "- The current implementation adds Ornsetein-Uhlenbeck noise to the action space.  However, it has [been shown](https://blog.openai.com/better-exploration-with-parameter-noise/) that **adding noise to the parameters of the neural network policy can improve performance.  Make this change to the code, to verify it for yourself!**\n",
        "- Write a blog post explaining the intuition behind the DDPG algorithm and demonstrating how to use it to solve an RL environment of your choosing.  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How well does DQN with Tile Coding how well does work?\n",
        "* Reuse DQN from Project 1, but use Tile Coding to turn continuous into discrete actions\n",
        "* Implement improvements on DQN from Project 1 first"
      ],
      "metadata": {
        "id": "kAsuFULRviuL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zvUpCpQ7wXIh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}